{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linkvarun/Jupyter_Notebook/blob/master/Optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJwPOH6WfFHJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import  Adadelta, Adam, RMSprop, Adagrad, Nadam, Adamax\n",
        "from tensorflow.keras.optimizers.experimental import SGD\n",
        "\n",
        "SEED = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkRl2yAIfs6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce460707-96e4-4f47-e823-fa590625507a"
      },
      "source": [
        "data = pd.read_csv('/content/winequality-red.csv', sep=\",\") # this could be used for classification or numerical regression\n",
        "print(data.head())\n",
        "print(len(data))\n",
        "y = data['quality']\n",
        "X = data.drop(['quality'], axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
            "0            7.4              0.70         0.00             1.9      0.076   \n",
            "1            7.8              0.88         0.00             2.6      0.098   \n",
            "2            7.8              0.76         0.04             2.3      0.092   \n",
            "3           11.2              0.28         0.56             1.9      0.075   \n",
            "4            7.4              0.70         0.00             1.9      0.076   \n",
            "\n",
            "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
            "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
            "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
            "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
            "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "\n",
            "   alcohol  quality  \n",
            "0      9.4        5  \n",
            "1      9.8        5  \n",
            "2      9.8        5  \n",
            "3      9.8        6  \n",
            "4      9.4        5  \n",
            "1599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTyjlymnf1HJ"
      },
      "source": [
        "def create_model(opt):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=X_train.shape[1],\n",
        "    activation='relu',kernel_initializer='glorot_normal'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(50, activation='tanh', activity_regularizer='L2'))\n",
        "    model.add(Dense(25, activation='relu', activity_regularizer='L2'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(10, activation='tanh', activity_regularizer='L2'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrvWuDvSf4zY"
      },
      "source": [
        "def create_callbacks(opt):\n",
        "    callbacks = [\n",
        "    EarlyStopping(monitor='mean_squared_error', min_delta = 0.0001, patience=5, verbose=1),\n",
        "    ModelCheckpoint('optimizers_best_' + opt + '.h5', monitor='mean_squared_error', save_best_only=True, verbose=1)\n",
        "    ]\n",
        "    return callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6xbVIYGf7Ff"
      },
      "source": [
        "# add momentum, adagrad, nesterov\n",
        "opts = dict({\n",
        "     #'sgd': SGD(),\n",
        "     'sgd-001': SGD(learning_rate=0.001, weight_decay=0.00001),\n",
        "     'sgd-momentum': SGD(learning_rate=0.0001, momentum=0.3, nesterov= True),\n",
        "     'adam': Adam(),\n",
        "     'adadelta': Adadelta(),\n",
        "     'rmsprop': RMSprop(),\n",
        "     'rmsprop-0001': RMSprop(learning_rate=0.0001),\n",
        "     'nadam': Nadam(),\n",
        "     'adamax': Adamax()\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AfzDjeJf9TC"
      },
      "source": [
        "batch_size = 64\n",
        "n_epochs = 1000\n",
        "\n",
        "results = []\n",
        "# Loop through the optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00_LNvEPf_w-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0db4f0f-c5c3-485a-897d-6d5f2d891601"
      },
      "source": [
        "for opt in opts:\n",
        "    model = create_model(opt)\n",
        "    callbacks = create_callbacks(opt)\n",
        "    model.compile(loss='mse', optimizer=opts[opt], metrics=['mean_squared_error'])\n",
        "    hist = model.fit(X_train.values, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val.values, y_val), verbose=1,\n",
        "    callbacks=callbacks)\n",
        "    best_epoch = np.argmin(hist.history['mean_squared_error'])\n",
        "    best_acc = hist.history['mean_squared_error'][best_epoch]\n",
        "    best_model = create_model(opt)\n",
        "\n",
        "    # Load the model weights with the highest validation accuracy\n",
        "    best_model.load_weights('optimizers_best_' + opt + '.h5')\n",
        "    best_model.compile(loss='mse', optimizer=opts[opt], metrics=['mean_squared_error'])\n",
        "    score = best_model.evaluate(X_test.values, y_test, verbose=1)\n",
        "    results.append([opt, best_epoch, best_acc, score[1]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 18.8769 - mean_squared_error: 18.1779  \n",
            "Epoch 1: mean_squared_error improved from inf to 16.61304, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 7s 22ms/step - loss: 17.3305 - mean_squared_error: 16.6130 - val_loss: 6.0778 - val_mean_squared_error: 5.1935\n",
            "Epoch 2/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 7.1438 - mean_squared_error: 6.2883\n",
            "Epoch 2: mean_squared_error improved from 16.61304 to 6.06680, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 6.9258 - mean_squared_error: 6.0668 - val_loss: 3.4936 - val_mean_squared_error: 2.5343\n",
            "Epoch 3/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 4.3951 - mean_squared_error: 3.4757\n",
            "Epoch 3: mean_squared_error improved from 6.06680 to 3.34892, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 4.2701 - mean_squared_error: 3.3489 - val_loss: 2.4548 - val_mean_squared_error: 1.4646\n",
            "Epoch 4/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 3.0199 - mean_squared_error: 2.0724\n",
            "Epoch 4: mean_squared_error improved from 3.34892 to 2.01677, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.9645 - mean_squared_error: 2.0168 - val_loss: 2.0022 - val_mean_squared_error: 1.0006\n",
            "Epoch 5/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.4827 - mean_squared_error: 1.5285\n",
            "Epoch 5: mean_squared_error improved from 2.01677 to 1.52522, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.4794 - mean_squared_error: 1.5252 - val_loss: 1.8223 - val_mean_squared_error: 0.8209\n",
            "Epoch 6/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.2021 - mean_squared_error: 1.2476\n",
            "Epoch 6: mean_squared_error improved from 1.52522 to 1.20489, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.1605 - mean_squared_error: 1.2049 - val_loss: 1.7619 - val_mean_squared_error: 0.7669\n",
            "Epoch 7/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.9819 - mean_squared_error: 1.0179\n",
            "Epoch 7: mean_squared_error improved from 1.20489 to 1.00143, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.9630 - mean_squared_error: 1.0014 - val_loss: 1.7569 - val_mean_squared_error: 0.7656\n",
            "Epoch 8/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.9258 - mean_squared_error: 0.9675\n",
            "Epoch 8: mean_squared_error improved from 1.00143 to 0.93374, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.8911 - mean_squared_error: 0.9337 - val_loss: 1.7660 - val_mean_squared_error: 0.7826\n",
            "Epoch 9/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.8070 - mean_squared_error: 0.8579\n",
            "Epoch 9: mean_squared_error improved from 0.93374 to 0.84790, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.7971 - mean_squared_error: 0.8479 - val_loss: 1.7772 - val_mean_squared_error: 0.8006\n",
            "Epoch 10/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.7487 - mean_squared_error: 0.8053\n",
            "Epoch 10: mean_squared_error improved from 0.84790 to 0.81442, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.7580 - mean_squared_error: 0.8144 - val_loss: 1.7875 - val_mean_squared_error: 0.8187\n",
            "Epoch 11/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.8229 - mean_squared_error: 0.8923\n",
            "Epoch 11: mean_squared_error did not improve from 0.81442\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.7841 - mean_squared_error: 0.8526 - val_loss: 1.7992 - val_mean_squared_error: 0.8354\n",
            "Epoch 12/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.7453 - mean_squared_error: 0.8207\n",
            "Epoch 12: mean_squared_error improved from 0.81442 to 0.81060, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7353 - mean_squared_error: 0.8106 - val_loss: 1.8023 - val_mean_squared_error: 0.8468\n",
            "Epoch 13/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.7342 - mean_squared_error: 0.8133\n",
            "Epoch 13: mean_squared_error did not improve from 0.81060\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7386 - mean_squared_error: 0.8176 - val_loss: 1.8029 - val_mean_squared_error: 0.8553\n",
            "Epoch 14/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.7094 - mean_squared_error: 0.7965\n",
            "Epoch 14: mean_squared_error did not improve from 0.81060\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.7436 - mean_squared_error: 0.8311 - val_loss: 1.8032 - val_mean_squared_error: 0.8633\n",
            "Epoch 15/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.7140 - mean_squared_error: 0.8081\n",
            "Epoch 15: mean_squared_error did not improve from 0.81060\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.7152 - mean_squared_error: 0.8114 - val_loss: 1.7980 - val_mean_squared_error: 0.8659\n",
            "Epoch 16/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.6697 - mean_squared_error: 0.7710\n",
            "Epoch 16: mean_squared_error improved from 0.81060 to 0.78999, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.6885 - mean_squared_error: 0.7900 - val_loss: 1.7933 - val_mean_squared_error: 0.8678\n",
            "Epoch 17/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.6634 - mean_squared_error: 0.7734\n",
            "Epoch 17: mean_squared_error improved from 0.78999 to 0.75751, saving model to optimizers_best_sgd-001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.6467 - mean_squared_error: 0.7575 - val_loss: 1.7890 - val_mean_squared_error: 0.8705\n",
            "Epoch 18/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.6765 - mean_squared_error: 0.7936\n",
            "Epoch 18: mean_squared_error did not improve from 0.75751\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6881 - mean_squared_error: 0.8063 - val_loss: 1.7863 - val_mean_squared_error: 0.8729\n",
            "Epoch 19/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.6798 - mean_squared_error: 0.7981\n",
            "Epoch 19: mean_squared_error did not improve from 0.75751\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6752 - mean_squared_error: 0.7957 - val_loss: 1.7831 - val_mean_squared_error: 0.8757\n",
            "Epoch 20/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.6549 - mean_squared_error: 0.7783\n",
            "Epoch 20: mean_squared_error did not improve from 0.75751\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.6486 - mean_squared_error: 0.7721 - val_loss: 1.7741 - val_mean_squared_error: 0.8745\n",
            "Epoch 21/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.6371 - mean_squared_error: 0.7733\n",
            "Epoch 21: mean_squared_error did not improve from 0.75751\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.6388 - mean_squared_error: 0.7750 - val_loss: 1.7658 - val_mean_squared_error: 0.8740\n",
            "Epoch 22/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.6414 - mean_squared_error: 0.7865\n",
            "Epoch 22: mean_squared_error did not improve from 0.75751\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.6295 - mean_squared_error: 0.7737 - val_loss: 1.7551 - val_mean_squared_error: 0.8729\n",
            "Epoch 22: early stopping\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.5186 - mean_squared_error: 0.6030\n",
            "Epoch 1/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 26.9126 - mean_squared_error: 26.2538 \n",
            "Epoch 1: mean_squared_error improved from inf to 25.80272, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 2s 20ms/step - loss: 26.4647 - mean_squared_error: 25.8027 - val_loss: 14.9798 - val_mean_squared_error: 14.2493\n",
            "Epoch 2/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 18.3482 - mean_squared_error: 17.6258\n",
            "Epoch 2: mean_squared_error improved from 25.80272 to 16.33339, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 17.0669 - mean_squared_error: 16.3334 - val_loss: 9.6505 - val_mean_squared_error: 8.8195\n",
            "Epoch 3/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 13.5844 - mean_squared_error: 12.7965\n",
            "Epoch 3: mean_squared_error improved from 16.33339 to 12.44025, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 13.2335 - mean_squared_error: 12.4402 - val_loss: 7.2565 - val_mean_squared_error: 6.3436\n",
            "Epoch 4/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 10.5686 - mean_squared_error: 9.7309 \n",
            "Epoch 4: mean_squared_error improved from 12.44025 to 9.44890, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 10.2937 - mean_squared_error: 9.4489 - val_loss: 6.1241 - val_mean_squared_error: 5.1542\n",
            "Epoch 5/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 9.2987 - mean_squared_error: 8.4185 \n",
            "Epoch 5: mean_squared_error improved from 9.44890 to 8.08451, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 8.9723 - mean_squared_error: 8.0845 - val_loss: 5.4256 - val_mean_squared_error: 4.4179\n",
            "Epoch 6/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 8.3564 - mean_squared_error: 7.4497\n",
            "Epoch 6: mean_squared_error improved from 8.08451 to 7.23714, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 8.1470 - mean_squared_error: 7.2371 - val_loss: 4.9238 - val_mean_squared_error: 3.8833\n",
            "Epoch 7/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 7.3027 - mean_squared_error: 6.3660\n",
            "Epoch 7: mean_squared_error improved from 7.23714 to 6.30940, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 7.2450 - mean_squared_error: 6.3094 - val_loss: 4.5362 - val_mean_squared_error: 3.4671\n",
            "Epoch 8/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 6.8181 - mean_squared_error: 5.8650\n",
            "Epoch 8: mean_squared_error improved from 6.30940 to 5.68236, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 6.6405 - mean_squared_error: 5.6824 - val_loss: 4.2140 - val_mean_squared_error: 3.1224\n",
            "Epoch 9/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 5.9263 - mean_squared_error: 4.9619\n",
            "Epoch 9: mean_squared_error improved from 5.68236 to 4.93583, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 5.9050 - mean_squared_error: 4.9358 - val_loss: 3.9165 - val_mean_squared_error: 2.8134\n",
            "Epoch 10/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 5.5833 - mean_squared_error: 4.6094\n",
            "Epoch 10: mean_squared_error improved from 4.93583 to 4.47780, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 5.4549 - mean_squared_error: 4.4778 - val_loss: 3.6577 - val_mean_squared_error: 2.5413\n",
            "Epoch 11/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 5.4266 - mean_squared_error: 4.4394\n",
            "Epoch 11: mean_squared_error improved from 4.47780 to 4.30400, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 5.2916 - mean_squared_error: 4.3040 - val_loss: 3.4143 - val_mean_squared_error: 2.2899\n",
            "Epoch 12/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 4.7375 - mean_squared_error: 3.7495\n",
            "Epoch 12: mean_squared_error improved from 4.30400 to 3.67587, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 4.6679 - mean_squared_error: 3.6759 - val_loss: 3.1857 - val_mean_squared_error: 2.0575\n",
            "Epoch 13/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 4.3983 - mean_squared_error: 3.4009\n",
            "Epoch 13: mean_squared_error improved from 3.67587 to 3.48066, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 4.4733 - mean_squared_error: 3.4807 - val_loss: 2.9784 - val_mean_squared_error: 1.8438\n",
            "Epoch 14/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 3.8474 - mean_squared_error: 2.8471\n",
            "Epoch 14: mean_squared_error improved from 3.48066 to 2.97354, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.9754 - mean_squared_error: 2.9735 - val_loss: 2.7699 - val_mean_squared_error: 1.6379\n",
            "Epoch 15/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 3.8628 - mean_squared_error: 2.8535\n",
            "Epoch 15: mean_squared_error improved from 2.97354 to 2.89315, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 3.9011 - mean_squared_error: 2.8932 - val_loss: 2.5943 - val_mean_squared_error: 1.4598\n",
            "Epoch 16/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 3.6369 - mean_squared_error: 2.6209\n",
            "Epoch 16: mean_squared_error improved from 2.89315 to 2.60153, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.6167 - mean_squared_error: 2.6015 - val_loss: 2.4441 - val_mean_squared_error: 1.3102\n",
            "Epoch 17/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 3.3719 - mean_squared_error: 2.3574\n",
            "Epoch 17: mean_squared_error improved from 2.60153 to 2.38401, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 3.3976 - mean_squared_error: 2.3840 - val_loss: 2.3167 - val_mean_squared_error: 1.1834\n",
            "Epoch 18/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 3.0912 - mean_squared_error: 2.0811\n",
            "Epoch 18: mean_squared_error improved from 2.38401 to 2.07568, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 3.0889 - mean_squared_error: 2.0757 - val_loss: 2.2114 - val_mean_squared_error: 1.0773\n",
            "Epoch 19/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.1705 - mean_squared_error: 2.1589\n",
            "Epoch 19: mean_squared_error did not improve from 2.07568\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 3.1353 - mean_squared_error: 2.1231 - val_loss: 2.1338 - val_mean_squared_error: 0.9959\n",
            "Epoch 20/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.8748 - mean_squared_error: 1.8612\n",
            "Epoch 20: mean_squared_error improved from 2.07568 to 1.82226, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.8381 - mean_squared_error: 1.8223 - val_loss: 2.0678 - val_mean_squared_error: 0.9299\n",
            "Epoch 21/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.8479 - mean_squared_error: 1.8317\n",
            "Epoch 21: mean_squared_error improved from 1.82226 to 1.79455, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8116 - mean_squared_error: 1.7945 - val_loss: 2.0140 - val_mean_squared_error: 0.8771\n",
            "Epoch 22/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.8270 - mean_squared_error: 1.8070\n",
            "Epoch 22: mean_squared_error improved from 1.79455 to 1.78592, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.8054 - mean_squared_error: 1.7859 - val_loss: 1.9782 - val_mean_squared_error: 0.8386\n",
            "Epoch 23/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.6926 - mean_squared_error: 1.6731\n",
            "Epoch 23: mean_squared_error improved from 1.78592 to 1.65882, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.6783 - mean_squared_error: 1.6588 - val_loss: 1.9509 - val_mean_squared_error: 0.8108\n",
            "Epoch 24/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.5440 - mean_squared_error: 1.5218\n",
            "Epoch 24: mean_squared_error improved from 1.65882 to 1.53234, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.5546 - mean_squared_error: 1.5323 - val_loss: 1.9257 - val_mean_squared_error: 0.7894\n",
            "Epoch 25/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.5629 - mean_squared_error: 1.5388\n",
            "Epoch 25: mean_squared_error improved from 1.53234 to 1.52248, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.5464 - mean_squared_error: 1.5225 - val_loss: 1.9101 - val_mean_squared_error: 0.7747\n",
            "Epoch 26/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.4228 - mean_squared_error: 1.3996\n",
            "Epoch 26: mean_squared_error improved from 1.52248 to 1.37270, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.3964 - mean_squared_error: 1.3727 - val_loss: 1.8982 - val_mean_squared_error: 0.7650\n",
            "Epoch 27/1000\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.4292 - mean_squared_error: 1.4058\n",
            "Epoch 27: mean_squared_error did not improve from 1.37270\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4292 - mean_squared_error: 1.4058 - val_loss: 1.8914 - val_mean_squared_error: 0.7591\n",
            "Epoch 28/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3833 - mean_squared_error: 1.3614\n",
            "Epoch 28: mean_squared_error improved from 1.37270 to 1.33121, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.3533 - mean_squared_error: 1.3312 - val_loss: 1.8854 - val_mean_squared_error: 0.7561\n",
            "Epoch 29/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.3243 - mean_squared_error: 1.3072\n",
            "Epoch 29: mean_squared_error improved from 1.33121 to 1.32593, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.3440 - mean_squared_error: 1.3259 - val_loss: 1.8839 - val_mean_squared_error: 0.7553\n",
            "Epoch 30/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3614 - mean_squared_error: 1.3418\n",
            "Epoch 30: mean_squared_error did not improve from 1.32593\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3649 - mean_squared_error: 1.3461 - val_loss: 1.8842 - val_mean_squared_error: 0.7562\n",
            "Epoch 31/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.2881 - mean_squared_error: 1.2646\n",
            "Epoch 31: mean_squared_error improved from 1.32593 to 1.27972, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.3024 - mean_squared_error: 1.2797 - val_loss: 1.8826 - val_mean_squared_error: 0.7581\n",
            "Epoch 32/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.2821 - mean_squared_error: 1.2631\n",
            "Epoch 32: mean_squared_error improved from 1.27972 to 1.24899, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.2669 - mean_squared_error: 1.2490 - val_loss: 1.8846 - val_mean_squared_error: 0.7610\n",
            "Epoch 33/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.2845 - mean_squared_error: 1.2656\n",
            "Epoch 33: mean_squared_error improved from 1.24899 to 1.23957, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.2579 - mean_squared_error: 1.2396 - val_loss: 1.8867 - val_mean_squared_error: 0.7654\n",
            "Epoch 34/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.1334 - mean_squared_error: 1.1136\n",
            "Epoch 34: mean_squared_error improved from 1.23957 to 1.13257, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1525 - mean_squared_error: 1.1326 - val_loss: 1.8889 - val_mean_squared_error: 0.7694\n",
            "Epoch 35/1000\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.2173 - mean_squared_error: 1.1956\n",
            "Epoch 35: mean_squared_error did not improve from 1.13257\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.2173 - mean_squared_error: 1.1956 - val_loss: 1.8909 - val_mean_squared_error: 0.7734\n",
            "Epoch 36/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.0887 - mean_squared_error: 1.0687\n",
            "Epoch 36: mean_squared_error improved from 1.13257 to 1.10350, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.1230 - mean_squared_error: 1.1035 - val_loss: 1.8923 - val_mean_squared_error: 0.7780\n",
            "Epoch 37/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.1347 - mean_squared_error: 1.1232\n",
            "Epoch 37: mean_squared_error did not improve from 1.10350\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.1161 - mean_squared_error: 1.1052 - val_loss: 1.8937 - val_mean_squared_error: 0.7826\n",
            "Epoch 38/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.1011 - mean_squared_error: 1.0873\n",
            "Epoch 38: mean_squared_error improved from 1.10350 to 1.08518, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.0980 - mean_squared_error: 1.0852 - val_loss: 1.8967 - val_mean_squared_error: 0.7873\n",
            "Epoch 39/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.1448 - mean_squared_error: 1.1293\n",
            "Epoch 39: mean_squared_error did not improve from 1.08518\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1666 - mean_squared_error: 1.1524 - val_loss: 1.8988 - val_mean_squared_error: 0.7921\n",
            "Epoch 40/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.1070 - mean_squared_error: 1.0985\n",
            "Epoch 40: mean_squared_error did not improve from 1.08518\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.1483 - mean_squared_error: 1.1382 - val_loss: 1.9022 - val_mean_squared_error: 0.7968\n",
            "Epoch 41/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.0926 - mean_squared_error: 1.0759\n",
            "Epoch 41: mean_squared_error improved from 1.08518 to 1.05289, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.0679 - mean_squared_error: 1.0529 - val_loss: 1.9038 - val_mean_squared_error: 0.8010\n",
            "Epoch 42/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.0192 - mean_squared_error: 1.0107\n",
            "Epoch 42: mean_squared_error improved from 1.05289 to 1.01576, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.0237 - mean_squared_error: 1.0158 - val_loss: 1.9057 - val_mean_squared_error: 0.8059\n",
            "Epoch 43/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.9938 - mean_squared_error: 0.9822\n",
            "Epoch 43: mean_squared_error improved from 1.01576 to 0.98579, saving model to optimizers_best_sgd-momentum.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.9971 - mean_squared_error: 0.9858 - val_loss: 1.9067 - val_mean_squared_error: 0.8108\n",
            "Epoch 44/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.1020 - mean_squared_error: 1.1005\n",
            "Epoch 44: mean_squared_error did not improve from 0.98579\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.0942 - mean_squared_error: 1.0921 - val_loss: 1.9088 - val_mean_squared_error: 0.8152\n",
            "Epoch 45/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.9869 - mean_squared_error: 0.9803\n",
            "Epoch 45: mean_squared_error did not improve from 0.98579\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.0381 - mean_squared_error: 1.0312 - val_loss: 1.9094 - val_mean_squared_error: 0.8194\n",
            "Epoch 46/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.1314 - mean_squared_error: 1.1247\n",
            "Epoch 46: mean_squared_error did not improve from 0.98579\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.1158 - mean_squared_error: 1.1103 - val_loss: 1.9114 - val_mean_squared_error: 0.8241\n",
            "Epoch 47/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.0560 - mean_squared_error: 1.0544\n",
            "Epoch 47: mean_squared_error did not improve from 0.98579\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.0513 - mean_squared_error: 1.0500 - val_loss: 1.9117 - val_mean_squared_error: 0.8281\n",
            "Epoch 48/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.0580 - mean_squared_error: 1.0632\n",
            "Epoch 48: mean_squared_error did not improve from 0.98579\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.0658 - mean_squared_error: 1.0698 - val_loss: 1.9129 - val_mean_squared_error: 0.8327\n",
            "Epoch 48: early stopping\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.6614 - mean_squared_error: 0.5693\n",
            "Epoch 1/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 25.3807 - mean_squared_error: 24.8098 \n",
            "Epoch 1: mean_squared_error improved from inf to 21.97117, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 3s 25ms/step - loss: 22.5669 - mean_squared_error: 21.9712 - val_loss: 11.5645 - val_mean_squared_error: 10.8672\n",
            "Epoch 2/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 13.2469 - mean_squared_error: 12.4881\n",
            "Epoch 2: mean_squared_error improved from 21.97117 to 11.68306, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 12.4781 - mean_squared_error: 11.6831 - val_loss: 7.4723 - val_mean_squared_error: 6.5165\n",
            "Epoch 3/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 9.7612 - mean_squared_error: 8.8337\n",
            "Epoch 3: mean_squared_error improved from 11.68306 to 8.25997, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 9.2177 - mean_squared_error: 8.2600 - val_loss: 5.5787 - val_mean_squared_error: 4.4891\n",
            "Epoch 4/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 7.7769 - mean_squared_error: 6.7149\n",
            "Epoch 4: mean_squared_error improved from 8.25997 to 6.29699, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 7.3786 - mean_squared_error: 6.2970 - val_loss: 4.7569 - val_mean_squared_error: 3.5775\n",
            "Epoch 5/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 6.6393 - mean_squared_error: 5.4865\n",
            "Epoch 5: mean_squared_error improved from 6.29699 to 5.08260, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 6.2454 - mean_squared_error: 5.0826 - val_loss: 4.2287 - val_mean_squared_error: 2.9937\n",
            "Epoch 6/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 5.6191 - mean_squared_error: 4.4195\n",
            "Epoch 6: mean_squared_error improved from 5.08260 to 4.07909, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 5.2882 - mean_squared_error: 4.0791 - val_loss: 3.8099 - val_mean_squared_error: 2.5349\n",
            "Epoch 7/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 4.7700 - mean_squared_error: 3.5441\n",
            "Epoch 7: mean_squared_error improved from 4.07909 to 3.36462, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 4.5921 - mean_squared_error: 3.3646 - val_loss: 3.4394 - val_mean_squared_error: 2.1516\n",
            "Epoch 8/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 4.1061 - mean_squared_error: 2.8788\n",
            "Epoch 8: mean_squared_error improved from 3.36462 to 2.79406, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 4.0170 - mean_squared_error: 2.7941 - val_loss: 3.1087 - val_mean_squared_error: 1.8286\n",
            "Epoch 9/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 3.5770 - mean_squared_error: 2.3653\n",
            "Epoch 9: mean_squared_error improved from 2.79406 to 2.34584, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 3.5528 - mean_squared_error: 2.3458 - val_loss: 2.8064 - val_mean_squared_error: 1.5613\n",
            "Epoch 10/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 3.0857 - mean_squared_error: 1.9101\n",
            "Epoch 10: mean_squared_error improved from 2.34584 to 1.91843, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 3.0844 - mean_squared_error: 1.9184 - val_loss: 2.5389 - val_mean_squared_error: 1.3525\n",
            "Epoch 11/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 2.8843 - mean_squared_error: 1.7549\n",
            "Epoch 11: mean_squared_error improved from 1.91843 to 1.65873, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 2.7814 - mean_squared_error: 1.6587 - val_loss: 2.3258 - val_mean_squared_error: 1.1858\n",
            "Epoch 12/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 2.5594 - mean_squared_error: 1.4667\n",
            "Epoch 12: mean_squared_error improved from 1.65873 to 1.43496, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.5157 - mean_squared_error: 1.4350 - val_loss: 2.1412 - val_mean_squared_error: 1.0600\n",
            "Epoch 13/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.3234 - mean_squared_error: 1.2860\n",
            "Epoch 13: mean_squared_error improved from 1.43496 to 1.28114, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.3145 - mean_squared_error: 1.2811 - val_loss: 2.0002 - val_mean_squared_error: 0.9661\n",
            "Epoch 14/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.1194 - mean_squared_error: 1.1257\n",
            "Epoch 14: mean_squared_error improved from 1.28114 to 1.10762, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.0992 - mean_squared_error: 1.1076 - val_loss: 1.8907 - val_mean_squared_error: 0.8968\n",
            "Epoch 15/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.9736 - mean_squared_error: 1.0196\n",
            "Epoch 15: mean_squared_error improved from 1.10762 to 1.01916, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.9699 - mean_squared_error: 1.0192 - val_loss: 1.8009 - val_mean_squared_error: 0.8480\n",
            "Epoch 16/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.9026 - mean_squared_error: 0.9825\n",
            "Epoch 16: mean_squared_error improved from 1.01916 to 0.95062, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.8684 - mean_squared_error: 0.9506 - val_loss: 1.7360 - val_mean_squared_error: 0.8122\n",
            "Epoch 17/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.7802 - mean_squared_error: 0.8897\n",
            "Epoch 17: mean_squared_error improved from 0.95062 to 0.89241, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.7801 - mean_squared_error: 0.8924 - val_loss: 1.6819 - val_mean_squared_error: 0.7896\n",
            "Epoch 18/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.6837 - mean_squared_error: 0.8185\n",
            "Epoch 18: mean_squared_error improved from 0.89241 to 0.83000, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.6930 - mean_squared_error: 0.8300 - val_loss: 1.6415 - val_mean_squared_error: 0.7757\n",
            "Epoch 19/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.6389 - mean_squared_error: 0.8015\n",
            "Epoch 19: mean_squared_error improved from 0.83000 to 0.79121, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.6246 - mean_squared_error: 0.7912 - val_loss: 1.5976 - val_mean_squared_error: 0.7700\n",
            "Epoch 20/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.6274 - mean_squared_error: 0.8145\n",
            "Epoch 20: mean_squared_error improved from 0.79121 to 0.78589, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.5970 - mean_squared_error: 0.7859 - val_loss: 1.5759 - val_mean_squared_error: 0.7659\n",
            "Epoch 21/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.5169 - mean_squared_error: 0.7254\n",
            "Epoch 21: mean_squared_error improved from 0.78589 to 0.74196, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.5325 - mean_squared_error: 0.7420 - val_loss: 1.5552 - val_mean_squared_error: 0.7653\n",
            "Epoch 22/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.5331 - mean_squared_error: 0.7571\n",
            "Epoch 22: mean_squared_error did not improve from 0.74196\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.5268 - mean_squared_error: 0.7514 - val_loss: 1.5454 - val_mean_squared_error: 0.7674\n",
            "Epoch 23/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.5072 - mean_squared_error: 0.7410\n",
            "Epoch 23: mean_squared_error improved from 0.74196 to 0.72376, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.4895 - mean_squared_error: 0.7238 - val_loss: 1.5354 - val_mean_squared_error: 0.7707\n",
            "Epoch 24/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.4665 - mean_squared_error: 0.7098\n",
            "Epoch 24: mean_squared_error did not improve from 0.72376\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4952 - mean_squared_error: 0.7398 - val_loss: 1.5317 - val_mean_squared_error: 0.7752\n",
            "Epoch 25/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.4561 - mean_squared_error: 0.7022\n",
            "Epoch 25: mean_squared_error improved from 0.72376 to 0.69578, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.4489 - mean_squared_error: 0.6958 - val_loss: 1.5259 - val_mean_squared_error: 0.7795\n",
            "Epoch 26/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.4821 - mean_squared_error: 0.7462\n",
            "Epoch 26: mean_squared_error did not improve from 0.69578\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4709 - mean_squared_error: 0.7360 - val_loss: 1.5170 - val_mean_squared_error: 0.7825\n",
            "Epoch 27/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.4540 - mean_squared_error: 0.7243\n",
            "Epoch 27: mean_squared_error did not improve from 0.69578\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4432 - mean_squared_error: 0.7138 - val_loss: 1.5170 - val_mean_squared_error: 0.7871\n",
            "Epoch 28/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.3763 - mean_squared_error: 0.6549\n",
            "Epoch 28: mean_squared_error did not improve from 0.69578\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.4222 - mean_squared_error: 0.7015 - val_loss: 1.5046 - val_mean_squared_error: 0.7879\n",
            "Epoch 29/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.3879 - mean_squared_error: 0.6758\n",
            "Epoch 29: mean_squared_error did not improve from 0.69578\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4141 - mean_squared_error: 0.7028 - val_loss: 1.4999 - val_mean_squared_error: 0.7912\n",
            "Epoch 30/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.3957 - mean_squared_error: 0.6902\n",
            "Epoch 30: mean_squared_error improved from 0.69578 to 0.68634, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.3916 - mean_squared_error: 0.6863 - val_loss: 1.4987 - val_mean_squared_error: 0.7957\n",
            "Epoch 31/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.4049 - mean_squared_error: 0.7081\n",
            "Epoch 31: mean_squared_error did not improve from 0.68634\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3917 - mean_squared_error: 0.6960 - val_loss: 1.4880 - val_mean_squared_error: 0.7946\n",
            "Epoch 32/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.4088 - mean_squared_error: 0.7198\n",
            "Epoch 32: mean_squared_error improved from 0.68634 to 0.68431, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.3743 - mean_squared_error: 0.6843 - val_loss: 1.4940 - val_mean_squared_error: 0.8007\n",
            "Epoch 33/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.4071 - mean_squared_error: 0.7091\n",
            "Epoch 33: mean_squared_error did not improve from 0.68431\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.4194 - mean_squared_error: 0.7218 - val_loss: 1.5113 - val_mean_squared_error: 0.8111\n",
            "Epoch 34/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.3764 - mean_squared_error: 0.6835\n",
            "Epoch 34: mean_squared_error improved from 0.68431 to 0.67596, saving model to optimizers_best_adam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.3668 - mean_squared_error: 0.6760 - val_loss: 1.4889 - val_mean_squared_error: 0.8028\n",
            "Epoch 35/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.3428 - mean_squared_error: 0.6651\n",
            "Epoch 35: mean_squared_error did not improve from 0.67596\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3560 - mean_squared_error: 0.6794 - val_loss: 1.4714 - val_mean_squared_error: 0.7962\n",
            "Epoch 36/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.3616 - mean_squared_error: 0.6868\n",
            "Epoch 36: mean_squared_error did not improve from 0.67596\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3567 - mean_squared_error: 0.6813 - val_loss: 1.5033 - val_mean_squared_error: 0.8159\n",
            "Epoch 37/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.3854 - mean_squared_error: 0.7042\n",
            "Epoch 37: mean_squared_error did not improve from 0.67596\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.3776 - mean_squared_error: 0.6966 - val_loss: 1.5000 - val_mean_squared_error: 0.8158\n",
            "Epoch 38/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.3949 - mean_squared_error: 0.7179\n",
            "Epoch 38: mean_squared_error did not improve from 0.67596\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3757 - mean_squared_error: 0.6989 - val_loss: 1.4851 - val_mean_squared_error: 0.8115\n",
            "Epoch 39/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.3905 - mean_squared_error: 0.7253\n",
            "Epoch 39: mean_squared_error did not improve from 0.67596\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3847 - mean_squared_error: 0.7187 - val_loss: 1.4892 - val_mean_squared_error: 0.8152\n",
            "Epoch 39: early stopping\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.2639 - mean_squared_error: 0.5772\n",
            "Epoch 1/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 36.0698 - mean_squared_error: 35.5007 \n",
            "Epoch 1: mean_squared_error improved from inf to 35.07548, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 2s 20ms/step - loss: 35.6459 - mean_squared_error: 35.0755 - val_loss: 33.9429 - val_mean_squared_error: 33.4172\n",
            "Epoch 2/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 35.9706 - mean_squared_error: 35.4001\n",
            "Epoch 2: mean_squared_error did not improve from 35.07548\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 35.6991 - mean_squared_error: 35.1278 - val_loss: 33.8631 - val_mean_squared_error: 33.3375\n",
            "Epoch 3/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 35.8113 - mean_squared_error: 35.2446\n",
            "Epoch 3: mean_squared_error did not improve from 35.07548\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 35.7833 - mean_squared_error: 35.2156 - val_loss: 33.7840 - val_mean_squared_error: 33.2584\n",
            "Epoch 4/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 35.4614 - mean_squared_error: 34.8911\n",
            "Epoch 4: mean_squared_error improved from 35.07548 to 34.95593, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 35.5253 - mean_squared_error: 34.9559 - val_loss: 33.7014 - val_mean_squared_error: 33.1759\n",
            "Epoch 5/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 35.4417 - mean_squared_error: 34.8730\n",
            "Epoch 5: mean_squared_error improved from 34.95593 to 34.84869, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 35.4167 - mean_squared_error: 34.8487 - val_loss: 33.6211 - val_mean_squared_error: 33.0957\n",
            "Epoch 6/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 35.3656 - mean_squared_error: 34.7997\n",
            "Epoch 6: mean_squared_error did not improve from 34.84869\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 35.5551 - mean_squared_error: 34.9871 - val_loss: 33.5415 - val_mean_squared_error: 33.0162\n",
            "Epoch 7/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 34.9120 - mean_squared_error: 34.3428\n",
            "Epoch 7: mean_squared_error improved from 34.84869 to 34.38745, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 34.9543 - mean_squared_error: 34.3875 - val_loss: 33.4628 - val_mean_squared_error: 32.9375\n",
            "Epoch 8/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 35.1753 - mean_squared_error: 34.6045\n",
            "Epoch 8: mean_squared_error did not improve from 34.38745\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 35.1139 - mean_squared_error: 34.5426 - val_loss: 33.3822 - val_mean_squared_error: 32.8569\n",
            "Epoch 9/1000\n",
            "16/16 [==============================] - ETA: 0s - loss: 35.5968 - mean_squared_error: 35.0271\n",
            "Epoch 9: mean_squared_error did not improve from 34.38745\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 35.5968 - mean_squared_error: 35.0271 - val_loss: 33.2999 - val_mean_squared_error: 32.7747\n",
            "Epoch 10/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 35.3020 - mean_squared_error: 34.7318\n",
            "Epoch 10: mean_squared_error did not improve from 34.38745\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 34.9646 - mean_squared_error: 34.3933 - val_loss: 33.2192 - val_mean_squared_error: 32.6940\n",
            "Epoch 11/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 34.9407 - mean_squared_error: 34.3733\n",
            "Epoch 11: mean_squared_error improved from 34.38745 to 34.37045, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 34.9409 - mean_squared_error: 34.3705 - val_loss: 33.1351 - val_mean_squared_error: 32.6100\n",
            "Epoch 12/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 34.5629 - mean_squared_error: 33.9947\n",
            "Epoch 12: mean_squared_error did not improve from 34.37045\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 34.9664 - mean_squared_error: 34.3975 - val_loss: 33.0516 - val_mean_squared_error: 32.5266\n",
            "Epoch 13/1000\n",
            "16/16 [==============================] - ETA: 0s - loss: 35.0016 - mean_squared_error: 34.4324\n",
            "Epoch 13: mean_squared_error did not improve from 34.37045\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 35.0016 - mean_squared_error: 34.4324 - val_loss: 32.9681 - val_mean_squared_error: 32.4432\n",
            "Epoch 14/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 34.6249 - mean_squared_error: 34.0552\n",
            "Epoch 14: mean_squared_error improved from 34.37045 to 34.30711, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 34.8770 - mean_squared_error: 34.3071 - val_loss: 32.8833 - val_mean_squared_error: 32.3584\n",
            "Epoch 15/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 35.3254 - mean_squared_error: 34.7553\n",
            "Epoch 15: mean_squared_error did not improve from 34.30711\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 35.1195 - mean_squared_error: 34.5512 - val_loss: 32.8000 - val_mean_squared_error: 32.2751\n",
            "Epoch 16/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 34.8315 - mean_squared_error: 34.2623\n",
            "Epoch 16: mean_squared_error did not improve from 34.30711\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 35.0931 - mean_squared_error: 34.5238 - val_loss: 32.7156 - val_mean_squared_error: 32.1908\n",
            "Epoch 17/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 34.6740 - mean_squared_error: 34.1050\n",
            "Epoch 17: mean_squared_error improved from 34.30711 to 34.23009, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 34.7980 - mean_squared_error: 34.2301 - val_loss: 32.6290 - val_mean_squared_error: 32.1042\n",
            "Epoch 18/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 34.9310 - mean_squared_error: 34.3612\n",
            "Epoch 18: mean_squared_error improved from 34.23009 to 33.86280, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 34.4321 - mean_squared_error: 33.8628 - val_loss: 32.5446 - val_mean_squared_error: 32.0200\n",
            "Epoch 19/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 34.3522 - mean_squared_error: 33.7819\n",
            "Epoch 19: mean_squared_error did not improve from 33.86280\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 34.6463 - mean_squared_error: 34.0780 - val_loss: 32.4590 - val_mean_squared_error: 31.9344\n",
            "Epoch 20/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 35.0417 - mean_squared_error: 34.4748\n",
            "Epoch 20: mean_squared_error did not improve from 33.86280\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 34.8091 - mean_squared_error: 34.2412 - val_loss: 32.3758 - val_mean_squared_error: 31.8512\n",
            "Epoch 21/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 34.2724 - mean_squared_error: 33.7006\n",
            "Epoch 21: mean_squared_error did not improve from 33.86280\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 34.5468 - mean_squared_error: 33.9756 - val_loss: 32.2881 - val_mean_squared_error: 31.7635\n",
            "Epoch 22/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 34.5917 - mean_squared_error: 34.0192\n",
            "Epoch 22: mean_squared_error improved from 33.86280 to 33.71534, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 34.2873 - mean_squared_error: 33.7153 - val_loss: 32.2033 - val_mean_squared_error: 31.6787\n",
            "Epoch 23/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 34.5151 - mean_squared_error: 33.9495\n",
            "Epoch 23: mean_squared_error did not improve from 33.71534\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 34.5809 - mean_squared_error: 34.0132 - val_loss: 32.1167 - val_mean_squared_error: 31.5921\n",
            "Epoch 24/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 34.4232 - mean_squared_error: 33.8545\n",
            "Epoch 24: mean_squared_error improved from 33.71534 to 33.65294, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 34.2219 - mean_squared_error: 33.6529 - val_loss: 32.0275 - val_mean_squared_error: 31.5030\n",
            "Epoch 25/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 34.1773 - mean_squared_error: 33.6093\n",
            "Epoch 25: mean_squared_error did not improve from 33.65294\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 34.4571 - mean_squared_error: 33.8885 - val_loss: 31.9394 - val_mean_squared_error: 31.4150\n",
            "Epoch 26/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 34.2807 - mean_squared_error: 33.7124\n",
            "Epoch 26: mean_squared_error did not improve from 33.65294\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 34.2433 - mean_squared_error: 33.6755 - val_loss: 31.8503 - val_mean_squared_error: 31.3259\n",
            "Epoch 27/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 33.9946 - mean_squared_error: 33.4250\n",
            "Epoch 27: mean_squared_error improved from 33.65294 to 33.60577, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 34.1758 - mean_squared_error: 33.6058 - val_loss: 31.7626 - val_mean_squared_error: 31.2382\n",
            "Epoch 28/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 34.4311 - mean_squared_error: 33.8593\n",
            "Epoch 28: mean_squared_error did not improve from 33.60577\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 34.1803 - mean_squared_error: 33.6111 - val_loss: 31.6718 - val_mean_squared_error: 31.1475\n",
            "Epoch 29/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 33.7689 - mean_squared_error: 33.2027\n",
            "Epoch 29: mean_squared_error improved from 33.60577 to 33.23663, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 33.8037 - mean_squared_error: 33.2366 - val_loss: 31.5826 - val_mean_squared_error: 31.0583\n",
            "Epoch 30/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 34.2730 - mean_squared_error: 33.7050\n",
            "Epoch 30: mean_squared_error did not improve from 33.23663\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 34.0256 - mean_squared_error: 33.4573 - val_loss: 31.4952 - val_mean_squared_error: 30.9709\n",
            "Epoch 31/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 33.8516 - mean_squared_error: 33.2873\n",
            "Epoch 31: mean_squared_error did not improve from 33.23663\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 33.9047 - mean_squared_error: 33.3376 - val_loss: 31.4051 - val_mean_squared_error: 30.8808\n",
            "Epoch 32/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 33.4491 - mean_squared_error: 32.8822\n",
            "Epoch 32: mean_squared_error improved from 33.23663 to 33.00087, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 33.5690 - mean_squared_error: 33.0009 - val_loss: 31.3173 - val_mean_squared_error: 30.7930\n",
            "Epoch 33/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 33.4527 - mean_squared_error: 32.8831\n",
            "Epoch 33: mean_squared_error did not improve from 33.00087\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 33.6215 - mean_squared_error: 33.0518 - val_loss: 31.2284 - val_mean_squared_error: 30.7040\n",
            "Epoch 34/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 33.3246 - mean_squared_error: 32.7593\n",
            "Epoch 34: mean_squared_error improved from 33.00087 to 32.75526, saving model to optimizers_best_adadelta.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 33.3225 - mean_squared_error: 32.7553 - val_loss: 31.1352 - val_mean_squared_error: 30.6109\n",
            "Epoch 35/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 34.0518 - mean_squared_error: 33.4834\n",
            "Epoch 35: mean_squared_error did not improve from 32.75526\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 33.7181 - mean_squared_error: 33.1487 - val_loss: 31.0434 - val_mean_squared_error: 30.5191\n",
            "Epoch 36/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 33.4073 - mean_squared_error: 32.8355\n",
            "Epoch 36: mean_squared_error did not improve from 32.75526\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 33.4292 - mean_squared_error: 32.8592 - val_loss: 30.9513 - val_mean_squared_error: 30.4270\n",
            "Epoch 37/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 33.5366 - mean_squared_error: 32.9659\n",
            "Epoch 37: mean_squared_error did not improve from 32.75526\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 33.6581 - mean_squared_error: 33.0875 - val_loss: 30.8624 - val_mean_squared_error: 30.3382\n",
            "Epoch 38/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 33.3545 - mean_squared_error: 32.7848\n",
            "Epoch 38: mean_squared_error did not improve from 32.75526\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 33.4589 - mean_squared_error: 32.8894 - val_loss: 30.7761 - val_mean_squared_error: 30.2518\n",
            "Epoch 39/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 33.6179 - mean_squared_error: 33.0461\n",
            "Epoch 39: mean_squared_error did not improve from 32.75526\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 33.4167 - mean_squared_error: 32.8469 - val_loss: 30.6888 - val_mean_squared_error: 30.1645\n",
            "Epoch 39: early stopping\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 32.5240 - mean_squared_error: 32.0070\n",
            "Epoch 1/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 19.3535 - mean_squared_error: 18.6861 \n",
            "Epoch 1: mean_squared_error improved from inf to 17.42761, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 2s 18ms/step - loss: 18.1144 - mean_squared_error: 17.4276 - val_loss: 9.1800 - val_mean_squared_error: 8.3125\n",
            "Epoch 2/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 9.2082 - mean_squared_error: 8.3233  \n",
            "Epoch 2: mean_squared_error improved from 17.42761 to 8.07881, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 8.9743 - mean_squared_error: 8.0788 - val_loss: 5.3403 - val_mean_squared_error: 4.2855\n",
            "Epoch 3/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 6.1635 - mean_squared_error: 5.1449\n",
            "Epoch 3: mean_squared_error improved from 8.07881 to 5.02327, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 6.0467 - mean_squared_error: 5.0233 - val_loss: 4.2688 - val_mean_squared_error: 3.1189\n",
            "Epoch 4/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 4.9513 - mean_squared_error: 3.8681\n",
            "Epoch 4: mean_squared_error improved from 5.02327 to 3.81266, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 4.8966 - mean_squared_error: 3.8127 - val_loss: 3.6636 - val_mean_squared_error: 2.5268\n",
            "Epoch 5/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 4.1703 - mean_squared_error: 3.0872\n",
            "Epoch 5: mean_squared_error improved from 3.81266 to 2.98740, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 4.0729 - mean_squared_error: 2.9874 - val_loss: 3.1015 - val_mean_squared_error: 1.9773\n",
            "Epoch 6/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 3.4534 - mean_squared_error: 2.3850\n",
            "Epoch 6: mean_squared_error improved from 2.98740 to 2.32881, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 3.3922 - mean_squared_error: 2.3288 - val_loss: 2.6708 - val_mean_squared_error: 1.6171\n",
            "Epoch 7/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.9253 - mean_squared_error: 1.9344\n",
            "Epoch 7: mean_squared_error improved from 2.32881 to 1.88866, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.8743 - mean_squared_error: 1.8887 - val_loss: 2.3043 - val_mean_squared_error: 1.3374\n",
            "Epoch 8/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.4795 - mean_squared_error: 1.5661\n",
            "Epoch 8: mean_squared_error improved from 1.88866 to 1.51614, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.4268 - mean_squared_error: 1.5161 - val_loss: 2.0358 - val_mean_squared_error: 1.1192\n",
            "Epoch 9/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.1037 - mean_squared_error: 1.2524\n",
            "Epoch 9: mean_squared_error improved from 1.51614 to 1.22777, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.0756 - mean_squared_error: 1.2278 - val_loss: 1.8137 - val_mean_squared_error: 0.9615\n",
            "Epoch 10/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.8446 - mean_squared_error: 1.0375\n",
            "Epoch 10: mean_squared_error improved from 1.22777 to 1.02346, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.8275 - mean_squared_error: 1.0235 - val_loss: 1.6695 - val_mean_squared_error: 0.8496\n",
            "Epoch 11/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.6670 - mean_squared_error: 0.8982\n",
            "Epoch 11: mean_squared_error improved from 1.02346 to 0.87960, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.6478 - mean_squared_error: 0.8796 - val_loss: 1.5658 - val_mean_squared_error: 0.7866\n",
            "Epoch 12/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.5033 - mean_squared_error: 0.7815\n",
            "Epoch 12: mean_squared_error improved from 0.87960 to 0.76338, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.4823 - mean_squared_error: 0.7634 - val_loss: 1.4717 - val_mean_squared_error: 0.7616\n",
            "Epoch 13/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.4043 - mean_squared_error: 0.7173\n",
            "Epoch 13: mean_squared_error improved from 0.76338 to 0.72253, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.4061 - mean_squared_error: 0.7225 - val_loss: 1.4860 - val_mean_squared_error: 0.7708\n",
            "Epoch 14/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.2997 - mean_squared_error: 0.6477\n",
            "Epoch 14: mean_squared_error improved from 0.72253 to 0.65837, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3075 - mean_squared_error: 0.6584 - val_loss: 1.4324 - val_mean_squared_error: 0.7815\n",
            "Epoch 15/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.2848 - mean_squared_error: 0.6646\n",
            "Epoch 15: mean_squared_error did not improve from 0.65837\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2886 - mean_squared_error: 0.6693 - val_loss: 1.4168 - val_mean_squared_error: 0.7952\n",
            "Epoch 16/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.2840 - mean_squared_error: 0.6812\n",
            "Epoch 16: mean_squared_error did not improve from 0.65837\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.2838 - mean_squared_error: 0.6813 - val_loss: 1.4086 - val_mean_squared_error: 0.8105\n",
            "Epoch 17/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.2766 - mean_squared_error: 0.6836\n",
            "Epoch 17: mean_squared_error did not improve from 0.65837\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.2809 - mean_squared_error: 0.6891 - val_loss: 1.4072 - val_mean_squared_error: 0.8215\n",
            "Epoch 18/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.2246 - mean_squared_error: 0.6506\n",
            "Epoch 18: mean_squared_error improved from 0.65837 to 0.65499, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.2276 - mean_squared_error: 0.6550 - val_loss: 1.3332 - val_mean_squared_error: 0.7874\n",
            "Epoch 19/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.2544 - mean_squared_error: 0.6877\n",
            "Epoch 19: mean_squared_error did not improve from 0.65499\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2515 - mean_squared_error: 0.6878 - val_loss: 1.3293 - val_mean_squared_error: 0.7929\n",
            "Epoch 20/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.2201 - mean_squared_error: 0.6706\n",
            "Epoch 20: mean_squared_error did not improve from 0.65499\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2282 - mean_squared_error: 0.6812 - val_loss: 1.3686 - val_mean_squared_error: 0.8291\n",
            "Epoch 21/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.1965 - mean_squared_error: 0.6671\n",
            "Epoch 21: mean_squared_error improved from 0.65499 to 0.64017, saving model to optimizers_best_rmsprop.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.1693 - mean_squared_error: 0.6402 - val_loss: 1.3205 - val_mean_squared_error: 0.8135\n",
            "Epoch 22/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.1908 - mean_squared_error: 0.6804\n",
            "Epoch 22: mean_squared_error did not improve from 0.64017\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.1839 - mean_squared_error: 0.6752 - val_loss: 1.2507 - val_mean_squared_error: 0.7822\n",
            "Epoch 23/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.1519 - mean_squared_error: 0.6568\n",
            "Epoch 23: mean_squared_error did not improve from 0.64017\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.1775 - mean_squared_error: 0.6827 - val_loss: 1.2857 - val_mean_squared_error: 0.8159\n",
            "Epoch 24/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.1273 - mean_squared_error: 0.6443\n",
            "Epoch 24: mean_squared_error did not improve from 0.64017\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.1353 - mean_squared_error: 0.6516 - val_loss: 1.2965 - val_mean_squared_error: 0.8271\n",
            "Epoch 25/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 1.1373 - mean_squared_error: 0.6551\n",
            "Epoch 25: mean_squared_error did not improve from 0.64017\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1214 - mean_squared_error: 0.6446 - val_loss: 1.2275 - val_mean_squared_error: 0.7900\n",
            "Epoch 26/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.1133 - mean_squared_error: 0.6590\n",
            "Epoch 26: mean_squared_error did not improve from 0.64017\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.0978 - mean_squared_error: 0.6445 - val_loss: 1.1864 - val_mean_squared_error: 0.7763\n",
            "Epoch 26: early stopping\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.0407 - mean_squared_error: 0.5408\n",
            "Epoch 1/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 24.6966 - mean_squared_error: 24.1034 \n",
            "Epoch 1: mean_squared_error improved from inf to 23.31806, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 2s 25ms/step - loss: 23.9120 - mean_squared_error: 23.3181 - val_loss: 17.3342 - val_mean_squared_error: 16.7457\n",
            "Epoch 2/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 21.0132 - mean_squared_error: 20.3976\n",
            "Epoch 2: mean_squared_error improved from 23.31806 to 19.97986, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 20.5962 - mean_squared_error: 19.9799 - val_loss: 14.9349 - val_mean_squared_error: 14.3097\n",
            "Epoch 3/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 18.6599 - mean_squared_error: 18.0222\n",
            "Epoch 3: mean_squared_error improved from 19.97986 to 17.66681, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 18.3097 - mean_squared_error: 17.6668 - val_loss: 13.1714 - val_mean_squared_error: 12.5039\n",
            "Epoch 4/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 16.4519 - mean_squared_error: 15.7853\n",
            "Epoch 4: mean_squared_error improved from 17.66681 to 15.68045, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 16.3564 - mean_squared_error: 15.6805 - val_loss: 11.6077 - val_mean_squared_error: 10.8892\n",
            "Epoch 5/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 14.9098 - mean_squared_error: 14.2063\n",
            "Epoch 5: mean_squared_error improved from 15.68045 to 14.04616, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 14.7555 - mean_squared_error: 14.0462 - val_loss: 9.9782 - val_mean_squared_error: 9.1986\n",
            "Epoch 6/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 13.5619 - mean_squared_error: 12.8193\n",
            "Epoch 6: mean_squared_error improved from 14.04616 to 12.62838, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 13.3769 - mean_squared_error: 12.6284 - val_loss: 8.4881 - val_mean_squared_error: 7.6429\n",
            "Epoch 7/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 12.3509 - mean_squared_error: 11.5737\n",
            "Epoch 7: mean_squared_error improved from 12.62838 to 11.47891, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 12.2598 - mean_squared_error: 11.4789 - val_loss: 7.6141 - val_mean_squared_error: 6.7100\n",
            "Epoch 8/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 11.2106 - mean_squared_error: 10.3904\n",
            "Epoch 8: mean_squared_error improved from 11.47891 to 10.24968, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 11.0752 - mean_squared_error: 10.2497 - val_loss: 7.0804 - val_mean_squared_error: 6.1266\n",
            "Epoch 9/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 10.2885 - mean_squared_error: 9.4308\n",
            "Epoch 9: mean_squared_error improved from 10.24968 to 9.35045, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 10.2125 - mean_squared_error: 9.3504 - val_loss: 6.7020 - val_mean_squared_error: 5.6988\n",
            "Epoch 10/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 9.6783 - mean_squared_error: 8.7755\n",
            "Epoch 10: mean_squared_error improved from 9.35045 to 8.63293, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 9.5404 - mean_squared_error: 8.6329 - val_loss: 6.4470 - val_mean_squared_error: 5.3988\n",
            "Epoch 11/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 9.0198 - mean_squared_error: 8.0803\n",
            "Epoch 11: mean_squared_error improved from 8.63293 to 8.12517, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 9.0699 - mean_squared_error: 8.1252 - val_loss: 6.2545 - val_mean_squared_error: 5.1611\n",
            "Epoch 12/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 8.4798 - mean_squared_error: 7.4958\n",
            "Epoch 12: mean_squared_error improved from 8.12517 to 7.42358, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 8.4120 - mean_squared_error: 7.4236 - val_loss: 6.1126 - val_mean_squared_error: 4.9748\n",
            "Epoch 13/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 8.0826 - mean_squared_error: 7.0603\n",
            "Epoch 13: mean_squared_error improved from 7.42358 to 6.96539, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 7.9919 - mean_squared_error: 6.9654 - val_loss: 6.0008 - val_mean_squared_error: 4.8251\n",
            "Epoch 14/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 7.6651 - mean_squared_error: 6.6106\n",
            "Epoch 14: mean_squared_error improved from 6.96539 to 6.61752, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 7.6753 - mean_squared_error: 6.6175 - val_loss: 5.9086 - val_mean_squared_error: 4.6978\n",
            "Epoch 15/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 7.4260 - mean_squared_error: 6.3302\n",
            "Epoch 15: mean_squared_error improved from 6.61752 to 6.26802, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 7.3645 - mean_squared_error: 6.2680 - val_loss: 5.8320 - val_mean_squared_error: 4.5886\n",
            "Epoch 16/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 7.1676 - mean_squared_error: 6.0420\n",
            "Epoch 16: mean_squared_error improved from 6.26802 to 6.02167, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 7.1467 - mean_squared_error: 6.0217 - val_loss: 5.7594 - val_mean_squared_error: 4.4930\n",
            "Epoch 17/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 7.0927 - mean_squared_error: 5.9441\n",
            "Epoch 17: mean_squared_error improved from 6.02167 to 5.84322, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 6.9946 - mean_squared_error: 5.8432 - val_loss: 5.6877 - val_mean_squared_error: 4.4028\n",
            "Epoch 18/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 6.8488 - mean_squared_error: 5.6837\n",
            "Epoch 18: mean_squared_error improved from 5.84322 to 5.63498, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 6.8034 - mean_squared_error: 5.6350 - val_loss: 5.6156 - val_mean_squared_error: 4.3174\n",
            "Epoch 19/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 6.6577 - mean_squared_error: 5.4769\n",
            "Epoch 19: mean_squared_error improved from 5.63498 to 5.48920, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 6.6710 - mean_squared_error: 5.4892 - val_loss: 5.5469 - val_mean_squared_error: 4.2401\n",
            "Epoch 20/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 6.4591 - mean_squared_error: 5.2632\n",
            "Epoch 20: mean_squared_error improved from 5.48920 to 5.27272, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 6.4691 - mean_squared_error: 5.2727 - val_loss: 5.4708 - val_mean_squared_error: 4.1646\n",
            "Epoch 21/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 6.4029 - mean_squared_error: 5.2089\n",
            "Epoch 21: mean_squared_error improved from 5.27272 to 5.19294, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 6.3876 - mean_squared_error: 5.1929 - val_loss: 5.3971 - val_mean_squared_error: 4.0923\n",
            "Epoch 22/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 6.1979 - mean_squared_error: 5.0037\n",
            "Epoch 22: mean_squared_error improved from 5.19294 to 5.06510, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 6.2603 - mean_squared_error: 5.0651 - val_loss: 5.3221 - val_mean_squared_error: 4.0208\n",
            "Epoch 23/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 6.1014 - mean_squared_error: 4.9134\n",
            "Epoch 23: mean_squared_error improved from 5.06510 to 4.93151, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 6.1210 - mean_squared_error: 4.9315 - val_loss: 5.2425 - val_mean_squared_error: 3.9525\n",
            "Epoch 24/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 5.9651 - mean_squared_error: 4.7774\n",
            "Epoch 24: mean_squared_error improved from 4.93151 to 4.86923, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 6.0536 - mean_squared_error: 4.8692 - val_loss: 5.1682 - val_mean_squared_error: 3.8843\n",
            "Epoch 25/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 5.9564 - mean_squared_error: 4.7754\n",
            "Epoch 25: mean_squared_error improved from 4.86923 to 4.74981, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 5.9337 - mean_squared_error: 4.7498 - val_loss: 5.0915 - val_mean_squared_error: 3.8175\n",
            "Epoch 26/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 5.7280 - mean_squared_error: 4.5523\n",
            "Epoch 26: mean_squared_error improved from 4.74981 to 4.65130, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 5.8247 - mean_squared_error: 4.6513 - val_loss: 5.0170 - val_mean_squared_error: 3.7519\n",
            "Epoch 27/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 5.7432 - mean_squared_error: 4.5757\n",
            "Epoch 27: mean_squared_error improved from 4.65130 to 4.56384, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 5.7323 - mean_squared_error: 4.5638 - val_loss: 4.9393 - val_mean_squared_error: 3.6889\n",
            "Epoch 28/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 5.6952 - mean_squared_error: 4.5404\n",
            "Epoch 28: mean_squared_error improved from 4.56384 to 4.44488, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 5.5995 - mean_squared_error: 4.4449 - val_loss: 4.8613 - val_mean_squared_error: 3.6255\n",
            "Epoch 29/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 5.5513 - mean_squared_error: 4.4033\n",
            "Epoch 29: mean_squared_error improved from 4.44488 to 4.34621, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 5.4896 - mean_squared_error: 4.3462 - val_loss: 4.7842 - val_mean_squared_error: 3.5634\n",
            "Epoch 30/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 5.3532 - mean_squared_error: 4.2205\n",
            "Epoch 30: mean_squared_error improved from 4.34621 to 4.27207, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 5.4033 - mean_squared_error: 4.2721 - val_loss: 4.7085 - val_mean_squared_error: 3.5022\n",
            "Epoch 31/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 5.3013 - mean_squared_error: 4.1810\n",
            "Epoch 31: mean_squared_error improved from 4.27207 to 4.16973, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 5.2894 - mean_squared_error: 4.1697 - val_loss: 4.6324 - val_mean_squared_error: 3.4427\n",
            "Epoch 32/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 5.1322 - mean_squared_error: 4.0274\n",
            "Epoch 32: mean_squared_error improved from 4.16973 to 4.11192, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 5.2154 - mean_squared_error: 4.1119 - val_loss: 4.5582 - val_mean_squared_error: 3.3839\n",
            "Epoch 33/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 5.0365 - mean_squared_error: 3.9441\n",
            "Epoch 33: mean_squared_error improved from 4.11192 to 3.98408, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 5.0762 - mean_squared_error: 3.9841 - val_loss: 4.4816 - val_mean_squared_error: 3.3260\n",
            "Epoch 34/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 5.0724 - mean_squared_error: 3.9921\n",
            "Epoch 34: mean_squared_error improved from 3.98408 to 3.96713, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 5.0478 - mean_squared_error: 3.9671 - val_loss: 4.4117 - val_mean_squared_error: 3.2690\n",
            "Epoch 35/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 4.8897 - mean_squared_error: 3.8207\n",
            "Epoch 35: mean_squared_error improved from 3.96713 to 3.87062, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 4.9380 - mean_squared_error: 3.8706 - val_loss: 4.3416 - val_mean_squared_error: 3.2128\n",
            "Epoch 36/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 4.8807 - mean_squared_error: 3.8230\n",
            "Epoch 36: mean_squared_error improved from 3.87062 to 3.80953, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 4.8670 - mean_squared_error: 3.8095 - val_loss: 4.2744 - val_mean_squared_error: 3.1573\n",
            "Epoch 37/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 4.8123 - mean_squared_error: 3.7649\n",
            "Epoch 37: mean_squared_error improved from 3.80953 to 3.74137, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 4.7880 - mean_squared_error: 3.7414 - val_loss: 4.2054 - val_mean_squared_error: 3.1028\n",
            "Epoch 38/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 4.6274 - mean_squared_error: 3.5938\n",
            "Epoch 38: mean_squared_error improved from 3.74137 to 3.62202, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 4.6550 - mean_squared_error: 3.6220 - val_loss: 4.1351 - val_mean_squared_error: 3.0495\n",
            "Epoch 39/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 4.5016 - mean_squared_error: 3.4802\n",
            "Epoch 39: mean_squared_error improved from 3.62202 to 3.56467, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 4.5846 - mean_squared_error: 3.5647 - val_loss: 4.0686 - val_mean_squared_error: 2.9964\n",
            "Epoch 40/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 4.4417 - mean_squared_error: 3.4343\n",
            "Epoch 40: mean_squared_error improved from 3.56467 to 3.49906, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 4.5064 - mean_squared_error: 3.4991 - val_loss: 4.0009 - val_mean_squared_error: 2.9449\n",
            "Epoch 41/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 4.4305 - mean_squared_error: 3.4292\n",
            "Epoch 41: mean_squared_error improved from 3.49906 to 3.44910, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 4.4495 - mean_squared_error: 3.4491 - val_loss: 3.9394 - val_mean_squared_error: 2.8934\n",
            "Epoch 42/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 4.3495 - mean_squared_error: 3.3611\n",
            "Epoch 42: mean_squared_error improved from 3.44910 to 3.35214, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 4.3396 - mean_squared_error: 3.3521 - val_loss: 3.8757 - val_mean_squared_error: 2.8425\n",
            "Epoch 43/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 4.2985 - mean_squared_error: 3.3206\n",
            "Epoch 43: mean_squared_error improved from 3.35214 to 3.30657, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 4.2843 - mean_squared_error: 3.3066 - val_loss: 3.8146 - val_mean_squared_error: 2.7925\n",
            "Epoch 44/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 4.2218 - mean_squared_error: 3.2531\n",
            "Epoch 44: mean_squared_error improved from 3.30657 to 3.23413, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 4.2017 - mean_squared_error: 3.2341 - val_loss: 3.7542 - val_mean_squared_error: 2.7435\n",
            "Epoch 45/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 4.0665 - mean_squared_error: 3.1050\n",
            "Epoch 45: mean_squared_error improved from 3.23413 to 3.17521, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 4.1349 - mean_squared_error: 3.1752 - val_loss: 3.6966 - val_mean_squared_error: 2.6937\n",
            "Epoch 46/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 4.1260 - mean_squared_error: 3.1738\n",
            "Epoch 46: mean_squared_error improved from 3.17521 to 3.13721, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 4.0886 - mean_squared_error: 3.1372 - val_loss: 3.6414 - val_mean_squared_error: 2.6452\n",
            "Epoch 47/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 4.0581 - mean_squared_error: 3.1084\n",
            "Epoch 47: mean_squared_error improved from 3.13721 to 3.04948, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.9977 - mean_squared_error: 3.0495 - val_loss: 3.5805 - val_mean_squared_error: 2.5986\n",
            "Epoch 48/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 3.9442 - mean_squared_error: 3.0099\n",
            "Epoch 48: mean_squared_error improved from 3.04948 to 2.97510, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.9081 - mean_squared_error: 2.9751 - val_loss: 3.5195 - val_mean_squared_error: 2.5527\n",
            "Epoch 49/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 3.8560 - mean_squared_error: 2.9289\n",
            "Epoch 49: mean_squared_error improved from 2.97510 to 2.94275, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 3.8690 - mean_squared_error: 2.9428 - val_loss: 3.4638 - val_mean_squared_error: 2.5064\n",
            "Epoch 50/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 3.7514 - mean_squared_error: 2.8334\n",
            "Epoch 50: mean_squared_error improved from 2.94275 to 2.88567, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.8028 - mean_squared_error: 2.8857 - val_loss: 3.4121 - val_mean_squared_error: 2.4596\n",
            "Epoch 51/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 3.6734 - mean_squared_error: 2.7640\n",
            "Epoch 51: mean_squared_error improved from 2.88567 to 2.80802, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.7168 - mean_squared_error: 2.8080 - val_loss: 3.3575 - val_mean_squared_error: 2.4149\n",
            "Epoch 52/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 3.6915 - mean_squared_error: 2.7902\n",
            "Epoch 52: mean_squared_error improved from 2.80802 to 2.77373, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.6751 - mean_squared_error: 2.7737 - val_loss: 3.3076 - val_mean_squared_error: 2.3700\n",
            "Epoch 53/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 3.5168 - mean_squared_error: 2.6193\n",
            "Epoch 53: mean_squared_error improved from 2.77373 to 2.70637, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.6025 - mean_squared_error: 2.7064 - val_loss: 3.2544 - val_mean_squared_error: 2.3265\n",
            "Epoch 54/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 3.6021 - mean_squared_error: 2.7106\n",
            "Epoch 54: mean_squared_error improved from 2.70637 to 2.65688, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.5482 - mean_squared_error: 2.6569 - val_loss: 3.2009 - val_mean_squared_error: 2.2847\n",
            "Epoch 55/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 3.5196 - mean_squared_error: 2.6419\n",
            "Epoch 55: mean_squared_error improved from 2.65688 to 2.61253, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.4902 - mean_squared_error: 2.6125 - val_loss: 3.1513 - val_mean_squared_error: 2.2415\n",
            "Epoch 56/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 3.4689 - mean_squared_error: 2.5944\n",
            "Epoch 56: mean_squared_error improved from 2.61253 to 2.56350, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 3.4377 - mean_squared_error: 2.5635 - val_loss: 3.1031 - val_mean_squared_error: 2.1990\n",
            "Epoch 57/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 3.3146 - mean_squared_error: 2.4456\n",
            "Epoch 57: mean_squared_error improved from 2.56350 to 2.51068, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.3788 - mean_squared_error: 2.5107 - val_loss: 3.0540 - val_mean_squared_error: 2.1576\n",
            "Epoch 58/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 3.3346 - mean_squared_error: 2.4736\n",
            "Epoch 58: mean_squared_error improved from 2.51068 to 2.45424, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.3154 - mean_squared_error: 2.4542 - val_loss: 3.0062 - val_mean_squared_error: 2.1171\n",
            "Epoch 59/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 3.2075 - mean_squared_error: 2.3506\n",
            "Epoch 59: mean_squared_error improved from 2.45424 to 2.40523, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.2616 - mean_squared_error: 2.4052 - val_loss: 2.9585 - val_mean_squared_error: 2.0769\n",
            "Epoch 60/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 3.1701 - mean_squared_error: 2.3176\n",
            "Epoch 60: mean_squared_error improved from 2.40523 to 2.35820, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.2099 - mean_squared_error: 2.3582 - val_loss: 2.9119 - val_mean_squared_error: 2.0376\n",
            "Epoch 61/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 3.1584 - mean_squared_error: 2.3122\n",
            "Epoch 61: mean_squared_error improved from 2.35820 to 2.32576, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.1717 - mean_squared_error: 2.3258 - val_loss: 2.8687 - val_mean_squared_error: 1.9982\n",
            "Epoch 62/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 3.1276 - mean_squared_error: 2.2855\n",
            "Epoch 62: mean_squared_error improved from 2.32576 to 2.26426, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.1058 - mean_squared_error: 2.2643 - val_loss: 2.8231 - val_mean_squared_error: 1.9603\n",
            "Epoch 63/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 3.0759 - mean_squared_error: 2.2402\n",
            "Epoch 63: mean_squared_error improved from 2.26426 to 2.22441, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.0594 - mean_squared_error: 2.2244 - val_loss: 2.7792 - val_mean_squared_error: 1.9226\n",
            "Epoch 64/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.9976 - mean_squared_error: 2.1681\n",
            "Epoch 64: mean_squared_error improved from 2.22441 to 2.17102, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.0008 - mean_squared_error: 2.1710 - val_loss: 2.7367 - val_mean_squared_error: 1.8854\n",
            "Epoch 65/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.9306 - mean_squared_error: 2.1051\n",
            "Epoch 65: mean_squared_error improved from 2.17102 to 2.13627, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.9612 - mean_squared_error: 2.1363 - val_loss: 2.6948 - val_mean_squared_error: 1.8489\n",
            "Epoch 66/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.8955 - mean_squared_error: 2.0748\n",
            "Epoch 66: mean_squared_error improved from 2.13627 to 2.09145, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.9114 - mean_squared_error: 2.0914 - val_loss: 2.6537 - val_mean_squared_error: 1.8135\n",
            "Epoch 67/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 2.9338 - mean_squared_error: 2.1171\n",
            "Epoch 67: mean_squared_error improved from 2.09145 to 2.04536, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.8592 - mean_squared_error: 2.0454 - val_loss: 2.6119 - val_mean_squared_error: 1.7786\n",
            "Epoch 68/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.8155 - mean_squared_error: 2.0059\n",
            "Epoch 68: mean_squared_error improved from 2.04536 to 1.99675, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.8061 - mean_squared_error: 1.9967 - val_loss: 2.5702 - val_mean_squared_error: 1.7450\n",
            "Epoch 69/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.7474 - mean_squared_error: 1.9435\n",
            "Epoch 69: mean_squared_error improved from 1.99675 to 1.96385, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.7666 - mean_squared_error: 1.9638 - val_loss: 2.5320 - val_mean_squared_error: 1.7104\n",
            "Epoch 70/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.7258 - mean_squared_error: 1.9254\n",
            "Epoch 70: mean_squared_error improved from 1.96385 to 1.92398, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.7250 - mean_squared_error: 1.9240 - val_loss: 2.4964 - val_mean_squared_error: 1.6759\n",
            "Epoch 71/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.6903 - mean_squared_error: 1.8920\n",
            "Epoch 71: mean_squared_error improved from 1.92398 to 1.88085, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.6780 - mean_squared_error: 1.8809 - val_loss: 2.4564 - val_mean_squared_error: 1.6442\n",
            "Epoch 72/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 2.5995 - mean_squared_error: 1.8056\n",
            "Epoch 72: mean_squared_error improved from 1.88085 to 1.84798, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.6401 - mean_squared_error: 1.8480 - val_loss: 2.4200 - val_mean_squared_error: 1.6117\n",
            "Epoch 73/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.6058 - mean_squared_error: 1.8176\n",
            "Epoch 73: mean_squared_error improved from 1.84798 to 1.80620, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.5943 - mean_squared_error: 1.8062 - val_loss: 2.3853 - val_mean_squared_error: 1.5797\n",
            "Epoch 74/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.5627 - mean_squared_error: 1.7790\n",
            "Epoch 74: mean_squared_error improved from 1.80620 to 1.75557, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.5391 - mean_squared_error: 1.7556 - val_loss: 2.3471 - val_mean_squared_error: 1.5505\n",
            "Epoch 75/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.5376 - mean_squared_error: 1.7615\n",
            "Epoch 75: mean_squared_error improved from 1.75557 to 1.73254, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.5084 - mean_squared_error: 1.7325 - val_loss: 2.3141 - val_mean_squared_error: 1.5195\n",
            "Epoch 76/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.4523 - mean_squared_error: 1.6772\n",
            "Epoch 76: mean_squared_error improved from 1.73254 to 1.68179, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 2.4561 - mean_squared_error: 1.6818 - val_loss: 2.2768 - val_mean_squared_error: 1.4908\n",
            "Epoch 77/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 2.4541 - mean_squared_error: 1.6875\n",
            "Epoch 77: mean_squared_error improved from 1.68179 to 1.64588, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 2.4131 - mean_squared_error: 1.6459 - val_loss: 2.2430 - val_mean_squared_error: 1.4620\n",
            "Epoch 78/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 2.3395 - mean_squared_error: 1.5766\n",
            "Epoch 78: mean_squared_error improved from 1.64588 to 1.62212, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 2.3840 - mean_squared_error: 1.6221 - val_loss: 2.2121 - val_mean_squared_error: 1.4321\n",
            "Epoch 79/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 2.2959 - mean_squared_error: 1.5340\n",
            "Epoch 79: mean_squared_error improved from 1.62212 to 1.57186, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 2.3318 - mean_squared_error: 1.5719 - val_loss: 2.1765 - val_mean_squared_error: 1.4062\n",
            "Epoch 80/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.2932 - mean_squared_error: 1.5400\n",
            "Epoch 80: mean_squared_error improved from 1.57186 to 1.56435, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.3178 - mean_squared_error: 1.5643 - val_loss: 2.1478 - val_mean_squared_error: 1.3777\n",
            "Epoch 81/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 2.2864 - mean_squared_error: 1.5321\n",
            "Epoch 81: mean_squared_error improved from 1.56435 to 1.52867, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 2.2810 - mean_squared_error: 1.5287 - val_loss: 2.1202 - val_mean_squared_error: 1.3495\n",
            "Epoch 82/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.2870 - mean_squared_error: 1.5372\n",
            "Epoch 82: mean_squared_error improved from 1.52867 to 1.47434, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 2.2237 - mean_squared_error: 1.4743 - val_loss: 2.0855 - val_mean_squared_error: 1.3257\n",
            "Epoch 83/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.1362 - mean_squared_error: 1.3982\n",
            "Epoch 83: mean_squared_error improved from 1.47434 to 1.44590, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.1848 - mean_squared_error: 1.4459 - val_loss: 2.0556 - val_mean_squared_error: 1.2997\n",
            "Epoch 84/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 2.1133 - mean_squared_error: 1.3750\n",
            "Epoch 84: mean_squared_error improved from 1.44590 to 1.42302, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 2.1618 - mean_squared_error: 1.4230 - val_loss: 2.0268 - val_mean_squared_error: 1.2750\n",
            "Epoch 85/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.1693 - mean_squared_error: 1.4322\n",
            "Epoch 85: mean_squared_error improved from 1.42302 to 1.39262, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 2.1288 - mean_squared_error: 1.3926 - val_loss: 1.9989 - val_mean_squared_error: 1.2511\n",
            "Epoch 86/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 2.1664 - mean_squared_error: 1.4350\n",
            "Epoch 86: mean_squared_error improved from 1.39262 to 1.36663, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 2.0979 - mean_squared_error: 1.3666 - val_loss: 1.9712 - val_mean_squared_error: 1.2277\n",
            "Epoch 87/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 2.1660 - mean_squared_error: 1.4372\n",
            "Epoch 87: mean_squared_error improved from 1.36663 to 1.33660, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 2.0649 - mean_squared_error: 1.3366 - val_loss: 1.9448 - val_mean_squared_error: 1.2042\n",
            "Epoch 88/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 2.0769 - mean_squared_error: 1.3536\n",
            "Epoch 88: mean_squared_error improved from 1.33660 to 1.30853, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 2.0304 - mean_squared_error: 1.3085 - val_loss: 1.9197 - val_mean_squared_error: 1.1807\n",
            "Epoch 89/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.0352 - mean_squared_error: 1.3134\n",
            "Epoch 89: mean_squared_error improved from 1.30853 to 1.28103, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 2.0018 - mean_squared_error: 1.2810 - val_loss: 1.8942 - val_mean_squared_error: 1.1589\n",
            "Epoch 90/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.9457 - mean_squared_error: 1.2290\n",
            "Epoch 90: mean_squared_error improved from 1.28103 to 1.25481, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.9718 - mean_squared_error: 1.2548 - val_loss: 1.8695 - val_mean_squared_error: 1.1370\n",
            "Epoch 91/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.9346 - mean_squared_error: 1.2221\n",
            "Epoch 91: mean_squared_error improved from 1.25481 to 1.22019, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.9319 - mean_squared_error: 1.2202 - val_loss: 1.8445 - val_mean_squared_error: 1.1167\n",
            "Epoch 92/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.9259 - mean_squared_error: 1.2152\n",
            "Epoch 92: mean_squared_error improved from 1.22019 to 1.20272, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.9133 - mean_squared_error: 1.2027 - val_loss: 1.8227 - val_mean_squared_error: 1.0962\n",
            "Epoch 93/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.8807 - mean_squared_error: 1.1728\n",
            "Epoch 93: mean_squared_error improved from 1.20272 to 1.16821, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.8754 - mean_squared_error: 1.1682 - val_loss: 1.7974 - val_mean_squared_error: 1.0774\n",
            "Epoch 94/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.8182 - mean_squared_error: 1.1192\n",
            "Epoch 94: mean_squared_error improved from 1.16821 to 1.14290, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.8420 - mean_squared_error: 1.1429 - val_loss: 1.7719 - val_mean_squared_error: 1.0589\n",
            "Epoch 95/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.7945 - mean_squared_error: 1.1002\n",
            "Epoch 95: mean_squared_error improved from 1.14290 to 1.12942, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.8238 - mean_squared_error: 1.1294 - val_loss: 1.7511 - val_mean_squared_error: 1.0403\n",
            "Epoch 96/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.7900 - mean_squared_error: 1.0965\n",
            "Epoch 96: mean_squared_error improved from 1.12942 to 1.10653, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.8002 - mean_squared_error: 1.1065 - val_loss: 1.7308 - val_mean_squared_error: 1.0223\n",
            "Epoch 97/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.7553 - mean_squared_error: 1.0648\n",
            "Epoch 97: mean_squared_error improved from 1.10653 to 1.07416, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.7644 - mean_squared_error: 1.0742 - val_loss: 1.7083 - val_mean_squared_error: 1.0056\n",
            "Epoch 98/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.7475 - mean_squared_error: 1.0615\n",
            "Epoch 98: mean_squared_error improved from 1.07416 to 1.05835, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.7431 - mean_squared_error: 1.0583 - val_loss: 1.6865 - val_mean_squared_error: 0.9894\n",
            "Epoch 99/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.6574 - mean_squared_error: 0.9772\n",
            "Epoch 99: mean_squared_error improved from 1.05835 to 1.03803, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.7182 - mean_squared_error: 1.0380 - val_loss: 1.6670 - val_mean_squared_error: 0.9726\n",
            "Epoch 100/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.7174 - mean_squared_error: 1.0399\n",
            "Epoch 100: mean_squared_error improved from 1.03803 to 1.01750, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.6935 - mean_squared_error: 1.0175 - val_loss: 1.6472 - val_mean_squared_error: 0.9582\n",
            "Epoch 101/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.6758 - mean_squared_error: 1.0023\n",
            "Epoch 101: mean_squared_error improved from 1.01750 to 0.99530, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.6679 - mean_squared_error: 0.9953 - val_loss: 1.6274 - val_mean_squared_error: 0.9443\n",
            "Epoch 102/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.6308 - mean_squared_error: 0.9673\n",
            "Epoch 102: mean_squared_error improved from 0.99530 to 0.97107, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.6349 - mean_squared_error: 0.9711 - val_loss: 1.6083 - val_mean_squared_error: 0.9296\n",
            "Epoch 103/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.6392 - mean_squared_error: 0.9784\n",
            "Epoch 103: mean_squared_error improved from 0.97107 to 0.95434, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.6150 - mean_squared_error: 0.9543 - val_loss: 1.5884 - val_mean_squared_error: 0.9168\n",
            "Epoch 104/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.6284 - mean_squared_error: 0.9684\n",
            "Epoch 104: mean_squared_error improved from 0.95434 to 0.94700, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.6064 - mean_squared_error: 0.9470 - val_loss: 1.5727 - val_mean_squared_error: 0.9033\n",
            "Epoch 105/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.5954 - mean_squared_error: 0.9380\n",
            "Epoch 105: mean_squared_error improved from 0.94700 to 0.91913, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.5746 - mean_squared_error: 0.9191 - val_loss: 1.5553 - val_mean_squared_error: 0.8914\n",
            "Epoch 106/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.5794 - mean_squared_error: 0.9253\n",
            "Epoch 106: mean_squared_error improved from 0.91913 to 0.91492, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.5672 - mean_squared_error: 0.9149 - val_loss: 1.5399 - val_mean_squared_error: 0.8791\n",
            "Epoch 107/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.5538 - mean_squared_error: 0.9095\n",
            "Epoch 107: mean_squared_error improved from 0.91492 to 0.88364, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.5281 - mean_squared_error: 0.8836 - val_loss: 1.5241 - val_mean_squared_error: 0.8680\n",
            "Epoch 108/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.5077 - mean_squared_error: 0.8657\n",
            "Epoch 108: mean_squared_error improved from 0.88364 to 0.86764, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.5099 - mean_squared_error: 0.8676 - val_loss: 1.5056 - val_mean_squared_error: 0.8585\n",
            "Epoch 109/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.4930 - mean_squared_error: 0.8608\n",
            "Epoch 109: mean_squared_error improved from 0.86764 to 0.84964, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.4827 - mean_squared_error: 0.8496 - val_loss: 1.4882 - val_mean_squared_error: 0.8489\n",
            "Epoch 110/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.5075 - mean_squared_error: 0.8755\n",
            "Epoch 110: mean_squared_error did not improve from 0.84964\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.4808 - mean_squared_error: 0.8497 - val_loss: 1.4770 - val_mean_squared_error: 0.8381\n",
            "Epoch 111/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.4890 - mean_squared_error: 0.8587\n",
            "Epoch 111: mean_squared_error improved from 0.84964 to 0.83116, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.4602 - mean_squared_error: 0.8312 - val_loss: 1.4642 - val_mean_squared_error: 0.8290\n",
            "Epoch 112/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.4431 - mean_squared_error: 0.8211\n",
            "Epoch 112: mean_squared_error improved from 0.83116 to 0.82031, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.4434 - mean_squared_error: 0.8203 - val_loss: 1.4522 - val_mean_squared_error: 0.8201\n",
            "Epoch 113/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.3859 - mean_squared_error: 0.7647\n",
            "Epoch 113: mean_squared_error improved from 0.82031 to 0.79987, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.4197 - mean_squared_error: 0.7999 - val_loss: 1.4337 - val_mean_squared_error: 0.8132\n",
            "Epoch 114/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.3996 - mean_squared_error: 0.7874\n",
            "Epoch 114: mean_squared_error improved from 0.79987 to 0.79643, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.4076 - mean_squared_error: 0.7964 - val_loss: 1.4227 - val_mean_squared_error: 0.8061\n",
            "Epoch 115/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.3843 - mean_squared_error: 0.7773\n",
            "Epoch 115: mean_squared_error improved from 0.79643 to 0.77966, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.3873 - mean_squared_error: 0.7797 - val_loss: 1.4119 - val_mean_squared_error: 0.7993\n",
            "Epoch 116/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.4120 - mean_squared_error: 0.8073\n",
            "Epoch 116: mean_squared_error improved from 0.77966 to 0.77701, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3828 - mean_squared_error: 0.7770 - val_loss: 1.4025 - val_mean_squared_error: 0.7930\n",
            "Epoch 117/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.3408 - mean_squared_error: 0.7429\n",
            "Epoch 117: mean_squared_error improved from 0.77701 to 0.75351, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3520 - mean_squared_error: 0.7535 - val_loss: 1.3887 - val_mean_squared_error: 0.7876\n",
            "Epoch 118/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.3920 - mean_squared_error: 0.7951\n",
            "Epoch 118: mean_squared_error did not improve from 0.75351\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3583 - mean_squared_error: 0.7604 - val_loss: 1.3819 - val_mean_squared_error: 0.7826\n",
            "Epoch 119/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.3389 - mean_squared_error: 0.7436\n",
            "Epoch 119: mean_squared_error improved from 0.75351 to 0.73920, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.3339 - mean_squared_error: 0.7392 - val_loss: 1.3692 - val_mean_squared_error: 0.7784\n",
            "Epoch 120/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.3334 - mean_squared_error: 0.7471\n",
            "Epoch 120: mean_squared_error improved from 0.73920 to 0.72135, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.3074 - mean_squared_error: 0.7214 - val_loss: 1.3552 - val_mean_squared_error: 0.7747\n",
            "Epoch 121/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.3045 - mean_squared_error: 0.7237\n",
            "Epoch 121: mean_squared_error did not improve from 0.72135\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3054 - mean_squared_error: 0.7264 - val_loss: 1.3476 - val_mean_squared_error: 0.7710\n",
            "Epoch 122/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.2747 - mean_squared_error: 0.6964\n",
            "Epoch 122: mean_squared_error improved from 0.72135 to 0.70841, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2847 - mean_squared_error: 0.7084 - val_loss: 1.3382 - val_mean_squared_error: 0.7683\n",
            "Epoch 123/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.2945 - mean_squared_error: 0.7223\n",
            "Epoch 123: mean_squared_error did not improve from 0.70841\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2864 - mean_squared_error: 0.7147 - val_loss: 1.3322 - val_mean_squared_error: 0.7656\n",
            "Epoch 124/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.3142 - mean_squared_error: 0.7462\n",
            "Epoch 124: mean_squared_error improved from 0.70841 to 0.70141, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2687 - mean_squared_error: 0.7014 - val_loss: 1.3217 - val_mean_squared_error: 0.7635\n",
            "Epoch 125/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.2482 - mean_squared_error: 0.6876\n",
            "Epoch 125: mean_squared_error improved from 0.70141 to 0.69839, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.2597 - mean_squared_error: 0.6984 - val_loss: 1.3142 - val_mean_squared_error: 0.7619\n",
            "Epoch 126/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.2207 - mean_squared_error: 0.6589\n",
            "Epoch 126: mean_squared_error improved from 0.69839 to 0.68169, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.2388 - mean_squared_error: 0.6817 - val_loss: 1.3049 - val_mean_squared_error: 0.7604\n",
            "Epoch 127/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.2407 - mean_squared_error: 0.6872\n",
            "Epoch 127: mean_squared_error improved from 0.68169 to 0.67860, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.2294 - mean_squared_error: 0.6786 - val_loss: 1.2947 - val_mean_squared_error: 0.7591\n",
            "Epoch 128/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.2397 - mean_squared_error: 0.6964\n",
            "Epoch 128: mean_squared_error did not improve from 0.67860\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2254 - mean_squared_error: 0.6799 - val_loss: 1.2898 - val_mean_squared_error: 0.7587\n",
            "Epoch 129/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.2005 - mean_squared_error: 0.6640\n",
            "Epoch 129: mean_squared_error improved from 0.67860 to 0.66466, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2037 - mean_squared_error: 0.6647 - val_loss: 1.2809 - val_mean_squared_error: 0.7580\n",
            "Epoch 130/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.1813 - mean_squared_error: 0.6454\n",
            "Epoch 130: mean_squared_error did not improve from 0.66466\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2079 - mean_squared_error: 0.6737 - val_loss: 1.2776 - val_mean_squared_error: 0.7581\n",
            "Epoch 131/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.2254 - mean_squared_error: 0.6973\n",
            "Epoch 131: mean_squared_error did not improve from 0.66466\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2143 - mean_squared_error: 0.6838 - val_loss: 1.2750 - val_mean_squared_error: 0.7593\n",
            "Epoch 132/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.1578 - mean_squared_error: 0.6287\n",
            "Epoch 132: mean_squared_error improved from 0.66466 to 0.65417, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.1823 - mean_squared_error: 0.6542 - val_loss: 1.2698 - val_mean_squared_error: 0.7599\n",
            "Epoch 133/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.1715 - mean_squared_error: 0.6423\n",
            "Epoch 133: mean_squared_error did not improve from 0.65417\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1874 - mean_squared_error: 0.6626 - val_loss: 1.2652 - val_mean_squared_error: 0.7606\n",
            "Epoch 134/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.2210 - mean_squared_error: 0.6990\n",
            "Epoch 134: mean_squared_error did not improve from 0.65417\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1763 - mean_squared_error: 0.6561 - val_loss: 1.2566 - val_mean_squared_error: 0.7605\n",
            "Epoch 135/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.1367 - mean_squared_error: 0.6219\n",
            "Epoch 135: mean_squared_error did not improve from 0.65417\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.1746 - mean_squared_error: 0.6601 - val_loss: 1.2550 - val_mean_squared_error: 0.7624\n",
            "Epoch 136/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.1244 - mean_squared_error: 0.6143\n",
            "Epoch 136: mean_squared_error improved from 0.65417 to 0.65173, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1595 - mean_squared_error: 0.6517 - val_loss: 1.2518 - val_mean_squared_error: 0.7634\n",
            "Epoch 137/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.1513 - mean_squared_error: 0.6372\n",
            "Epoch 137: mean_squared_error did not improve from 0.65173\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1722 - mean_squared_error: 0.6612 - val_loss: 1.2520 - val_mean_squared_error: 0.7660\n",
            "Epoch 138/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.1242 - mean_squared_error: 0.6212\n",
            "Epoch 138: mean_squared_error improved from 0.65173 to 0.64636, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.1495 - mean_squared_error: 0.6464 - val_loss: 1.2455 - val_mean_squared_error: 0.7654\n",
            "Epoch 139/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.1730 - mean_squared_error: 0.6701\n",
            "Epoch 139: mean_squared_error did not improve from 0.64636\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1544 - mean_squared_error: 0.6548 - val_loss: 1.2474 - val_mean_squared_error: 0.7686\n",
            "Epoch 140/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.1604 - mean_squared_error: 0.6638\n",
            "Epoch 140: mean_squared_error improved from 0.64636 to 0.64468, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1431 - mean_squared_error: 0.6447 - val_loss: 1.2416 - val_mean_squared_error: 0.7686\n",
            "Epoch 141/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.1479 - mean_squared_error: 0.6545\n",
            "Epoch 141: mean_squared_error did not improve from 0.64468\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.1427 - mean_squared_error: 0.6493 - val_loss: 1.2405 - val_mean_squared_error: 0.7711\n",
            "Epoch 142/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.1379 - mean_squared_error: 0.6467\n",
            "Epoch 142: mean_squared_error did not improve from 0.64468\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.1356 - mean_squared_error: 0.6453 - val_loss: 1.2325 - val_mean_squared_error: 0.7702\n",
            "Epoch 143/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.1394 - mean_squared_error: 0.6575\n",
            "Epoch 143: mean_squared_error did not improve from 0.64468\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1362 - mean_squared_error: 0.6521 - val_loss: 1.2302 - val_mean_squared_error: 0.7715\n",
            "Epoch 144/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.1296 - mean_squared_error: 0.6510\n",
            "Epoch 144: mean_squared_error did not improve from 0.64468\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.1278 - mean_squared_error: 0.6462 - val_loss: 1.2271 - val_mean_squared_error: 0.7720\n",
            "Epoch 145/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.1180 - mean_squared_error: 0.6425\n",
            "Epoch 145: mean_squared_error improved from 0.64468 to 0.63073, saving model to optimizers_best_rmsprop-0001.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.1080 - mean_squared_error: 0.6307 - val_loss: 1.2199 - val_mean_squared_error: 0.7712\n",
            "Epoch 146/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.1088 - mean_squared_error: 0.6324\n",
            "Epoch 146: mean_squared_error did not improve from 0.63073\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.1102 - mean_squared_error: 0.6353 - val_loss: 1.2169 - val_mean_squared_error: 0.7719\n",
            "Epoch 147/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.1100 - mean_squared_error: 0.6369\n",
            "Epoch 147: mean_squared_error did not improve from 0.63073\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1035 - mean_squared_error: 0.6325 - val_loss: 1.2154 - val_mean_squared_error: 0.7741\n",
            "Epoch 148/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.1084 - mean_squared_error: 0.6407\n",
            "Epoch 148: mean_squared_error did not improve from 0.63073\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1108 - mean_squared_error: 0.6431 - val_loss: 1.2171 - val_mean_squared_error: 0.7775\n",
            "Epoch 149/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.1143 - mean_squared_error: 0.6492\n",
            "Epoch 149: mean_squared_error did not improve from 0.63073\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1053 - mean_squared_error: 0.6406 - val_loss: 1.2120 - val_mean_squared_error: 0.7770\n",
            "Epoch 150/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.0798 - mean_squared_error: 0.6139\n",
            "Epoch 150: mean_squared_error did not improve from 0.63073\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.1137 - mean_squared_error: 0.6499 - val_loss: 1.2116 - val_mean_squared_error: 0.7788\n",
            "Epoch 150: early stopping\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.9919 - mean_squared_error: 0.5497\n",
            "Epoch 1/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 31.4870 - mean_squared_error: 30.9302 \n",
            "Epoch 1: mean_squared_error improved from inf to 27.62367, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 3s 21ms/step - loss: 28.1924 - mean_squared_error: 27.6237 - val_loss: 14.9635 - val_mean_squared_error: 14.3224\n",
            "Epoch 2/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 15.0367 - mean_squared_error: 14.3932\n",
            "Epoch 2: mean_squared_error improved from 27.62367 to 13.21380, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 13.8803 - mean_squared_error: 13.2138 - val_loss: 7.2793 - val_mean_squared_error: 6.5006\n",
            "Epoch 3/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 8.7176 - mean_squared_error: 7.9341\n",
            "Epoch 3: mean_squared_error improved from 13.21380 to 7.68546, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 8.4847 - mean_squared_error: 7.6855 - val_loss: 5.3781 - val_mean_squared_error: 4.4931\n",
            "Epoch 4/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 6.6267 - mean_squared_error: 5.7429\n",
            "Epoch 4: mean_squared_error improved from 7.68546 to 5.56207, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 6.4589 - mean_squared_error: 5.5621 - val_loss: 4.4942 - val_mean_squared_error: 3.5380\n",
            "Epoch 5/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 5.6420 - mean_squared_error: 4.6902\n",
            "Epoch 5: mean_squared_error improved from 5.56207 to 4.52933, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.4872 - mean_squared_error: 4.5293 - val_loss: 3.9113 - val_mean_squared_error: 2.9140\n",
            "Epoch 6/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 4.7984 - mean_squared_error: 3.8126\n",
            "Epoch 6: mean_squared_error improved from 4.52933 to 3.75350, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 4.7441 - mean_squared_error: 3.7535 - val_loss: 3.4671 - val_mean_squared_error: 2.4414\n",
            "Epoch 7/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 4.2456 - mean_squared_error: 3.2285\n",
            "Epoch 7: mean_squared_error improved from 3.75350 to 3.10155, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.1229 - mean_squared_error: 3.1016 - val_loss: 3.1111 - val_mean_squared_error: 2.0528\n",
            "Epoch 8/1000\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 3.9128 - mean_squared_error: 2.8681\n",
            "Epoch 8: mean_squared_error improved from 3.10155 to 2.58207, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.6336 - mean_squared_error: 2.5821 - val_loss: 2.8359 - val_mean_squared_error: 1.7483\n",
            "Epoch 9/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 3.2323 - mean_squared_error: 2.1694\n",
            "Epoch 9: mean_squared_error improved from 2.58207 to 2.17171, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.2345 - mean_squared_error: 2.1717 - val_loss: 2.6035 - val_mean_squared_error: 1.5155\n",
            "Epoch 10/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 2.9010 - mean_squared_error: 1.8404\n",
            "Epoch 10: mean_squared_error improved from 2.17171 to 1.81708, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 2.8746 - mean_squared_error: 1.8171 - val_loss: 2.4052 - val_mean_squared_error: 1.3270\n",
            "Epoch 11/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.6577 - mean_squared_error: 1.6077\n",
            "Epoch 11: mean_squared_error improved from 1.81708 to 1.58878, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.6371 - mean_squared_error: 1.5888 - val_loss: 2.2471 - val_mean_squared_error: 1.1794\n",
            "Epoch 12/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 2.5295 - mean_squared_error: 1.4936\n",
            "Epoch 12: mean_squared_error improved from 1.58878 to 1.42909, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.4650 - mean_squared_error: 1.4291 - val_loss: 2.1229 - val_mean_squared_error: 1.0631\n",
            "Epoch 13/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 2.2916 - mean_squared_error: 1.2675\n",
            "Epoch 13: mean_squared_error improved from 1.42909 to 1.20791, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.2292 - mean_squared_error: 1.2079 - val_loss: 2.0129 - val_mean_squared_error: 0.9742\n",
            "Epoch 14/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 2.0883 - mean_squared_error: 1.0853\n",
            "Epoch 14: mean_squared_error improved from 1.20791 to 1.09988, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.1002 - mean_squared_error: 1.0999 - val_loss: 1.9206 - val_mean_squared_error: 0.9076\n",
            "Epoch 15/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.9774 - mean_squared_error: 0.9952\n",
            "Epoch 15: mean_squared_error improved from 1.09988 to 1.04388, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.0234 - mean_squared_error: 1.0439 - val_loss: 1.8515 - val_mean_squared_error: 0.8570\n",
            "Epoch 16/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.8829 - mean_squared_error: 0.9165\n",
            "Epoch 16: mean_squared_error improved from 1.04388 to 0.91901, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.8814 - mean_squared_error: 0.9190 - val_loss: 1.7928 - val_mean_squared_error: 0.8219\n",
            "Epoch 17/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.8725 - mean_squared_error: 0.9317\n",
            "Epoch 17: mean_squared_error improved from 0.91901 to 0.90477, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.8445 - mean_squared_error: 0.9048 - val_loss: 1.7449 - val_mean_squared_error: 0.7972\n",
            "Epoch 18/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.7990 - mean_squared_error: 0.8753\n",
            "Epoch 18: mean_squared_error improved from 0.90477 to 0.84287, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.7629 - mean_squared_error: 0.8429 - val_loss: 1.7055 - val_mean_squared_error: 0.7806\n",
            "Epoch 19/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.7193 - mean_squared_error: 0.8196\n",
            "Epoch 19: mean_squared_error improved from 0.84287 to 0.81289, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.7117 - mean_squared_error: 0.8129 - val_loss: 1.6753 - val_mean_squared_error: 0.7705\n",
            "Epoch 20/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.7053 - mean_squared_error: 0.8155\n",
            "Epoch 20: mean_squared_error improved from 0.81289 to 0.80937, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 1.6988 - mean_squared_error: 0.8094 - val_loss: 1.6660 - val_mean_squared_error: 0.7653\n",
            "Epoch 21/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.5949 - mean_squared_error: 0.7156\n",
            "Epoch 21: mean_squared_error improved from 0.80937 to 0.76259, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.6370 - mean_squared_error: 0.7626 - val_loss: 1.6365 - val_mean_squared_error: 0.7637\n",
            "Epoch 22/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.6276 - mean_squared_error: 0.7748\n",
            "Epoch 22: mean_squared_error improved from 0.76259 to 0.74773, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.5979 - mean_squared_error: 0.7477 - val_loss: 1.6147 - val_mean_squared_error: 0.7641\n",
            "Epoch 23/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.5651 - mean_squared_error: 0.7316\n",
            "Epoch 23: mean_squared_error improved from 0.74773 to 0.74158, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 1.5745 - mean_squared_error: 0.7416 - val_loss: 1.5965 - val_mean_squared_error: 0.7661\n",
            "Epoch 24/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 1.5687 - mean_squared_error: 0.7472\n",
            "Epoch 24: mean_squared_error did not improve from 0.74158\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.5789 - mean_squared_error: 0.7581 - val_loss: 1.5999 - val_mean_squared_error: 0.7703\n",
            "Epoch 25/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.5449 - mean_squared_error: 0.7295\n",
            "Epoch 25: mean_squared_error improved from 0.74158 to 0.71644, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 1.5285 - mean_squared_error: 0.7164 - val_loss: 1.5831 - val_mean_squared_error: 0.7736\n",
            "Epoch 26/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.5802 - mean_squared_error: 0.7818\n",
            "Epoch 26: mean_squared_error did not improve from 0.71644\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 1.5920 - mean_squared_error: 0.7941 - val_loss: 1.5784 - val_mean_squared_error: 0.7783\n",
            "Epoch 27/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.4944 - mean_squared_error: 0.7022\n",
            "Epoch 27: mean_squared_error improved from 0.71644 to 0.70404, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 1.4951 - mean_squared_error: 0.7040 - val_loss: 1.5642 - val_mean_squared_error: 0.7816\n",
            "Epoch 28/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 1.4496 - mean_squared_error: 0.6752\n",
            "Epoch 28: mean_squared_error did not improve from 0.70404\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.4909 - mean_squared_error: 0.7194 - val_loss: 1.5419 - val_mean_squared_error: 0.7825\n",
            "Epoch 29/1000\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.5088 - mean_squared_error: 0.7469\n",
            "Epoch 29: mean_squared_error did not improve from 0.70404\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 1.5088 - mean_squared_error: 0.7469 - val_loss: 1.5468 - val_mean_squared_error: 0.7875\n",
            "Epoch 30/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.4689 - mean_squared_error: 0.7141\n",
            "Epoch 30: mean_squared_error did not improve from 0.70404\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 1.4664 - mean_squared_error: 0.7119 - val_loss: 1.5360 - val_mean_squared_error: 0.7892\n",
            "Epoch 31/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.4635 - mean_squared_error: 0.7196\n",
            "Epoch 31: mean_squared_error did not improve from 0.70404\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1.4665 - mean_squared_error: 0.7229 - val_loss: 1.5299 - val_mean_squared_error: 0.7930\n",
            "Epoch 32/1000\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.4330 - mean_squared_error: 0.6986\n",
            "Epoch 32: mean_squared_error improved from 0.70404 to 0.69860, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 1.4330 - mean_squared_error: 0.6986 - val_loss: 1.5168 - val_mean_squared_error: 0.7934\n",
            "Epoch 33/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.4882 - mean_squared_error: 0.7618\n",
            "Epoch 33: mean_squared_error did not improve from 0.69860\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.4778 - mean_squared_error: 0.7517 - val_loss: 1.5135 - val_mean_squared_error: 0.7958\n",
            "Epoch 34/1000\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.4230 - mean_squared_error: 0.7045\n",
            "Epoch 34: mean_squared_error did not improve from 0.69860\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 1.4230 - mean_squared_error: 0.7045 - val_loss: 1.5027 - val_mean_squared_error: 0.7959\n",
            "Epoch 35/1000\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.4137 - mean_squared_error: 0.7029\n",
            "Epoch 35: mean_squared_error did not improve from 0.69860\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 1.4137 - mean_squared_error: 0.7029 - val_loss: 1.4842 - val_mean_squared_error: 0.7915\n",
            "Epoch 36/1000\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.3647 - mean_squared_error: 0.6683\n",
            "Epoch 36: mean_squared_error improved from 0.69860 to 0.66809, saving model to optimizers_best_nadam.h5\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1.3633 - mean_squared_error: 0.6681 - val_loss: 1.4501 - val_mean_squared_error: 0.7795\n",
            "Epoch 37/1000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.4022 - mean_squared_error: 0.7152\n",
            "Epoch 37: mean_squared_error did not improve from 0.66809\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.4065 - mean_squared_error: 0.7191 - val_loss: 1.4584 - val_mean_squared_error: 0.7871\n",
            "Epoch 38/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.3948 - mean_squared_error: 0.7066\n",
            "Epoch 38: mean_squared_error did not improve from 0.66809\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.4073 - mean_squared_error: 0.7162 - val_loss: 1.4963 - val_mean_squared_error: 0.8080\n",
            "Epoch 39/1000\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 1.3691 - mean_squared_error: 0.6719\n",
            "Epoch 39: mean_squared_error did not improve from 0.66809\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.3724 - mean_squared_error: 0.6807 - val_loss: 1.4648 - val_mean_squared_error: 0.7970\n",
            "Epoch 40/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.3652 - mean_squared_error: 0.6878\n",
            "Epoch 40: mean_squared_error did not improve from 0.66809\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3790 - mean_squared_error: 0.7038 - val_loss: 1.4554 - val_mean_squared_error: 0.7965\n",
            "Epoch 41/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.3583 - mean_squared_error: 0.6904\n",
            "Epoch 41: mean_squared_error did not improve from 0.66809\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3694 - mean_squared_error: 0.7030 - val_loss: 1.4420 - val_mean_squared_error: 0.7924\n",
            "Epoch 41: early stopping\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.2208 - mean_squared_error: 0.5547\n",
            "Epoch 1/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 23.7683 - mean_squared_error: 23.2345 \n",
            "Epoch 1: mean_squared_error improved from inf to 21.10130, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 2s 19ms/step - loss: 21.6499 - mean_squared_error: 21.1013 - val_loss: 9.8379 - val_mean_squared_error: 9.2330\n",
            "Epoch 2/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 10.7176 - mean_squared_error: 10.0649\n",
            "Epoch 2: mean_squared_error improved from 21.10130 to 9.33219, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 10.0020 - mean_squared_error: 9.3322 - val_loss: 4.5789 - val_mean_squared_error: 3.8049\n",
            "Epoch 3/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 6.4164 - mean_squared_error: 5.6337\n",
            "Epoch 3: mean_squared_error improved from 9.33219 to 5.36527, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 6.1599 - mean_squared_error: 5.3653 - val_loss: 3.2654 - val_mean_squared_error: 2.3526\n",
            "Epoch 4/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 4.7002 - mean_squared_error: 3.8196\n",
            "Epoch 4: mean_squared_error improved from 5.36527 to 3.71001, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.5978 - mean_squared_error: 3.7100 - val_loss: 2.5903 - val_mean_squared_error: 1.5850\n",
            "Epoch 5/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 3.8266 - mean_squared_error: 2.8787\n",
            "Epoch 5: mean_squared_error improved from 3.71001 to 2.75372, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.7044 - mean_squared_error: 2.7537 - val_loss: 2.3350 - val_mean_squared_error: 1.2676\n",
            "Epoch 6/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 3.2433 - mean_squared_error: 2.2575\n",
            "Epoch 6: mean_squared_error improved from 2.75372 to 2.27228, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.2628 - mean_squared_error: 2.2723 - val_loss: 2.2085 - val_mean_squared_error: 1.1039\n",
            "Epoch 7/1000\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 3.1242 - mean_squared_error: 2.1175\n",
            "Epoch 7: mean_squared_error improved from 2.27228 to 1.93036, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.9420 - mean_squared_error: 1.9304 - val_loss: 2.1269 - val_mean_squared_error: 0.9992\n",
            "Epoch 8/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.7973 - mean_squared_error: 1.7687\n",
            "Epoch 8: mean_squared_error improved from 1.93036 to 1.70181, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.7285 - mean_squared_error: 1.7018 - val_loss: 2.0681 - val_mean_squared_error: 0.9252\n",
            "Epoch 9/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.4888 - mean_squared_error: 1.4530\n",
            "Epoch 9: mean_squared_error improved from 1.70181 to 1.44048, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.4815 - mean_squared_error: 1.4405 - val_loss: 2.0229 - val_mean_squared_error: 0.8724\n",
            "Epoch 10/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.3556 - mean_squared_error: 1.3146\n",
            "Epoch 10: mean_squared_error improved from 1.44048 to 1.33560, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.3783 - mean_squared_error: 1.3356 - val_loss: 1.9860 - val_mean_squared_error: 0.8350\n",
            "Epoch 11/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.1964 - mean_squared_error: 1.1543\n",
            "Epoch 11: mean_squared_error improved from 1.33560 to 1.16753, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.2121 - mean_squared_error: 1.1675 - val_loss: 1.9568 - val_mean_squared_error: 0.8088\n",
            "Epoch 12/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.2360 - mean_squared_error: 1.1930\n",
            "Epoch 12: mean_squared_error improved from 1.16753 to 1.11504, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.1575 - mean_squared_error: 1.1150 - val_loss: 1.9333 - val_mean_squared_error: 0.7899\n",
            "Epoch 13/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.0533 - mean_squared_error: 1.0154\n",
            "Epoch 13: mean_squared_error improved from 1.11504 to 1.05751, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.0916 - mean_squared_error: 1.0575 - val_loss: 1.9108 - val_mean_squared_error: 0.7777\n",
            "Epoch 14/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.9639 - mean_squared_error: 0.9331\n",
            "Epoch 14: mean_squared_error improved from 1.05751 to 0.94968, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.9812 - mean_squared_error: 0.9497 - val_loss: 1.8918 - val_mean_squared_error: 0.7699\n",
            "Epoch 15/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.9187 - mean_squared_error: 0.8943\n",
            "Epoch 15: mean_squared_error improved from 0.94968 to 0.93215, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.9543 - mean_squared_error: 0.9322 - val_loss: 1.8766 - val_mean_squared_error: 0.7659\n",
            "Epoch 16/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.9285 - mean_squared_error: 0.9083\n",
            "Epoch 16: mean_squared_error did not improve from 0.93215\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.9498 - mean_squared_error: 0.9323 - val_loss: 1.8634 - val_mean_squared_error: 0.7646\n",
            "Epoch 17/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.9103 - mean_squared_error: 0.9050\n",
            "Epoch 17: mean_squared_error improved from 0.93215 to 0.87728, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.8841 - mean_squared_error: 0.8773 - val_loss: 1.8519 - val_mean_squared_error: 0.7656\n",
            "Epoch 18/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.8435 - mean_squared_error: 0.8448\n",
            "Epoch 18: mean_squared_error improved from 0.87728 to 0.84212, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.8388 - mean_squared_error: 0.8421 - val_loss: 1.8390 - val_mean_squared_error: 0.7680\n",
            "Epoch 19/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.8261 - mean_squared_error: 0.8351\n",
            "Epoch 19: mean_squared_error improved from 0.84212 to 0.83255, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.8210 - mean_squared_error: 0.8326 - val_loss: 1.8274 - val_mean_squared_error: 0.7718\n",
            "Epoch 20/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.7656 - mean_squared_error: 0.7924\n",
            "Epoch 20: mean_squared_error improved from 0.83255 to 0.80005, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.7740 - mean_squared_error: 0.8000 - val_loss: 1.8175 - val_mean_squared_error: 0.7763\n",
            "Epoch 21/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.7845 - mean_squared_error: 0.8156\n",
            "Epoch 21: mean_squared_error improved from 0.80005 to 0.79799, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.7651 - mean_squared_error: 0.7980 - val_loss: 1.8080 - val_mean_squared_error: 0.7813\n",
            "Epoch 22/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.7939 - mean_squared_error: 0.8399\n",
            "Epoch 22: mean_squared_error did not improve from 0.79799\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.7677 - mean_squared_error: 0.8135 - val_loss: 1.7994 - val_mean_squared_error: 0.7862\n",
            "Epoch 23/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.7211 - mean_squared_error: 0.7816\n",
            "Epoch 23: mean_squared_error improved from 0.79799 to 0.77803, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.7191 - mean_squared_error: 0.7780 - val_loss: 1.7899 - val_mean_squared_error: 0.7912\n",
            "Epoch 24/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.7041 - mean_squared_error: 0.7729\n",
            "Epoch 24: mean_squared_error improved from 0.77803 to 0.77369, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.7038 - mean_squared_error: 0.7737 - val_loss: 1.7824 - val_mean_squared_error: 0.7965\n",
            "Epoch 25/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.6964 - mean_squared_error: 0.7726\n",
            "Epoch 25: mean_squared_error did not improve from 0.77369\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.7140 - mean_squared_error: 0.7932 - val_loss: 1.7765 - val_mean_squared_error: 0.8018\n",
            "Epoch 26/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.6728 - mean_squared_error: 0.7553\n",
            "Epoch 26: mean_squared_error improved from 0.77369 to 0.76804, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.6840 - mean_squared_error: 0.7680 - val_loss: 1.7697 - val_mean_squared_error: 0.8073\n",
            "Epoch 27/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.7537 - mean_squared_error: 0.8445\n",
            "Epoch 27: mean_squared_error did not improve from 0.76804\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.7336 - mean_squared_error: 0.8275 - val_loss: 1.7641 - val_mean_squared_error: 0.8131\n",
            "Epoch 28/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.6552 - mean_squared_error: 0.7574\n",
            "Epoch 28: mean_squared_error did not improve from 0.76804\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.6669 - mean_squared_error: 0.7690 - val_loss: 1.7600 - val_mean_squared_error: 0.8191\n",
            "Epoch 29/1000\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 1.6251 - mean_squared_error: 0.7391\n",
            "Epoch 29: mean_squared_error improved from 0.76804 to 0.72492, saving model to optimizers_best_adamax.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.6096 - mean_squared_error: 0.7249 - val_loss: 1.7510 - val_mean_squared_error: 0.8232\n",
            "Epoch 30/1000\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 1.5951 - mean_squared_error: 0.7197\n",
            "Epoch 30: mean_squared_error did not improve from 0.72492\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6062 - mean_squared_error: 0.7347 - val_loss: 1.7414 - val_mean_squared_error: 0.8266\n",
            "Epoch 31/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.6108 - mean_squared_error: 0.7473\n",
            "Epoch 31: mean_squared_error did not improve from 0.72492\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.6088 - mean_squared_error: 0.7450 - val_loss: 1.7328 - val_mean_squared_error: 0.8299\n",
            "Epoch 32/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.5917 - mean_squared_error: 0.7349\n",
            "Epoch 32: mean_squared_error did not improve from 0.72492\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.6010 - mean_squared_error: 0.7490 - val_loss: 1.7247 - val_mean_squared_error: 0.8333\n",
            "Epoch 33/1000\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 1.5634 - mean_squared_error: 0.7163\n",
            "Epoch 33: mean_squared_error did not improve from 0.72492\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5827 - mean_squared_error: 0.7386 - val_loss: 1.7167 - val_mean_squared_error: 0.8355\n",
            "Epoch 34/1000\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.5447 - mean_squared_error: 0.7043\n",
            "Epoch 34: mean_squared_error did not improve from 0.72492\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.6040 - mean_squared_error: 0.7676 - val_loss: 1.7078 - val_mean_squared_error: 0.8376\n",
            "Epoch 34: early stopping\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.5085 - mean_squared_error: 0.5843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcpjSvPoDdjz",
        "outputId": "08a770bb-3082-4d74-b6d6-abbab3c1914c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1064\n",
            "-rw-r--r-- 1 root root 143136 Jul 17 03:48 optimizers_best_adadelta.h5\n",
            "-rw-r--r-- 1 root root 143136 Jul 17 03:49 optimizers_best_adamax.h5\n",
            "-rw-r--r-- 1 root root 143136 Jul 17 03:48 optimizers_best_adam.h5\n",
            "-rw-r--r-- 1 root root 143136 Jul 17 03:49 optimizers_best_nadam.h5\n",
            "-rw-r--r-- 1 root root 102224 Jul 17 03:49 optimizers_best_rmsprop-0001.h5\n",
            "-rw-r--r-- 1 root root 102224 Jul 17 03:48 optimizers_best_rmsprop.h5\n",
            "-rw-r--r-- 1 root root 102224 Jul 17 03:48 optimizers_best_sgd-001.h5\n",
            "-rw-r--r-- 1 root root 102224 Jul 17 03:48 optimizers_best_sgd-momentum.h5\n",
            "drwxr-xr-x 1 root root   4096 Jul 13 13:33 sample_data\n",
            "-rw-r--r-- 1 root root 100951 Jul 17 03:13 winequality-red.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_PEEH-3gBTr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "c95c7960-e563-4289-837d-3792f502bb97"
      },
      "source": [
        "res = pd.DataFrame(results)\n",
        "res.columns = ['optimizer', 'epochs', 'val_mean_squared_error', 'mean_squared_error']\n",
        "res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      optimizer  epochs  val_mean_squared_error  mean_squared_error\n",
              "0       sgd-001      16                0.757509            0.602991\n",
              "1  sgd-momentum      42                0.985794            0.569350\n",
              "2          adam      33                0.675962            0.577201\n",
              "3      adadelta      33               32.755257           32.006981\n",
              "4       rmsprop      20                0.640166            0.540763\n",
              "5  rmsprop-0001     144                0.630730            0.549706\n",
              "6         nadam      35                0.668086            0.554688\n",
              "7        adamax      28                0.724924            0.584273"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-56a3923a-568b-4fa3-93cb-b08d570a3b16\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>optimizer</th>\n",
              "      <th>epochs</th>\n",
              "      <th>val_mean_squared_error</th>\n",
              "      <th>mean_squared_error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sgd-001</td>\n",
              "      <td>16</td>\n",
              "      <td>0.757509</td>\n",
              "      <td>0.602991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sgd-momentum</td>\n",
              "      <td>42</td>\n",
              "      <td>0.985794</td>\n",
              "      <td>0.569350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>adam</td>\n",
              "      <td>33</td>\n",
              "      <td>0.675962</td>\n",
              "      <td>0.577201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>adadelta</td>\n",
              "      <td>33</td>\n",
              "      <td>32.755257</td>\n",
              "      <td>32.006981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rmsprop</td>\n",
              "      <td>20</td>\n",
              "      <td>0.640166</td>\n",
              "      <td>0.540763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>rmsprop-0001</td>\n",
              "      <td>144</td>\n",
              "      <td>0.630730</td>\n",
              "      <td>0.549706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>nadam</td>\n",
              "      <td>35</td>\n",
              "      <td>0.668086</td>\n",
              "      <td>0.554688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>adamax</td>\n",
              "      <td>28</td>\n",
              "      <td>0.724924</td>\n",
              "      <td>0.584273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56a3923a-568b-4fa3-93cb-b08d570a3b16')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-7b8aeb07-da07-4132-bd90-934be52f4f4b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7b8aeb07-da07-4132-bd90-934be52f4f4b')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-7b8aeb07-da07-4132-bd90-934be52f4f4b button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-56a3923a-568b-4fa3-93cb-b08d570a3b16 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-56a3923a-568b-4fa3-93cb-b08d570a3b16');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgm040mNTtO2",
        "outputId": "f1f3c2f9-5734-47f1-face-0b74cf6c41f1"
      },
      "source": [
        "import tensorflow as tf\n",
        "initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)\n",
        "values = initializer(shape=(7,7))\n",
        "print(values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.5033965   0.0339929  -0.11890381 -0.31825864 -0.70629233  1.0356468\n",
            "  -0.1009609 ]\n",
            " [ 0.46455902  0.6481436  -0.7725586   0.4411914   0.40547988  0.9479252\n",
            "   0.65615755]\n",
            " [ 0.6783782   0.13453487  0.10115083 -0.9447473   1.5691861  -0.3007577\n",
            "   0.71164674]\n",
            " [ 0.8791011  -0.08741058 -1.2740767   0.16965912 -1.9728805  -0.55733126\n",
            "  -1.0780123 ]\n",
            " [ 0.09958404  0.70558786  0.03193973 -0.01851057  0.49650174 -0.10525787\n",
            "   0.25365728]\n",
            " [-1.0209355   0.38817742  1.0137835   1.3609387   0.09022329  0.3573908\n",
            "   0.6081653 ]\n",
            " [-0.73727465  0.57941383  0.57129896  0.57133746 -1.0857868  -0.35548192\n",
            "  -0.71652466]], shape=(7, 7), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mi0_BzATukS",
        "outputId": "f6ad2fdd-8825-40f8-d4aa-b1275e98d4ff"
      },
      "source": [
        "initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
        "values = initializer(shape=(7,7))\n",
        "print(values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 8.39114666e-01 -6.57521546e-01 -2.35330582e+00 -5.64857900e-01\n",
            "   1.26034653e+00 -7.40655482e-01  1.95219862e+00]\n",
            " [ 4.05456161e+00  1.18379378e+00 -1.27772462e+00 -1.26333439e+00\n",
            "   4.39128906e-01 -4.47810501e-01  6.98302865e-01]\n",
            " [ 1.10090666e-01 -6.07119203e-01  2.49132410e-01 -1.28387630e-01\n",
            "   1.86336327e+00 -1.05968976e+00 -3.04458797e-01]\n",
            " [ 7.09035456e-01  6.75821066e-01  1.06326234e+00  6.74264610e-01\n",
            "  -1.16655803e+00  7.91552246e-01 -1.39003372e+00]\n",
            " [ 1.77362633e+00  1.27789891e+00  1.39691517e-01 -6.53284013e-01\n",
            "   1.17303634e+00  3.77456337e-01 -2.20998335e+00]\n",
            " [ 6.12794936e-01 -5.58811665e-01  1.78640798e-01 -1.37357712e+00\n",
            "   1.32549763e+00  1.39280688e-03 -1.84837971e-02]\n",
            " [ 9.41408217e-01  6.56216681e-01 -3.07735831e-01  1.56174922e+00\n",
            "   4.16374803e-01 -5.03594697e-01  5.81789970e-01]], shape=(7, 7), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}