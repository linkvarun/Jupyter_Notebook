{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2XjyLHxfvna9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linkvarun/Jupyter_Notebook/blob/master/Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61npX6ajxxej"
      },
      "source": [
        "## Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG04tIcvn8-k"
      },
      "source": [
        "![alt text](https://cdn-media-1.freecodecamp.org/images/qRGh8boBcLLQfBvDnWTXKxZIEAk5LNfNABHF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyKhu535oK8Z"
      },
      "source": [
        "Bag of Words (BOW) is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set.\n",
        "\n",
        "In simple terms, it’s a collection of words to represent a sentence with word count and mostly disregarding the order in which they appear.\n",
        "\n",
        "BOW is an approach widely used with:\n",
        "\n",
        "* Natural language processing\n",
        "* Information retrieval from documents\n",
        "* Document classifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbeniDfMoW2m"
      },
      "source": [
        "Let’s start with an example to understand by taking some sentences and generating vectors for those.\n",
        "\n",
        "1. \"John likes to watch movies. Mary likes movies too\"\n",
        "2. \"John also likes to watch football games\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsAWDjr_oiXv"
      },
      "source": [
        "Further, for each sentence, remove multiple occurrences of the word and use the word count to represent this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjCMcVOHnvtQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "e7cd692b-6bfd-4be7-96d4-48226deb7846"
      },
      "source": [
        "1. {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1}\n",
        "2. {\"John\":1,\"also\":1,\"likes\":1,\"to\":1,\"watch\":1,\"football\":1,\"games\":1}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c63413d58278>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1. {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1}\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJuyU6UForrJ"
      },
      "source": [
        "Assuming these sentences are part of a document, below is the combined word frequency for our entire document. Both sentences are taken into account."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ri81-fGot1V"
      },
      "source": [
        "{\"John\":2,\"likes\":3,\"to\":2,\"watch\":2,\"movies\":2,\"Mary\":1,\"too\":1,  \"also\":1,\"football\":1,\"games\":1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS2q4K9MsVY9"
      },
      "source": [
        "**The length of the vector will always be equal to vocabulary size. In this case the vector length is 10.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUxIFxlRsgLk"
      },
      "source": [
        "In order to represent our original sentences in a vector, each vector is initialized with all zeros — [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "This is followed by iteration and comparison with each word in our vocabulary, and incrementing the vector value if the sentence has that word.\n",
        "\n",
        "John likes to watch movies. Mary likes movies too: [1, 2, 1, 1, 2, 1, 1, 0, 0, 0]\n",
        "\n",
        "John also likes to watch football games: [1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
        "\n",
        "For example, in sentence 1 the word likes appears in second position and appears two times. So the second element of our vector for sentence 1 will be 2: [1, 2, 1, 1, 2, 1, 1, 0, 0, 0]\n",
        "\n",
        "The vector is always proportional to the size of our vocabulary.\n",
        "\n",
        "A big document where the generated vocabulary is huge may result in a vector with lots of 0 values. This is called a sparse vector. Sparse vectors require more memory and computational resources when modeling. The vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF_CFDB8o5ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82df5354-48ed-4801-d17a-fda2603c1061"
      },
      "source": [
        "import numpy\n",
        "import re\n",
        "\n",
        "'''\n",
        "Tokenize each the sentences, example\n",
        "Input : \"John likes to watch movies. Mary likes movies too\"\n",
        "Output : \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\",\"likes\",\"movies\",\"too\"\n",
        "'''\n",
        "def tokenize(sentences):\n",
        "    words = []\n",
        "    for sentence in sentences:\n",
        "        w = word_extraction(sentence)\n",
        "        words.extend(w)\n",
        "\n",
        "    words = sorted(list(set(words)))\n",
        "    return words\n",
        "\n",
        "def word_extraction(sentence):\n",
        "    ignore = ['a', 'i',\"the\", \"is\"]\n",
        "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # \\w = [a-zA-Z0-9_]\n",
        "    cleaned_text = [w.lower() for w in words if w not in ignore]\n",
        "    return cleaned_text\n",
        "\n",
        "def generate_bow(allsentences):\n",
        "    vocab = tokenize(allsentences)\n",
        "    print(\"Word List for Document \\n{0} \\n\".format(vocab));\n",
        "\n",
        "    for sentence in allsentences:\n",
        "        words = word_extraction(sentence)\n",
        "        bag_vector = numpy.zeros(len(vocab))\n",
        "        for w in words:\n",
        "            for i,word in enumerate(vocab):\n",
        "                if word == w:\n",
        "                    bag_vector[i] += 1\n",
        "\n",
        "        print(\"{0} \\n{1}\\n\".format(sentence,numpy.array(bag_vector)))\n",
        "\n",
        "\n",
        "allsentences = [\"joe waited for the train\",\"the train waited for joe\", \"the train was late\", \"mary and samantha took the bus\",\n",
        "            \"i looked for mary and samantha at the bus station\",\n",
        "            \"mary and samantha arrived at the bus station early but waited until noon for the bus\"]\n",
        "\n",
        "\n",
        "generate_bow(allsentences)\n",
        "\n",
        "# or one can use sklearn\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words=['a', \"the\", \"is\"])\n",
        "X = vectorizer.fit_transform(allsentences)\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word List for Document \n",
            "['and', 'arrived', 'at', 'bus', 'but', 'early', 'for', 'joe', 'late', 'looked', 'mary', 'noon', 'samantha', 'station', 'took', 'train', 'until', 'waited', 'was'] \n",
            "\n",
            "joe waited for the train \n",
            "[0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "\n",
            "the train waited for joe \n",
            "[0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "\n",
            "the train was late \n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "\n",
            "mary and samantha took the bus \n",
            "[1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            "\n",
            "i looked for mary and samantha at the bus station \n",
            "[1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            "\n",
            "mary and samantha arrived at the bus station early but waited until noon for the bus \n",
            "[1. 1. 1. 2. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0.]\n",
            "\n",
            "[[0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0]\n",
            " [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1]\n",
            " [1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0]\n",
            " [1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0]\n",
            " [1 1 1 2 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.stop_words_"
      ],
      "metadata": {
        "id": "ZeRjCCvHcqGc",
        "outputId": "5a8c249b-64f7-42bd-fe2d-c15ab872ea6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9JKJBaxE8DU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea63add-790b-4397-cce0-9cbcaad74be3"
      },
      "source": [
        "vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'joe': 7,\n",
              " 'waited': 17,\n",
              " 'for': 6,\n",
              " 'train': 15,\n",
              " 'was': 18,\n",
              " 'late': 8,\n",
              " 'mary': 10,\n",
              " 'and': 0,\n",
              " 'samantha': 12,\n",
              " 'took': 14,\n",
              " 'bus': 3,\n",
              " 'looked': 9,\n",
              " 'at': 2,\n",
              " 'station': 13,\n",
              " 'arrived': 1,\n",
              " 'early': 5,\n",
              " 'but': 4,\n",
              " 'until': 16,\n",
              " 'noon': 11}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XjyLHxfvna9"
      },
      "source": [
        "### Limitations of BOW\n",
        "\n",
        "**Semantic meaning**: the basic BOW approach does not consider the meaning of the word in the document. It completely ignores the context in which it’s used. The same word can be used in multiple places based on the context or nearby words.\n",
        "\n",
        "**Vector size**: For a large document, the vector size can be huge resulting in a lot of computation and time. You may need to ignore words based on relevance to your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AggHl0qqxtV0"
      },
      "source": [
        "## Bi-gram / N-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyVMK1hxzJ6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae059af-2b40-4f9e-ca3b-3d1d2e1449ef"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXk0ashjzU8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25080c4-d3a5-48a3-85ee-7685ea97211e"
      },
      "source": [
        "nltk.download(\"gutenberg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS5_8cdvzFeE"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "word_list = []\n",
        "\n",
        "# Set up a quick lookup table for common words like \"the\" and \"an\" so they can be excluded\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "# For all 18 novels in the public domain book corpus, extract all their words\n",
        "[word_list.extend(nltk.corpus.gutenberg.words(f)) for f in nltk.corpus.gutenberg.fileids()]\n",
        "\n",
        "# Filter out words that have punctuation and make everything lower-case\n",
        "cleaned_words = [w.lower() for w in word_list if w.isalnum()]\n",
        "\n",
        "# Ask NLTK to generate a list of bigrams for the word \"sun\", excluding\n",
        "# those words which are too common to be interesing\n",
        "sun_bigrams = [b for b in nltk.bigrams(cleaned_words) if (b[0] == 'sun' or b[1] == 'sun') \\\n",
        "  and b[0] not in stops and b[1] not in stops]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WemP6_ULzpTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe4192c-2d54-4bfb-81e1-1aa57c054d8d"
      },
      "source": [
        "print(set(sun_bigrams))\n",
        "print(len(set(sun_bigrams)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('sun', 'shines'), ('setting', 'sun'), ('sun', 'hath'), ('sun', 'stood'), ('equatorial', 'sun'), ('lucifer', 'sun'), ('japanese', 'sun'), ('forenoon', 'sun'), ('sun', 'shineth'), ('sun', 'shine'), ('sun', '58'), ('sun', 'returned'), ('blinding', 'sun'), ('west', 'sun'), ('sun', 'seems'), ('sun', 'soon'), ('sun', '16'), ('sun', 'shifted'), ('orient', 'sun'), ('sun', 'ho'), ('sun', 'shone'), ('invisible', 'sun'), ('beaming', 'sun'), ('sun', 'swings'), ('sun', 'animals'), ('sun', 'till'), ('glad', 'sun'), ('sun', 'glade'), ('sun', 'saying'), ('sun', '17'), ('sun', 'goes'), ('sun', 'shot'), ('sun', 'bright'), ('burnished', 'sun'), ('sun', 'slowly'), ('sun', '19'), ('sun', 'hark'), ('sun', 'light'), ('volcanoes', 'sun'), ('sun', 'stands'), ('sun', 'wilt'), ('sun', 'waxed'), ('sun', 'falls'), ('sun', 'freighted'), ('sun', 'dial'), ('sun', 'also'), ('strong', 'sun'), ('sun', 'aye'), ('hot', 'sun'), ('sun', 'stars'), ('sultry', 'sun'), ('keystone', 'sun'), ('sun', 'something'), ('sun', '9'), ('flashing', 'sun'), ('sun', 'upon'), ('sun', 'gained'), ('sun', 'hides'), ('sun', 'excludes'), ('sun', 'riseth'), ('sun', 'wears'), ('low', 'sun'), ('sun', 'instantly'), ('sun', 'playing'), ('sun', 'besides'), ('sun', 'half'), ('sun', 'turnbull'), ('sun', 'never'), ('sun', 'descending'), ('sun', 'slow'), ('upper', 'sun'), ('sun', 'toward'), ('sun', 'goeth'), ('sun', 'tired'), ('sun', '7'), ('sun', 'dropt'), ('needed', 'sun'), ('sun', 'action'), ('slanting', 'sun'), ('sun', 'moby'), ('abated', 'sun'), ('sun', 'like'), ('sun', 'declined'), ('th', 'sun'), ('sun', 'another'), ('sun', 'moon'), ('sun', 'said'), ('sun', 'burned'), ('sun', '8'), ('red', 'sun'), ('sun', 'queen'), ('sun', 'level'), ('sun', '4'), ('dazzling', 'sun'), ('excellent', 'sun'), ('sun', 'dried'), ('mounted', 'sun'), ('sun', 'producing'), ('sun', 'carefully'), ('seeing', 'sun'), ('sun', 'going'), ('sun', 'beat'), ('sun', 'toasted'), ('white', 'sun'), ('sun', 'oh'), ('sun', 'burning'), ('sun', 'swept'), ('sun', 'beam'), ('sun', 'move'), ('sun', 'sinking'), ('sun', 'meets'), ('sun', 'smite'), ('sun', 'seeking'), ('sun', '2'), ('sun', 'predominant'), ('sun', 'impearls'), ('commanding', 'sun'), ('coming', 'sun'), ('sun', '11'), ('sun', 'appeared'), ('sun', 'evan'), ('sun', 'even'), ('sun', 'gilds'), ('sun', 'splashed'), ('sun', 'cast'), ('sun', 'ran'), ('day', 'sun'), ('golden', 'sun'), ('poor', 'sun'), ('midnight', 'sun'), ('sun', 'bursts'), ('israel', 'sun'), ('sun', 'ingendered'), ('ratifying', 'sun'), ('modern', 'sun'), ('radiant', 'sun'), ('sun', 'shall'), ('haired', 'sun'), ('sun', 'astern'), ('mounting', 'sun'), ('silent', 'sun'), ('western', 'sun'), ('sun', 'first'), ('send', 'sun'), ('pleasant', 'sun'), ('downward', 'sun'), ('sun', 'measuring'), ('sun', 'became'), ('sun', 'bonnet'), ('sun', 'stand'), ('sun', 'threw'), ('sun', 'entering'), ('quickening', 'sun'), ('midday', 'sun'), ('clear', 'sun'), ('sun', 'set'), ('hump', 'sun'), ('torrid', 'sun'), ('sun', 'afloat'), ('sun', 'ever'), ('sun', 'keep'), ('sun', 'shining'), ('coined', 'sun'), ('sun', 'rise'), ('sky', 'sun'), ('chemick', 'sun'), ('sun', '12'), ('mother', 'sun'), ('sun', 'ashamed'), ('neither', 'sun'), ('mr', 'sun'), ('sun', 'go'), ('victorious', 'sun'), ('sun', 'knoweth'), ('sun', 'would'), ('sun', 'neither'), ('sun', 'soaked'), ('sun', 'thought'), ('sun', 'rose'), ('immense', 'sun'), ('sun', 'making'), ('sun', 'though'), ('ye', 'sun'), ('sun', '1'), ('sun', 'science'), ('great', 'sun'), ('sun', 'come'), ('shining', 'sun'), ('blazing', 'sun'), ('sun', 'beyond'), ('sun', 'unto'), ('sun', 'chapter'), ('summer', 'sun'), ('sun', 'following'), ('sun', 'failing'), ('sun', 'new'), ('sun', 'radiant'), ('sun', 'breed'), ('sun', 'seemed'), ('sun', 'thy'), ('sun', 'lit'), ('afternoon', 'sun'), ('sun', 'burnt'), ('diver', 'sun'), ('sinking', 'sun'), ('sun', 'turned'), ('runaway', 'sun'), ('spangling', 'sun'), ('sun', 'frequently'), ('sun', 'namely'), ('sun', 'two'), ('sun', 'see'), ('sun', 'usher'), ('earth', 'sun'), ('running', 'sun'), ('sheeny', 'sun'), ('sun', 'wheels'), ('sun', 'whether'), ('sun', 'around'), ('sun', 'entered'), ('weaker', 'sun'), ('sun', 'ahab'), ('sun', 'dalroy'), ('natural', 'sun'), ('thou', 'sun'), ('sun', '74'), ('sun', 'paint'), ('thy', 'sun'), ('sun', 'descried'), ('rising', 'sun'), ('sun', 'tan'), ('glaring', 'sun'), ('bright', 'sun'), ('sun', 'went'), ('gayety', 'sun'), ('earliest', 'sun'), ('sun', 'ariseth'), ('morning', 'sun'), ('bearded', 'sun'), ('parting', 'sun'), ('sun', 'embracing'), ('sun', 'things'), ('sun', 'climbed'), ('risen', 'sun'), ('sun', 'gave')}\n",
            "245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biHZ6yaX0EAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d6dfe1d-a76f-4f90-96a0-de13e2d688ef"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3feHxbLz_pp"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "sentences = [\"To Sherlock Holmes she is always the woman\", \"I have seldom heard him mention her under any other name\"]\n",
        "\n",
        "bigrams = []\n",
        "for sentence in sentences:\n",
        "    sequence = word_tokenize(sentence)\n",
        "    bigrams.extend(list(ngrams(sequence, 2)))\n",
        "\n",
        "freq_dist = nltk.FreqDist(bigrams)\n",
        "prob_dist = nltk.MLEProbDist(freq_dist)\n",
        "number_of_bigrams = freq_dist.N()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUOg5nlt0K9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e46caf-53e8-42e4-cf44-6d8d68043066"
      },
      "source": [
        "print(bigrams)\n",
        "print(number_of_bigrams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('To', 'Sherlock'), ('Sherlock', 'Holmes'), ('Holmes', 'she'), ('she', 'is'), ('is', 'always'), ('always', 'the'), ('the', 'woman'), ('I', 'have'), ('have', 'seldom'), ('seldom', 'heard'), ('heard', 'him'), ('him', 'mention'), ('mention', 'her'), ('her', 'under'), ('under', 'any'), ('any', 'other'), ('other', 'name')]\n",
            "17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_KIdW26r4go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "955bef3d-4fd5-465e-b929-3c0eb638ef20"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "text = \"I am aware that nltk only offers bigrams and trigrams, but is there a way to split my text in four-grams, five-grams or even hundred-grams\"\n",
        "tokenize = nltk.word_tokenize(text)\n",
        "print(tokenize)\n",
        "print (len(tokenize))\n",
        "trigrams=ngrams(tokenize,3)\n",
        "print(trigrams)\n",
        "fourgrams=ngrams(tokenize,4)\n",
        "print(fourgrams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'aware', 'that', 'nltk', 'only', 'offers', 'bigrams', 'and', 'trigrams', ',', 'but', 'is', 'there', 'a', 'way', 'to', 'split', 'my', 'text', 'in', 'four-grams', ',', 'five-grams', 'or', 'even', 'hundred-grams']\n",
            "27\n",
            "<zip object at 0x7ad9b60a3d80>\n",
            "<zip object at 0x7ad9b60a3f40>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfRfPH_KtBue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6745b83d-5161-4ffb-caae-d0a00659956a"
      },
      "source": [
        "def get_ngrams(n_grams):\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        "get_ngrams(trigrams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am aware',\n",
              " 'am aware that',\n",
              " 'aware that nltk',\n",
              " 'that nltk only',\n",
              " 'nltk only offers',\n",
              " 'only offers bigrams',\n",
              " 'offers bigrams and',\n",
              " 'bigrams and trigrams',\n",
              " 'and trigrams ,',\n",
              " 'trigrams , but',\n",
              " ', but is',\n",
              " 'but is there',\n",
              " 'is there a',\n",
              " 'there a way',\n",
              " 'a way to',\n",
              " 'way to split',\n",
              " 'to split my',\n",
              " 'split my text',\n",
              " 'my text in',\n",
              " 'text in four-grams',\n",
              " 'in four-grams ,',\n",
              " 'four-grams , five-grams',\n",
              " ', five-grams or',\n",
              " 'five-grams or even',\n",
              " 'or even hundred-grams']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyfmZItvtcvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458b8aec-a55e-4a0d-abe1-59fa7afc3970"
      },
      "source": [
        "get_ngrams(fourgrams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgbXsRVxj_Dt",
        "outputId": "c169207a-de6b-4fec-d531-03465dabb686"
      },
      "source": [
        "allsentences = [\"The idea for a maths collaboration was sparked by a casual conversation in 2019 between mathematician Geordie Williamson at the University of Sydney in Australia and DeepMind’s chief executive, neuroscientist Demis Hassabis. Lackenby and a colleague at Oxford, András Juhász, both knot theorists, soon joined the project.\", \"Initially, the work focused on identifying mathematical problems that could be attacked using DeepMind’s technology. Machine learning enables computers to feed on large data sets and make guesses, such as matching a surveillance-camera image to a known face from a database of photographs. But its answers are inherently probabilistic, and mathematical proofs require certainty.\", \"But the team reasoned that machine learning could help to detect patterns, such as the relationship between two types of object. Mathematicians could then try to work out the precise relationship by formulating what they call a conjecture, and then attempting to write a rigorous proof that turns that statement into a certainty.\",\n",
        "            \"Because machine learning requires lots of data to train on, one requirement was to be able to calculate properties for large numbers of objects: in the case of knots, the team calculated several properties, called invariants, for millions of different knots.\",\n",
        "            \"he researchers then moved on to working out which AI technique would be most helpful for finding a pattern that linked two properties. One technique in particular, called saliency maps, turned out to be especially helpful. It is often used in computer vision to identify which parts of an image carry the most-relevant information. Saliency maps pointed to knot properties that were likely to be linked to each other, and generated a formula that seemed to be correct in all cases that could be tested. Lackenby and Juhász then provided a rigorous proof that the formula applied to a very large class of knots\"]\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer_bigram = CountVectorizer(ngram_range=(2,2))\n",
        "X = vectorizer_bigram.fit_transform(allsentences)\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 1 ... 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectorizer_bigram.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzNoMOVPM_Bv",
        "outputId": "161911e7-4cca-4cd0-ac61-23e4b1b03cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "269"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mel9_RzakjSN",
        "outputId": "5b3cac86-03e9-4ff5-c38b-aefcbc3b872a"
      },
      "source": [
        "vectorizer_bigram.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the idea': 218,\n",
              " 'idea for': 92,\n",
              " 'for maths': 78,\n",
              " 'maths collaboration': 138,\n",
              " 'collaboration was': 50,\n",
              " 'was sparked': 257,\n",
              " 'sparked by': 197,\n",
              " 'by casual': 36,\n",
              " 'casual conversation': 47,\n",
              " 'conversation in': 55,\n",
              " 'in 2019': 97,\n",
              " '2019 between': 0,\n",
              " 'between mathematician': 31,\n",
              " 'mathematician geordie': 136,\n",
              " 'geordie williamson': 85,\n",
              " 'williamson at': 263,\n",
              " 'at the': 19,\n",
              " 'the university': 224,\n",
              " 'university of': 252,\n",
              " 'of sydney': 154,\n",
              " 'sydney in': 201,\n",
              " 'in australia': 99,\n",
              " 'australia and': 22,\n",
              " 'and deepmind': 6,\n",
              " 'deepmind chief': 63,\n",
              " 'chief executive': 48,\n",
              " 'executive neuroscientist': 71,\n",
              " 'neuroscientist demis': 143,\n",
              " 'demis hassabis': 65,\n",
              " 'hassabis lackenby': 87,\n",
              " 'lackenby and': 118,\n",
              " 'and colleague': 5,\n",
              " 'colleague at': 51,\n",
              " 'at oxford': 18,\n",
              " 'oxford andrás': 166,\n",
              " 'andrás juhász': 12,\n",
              " 'juhász both': 112,\n",
              " 'both knot': 33,\n",
              " 'knot theorists': 115,\n",
              " 'theorists soon': 230,\n",
              " 'soon joined': 196,\n",
              " 'joined the': 111,\n",
              " 'the project': 221,\n",
              " 'initially the': 105,\n",
              " 'the work': 225,\n",
              " 'work focused': 264,\n",
              " 'focused on': 75,\n",
              " 'on identifying': 156,\n",
              " 'identifying mathematical': 94,\n",
              " 'mathematical problems': 134,\n",
              " 'problems that': 175,\n",
              " 'that could': 208,\n",
              " 'could be': 57,\n",
              " 'be attacked': 24,\n",
              " 'attacked using': 20,\n",
              " 'using deepmind': 254,\n",
              " 'deepmind technology': 64,\n",
              " 'technology machine': 206,\n",
              " 'machine learning': 129,\n",
              " 'learning enables': 123,\n",
              " 'enables computers': 69,\n",
              " 'computers to': 53,\n",
              " 'to feed': 236,\n",
              " 'feed on': 73,\n",
              " 'on large': 157,\n",
              " 'large data': 120,\n",
              " 'data sets': 60,\n",
              " 'sets and': 194,\n",
              " 'and make': 9,\n",
              " 'make guesses': 130,\n",
              " 'guesses such': 86,\n",
              " 'such as': 199,\n",
              " 'as matching': 16,\n",
              " 'matching surveillance': 133,\n",
              " 'surveillance camera': 200,\n",
              " 'camera image': 43,\n",
              " 'image to': 96,\n",
              " 'to known': 239,\n",
              " 'known face': 117,\n",
              " 'face from': 72,\n",
              " 'from database': 83,\n",
              " 'database of': 62,\n",
              " 'of photographs': 153,\n",
              " 'photographs but': 171,\n",
              " 'but its': 34,\n",
              " 'its answers': 110,\n",
              " 'answers are': 13,\n",
              " 'are inherently': 15,\n",
              " 'inherently probabilistic': 104,\n",
              " 'probabilistic and': 174,\n",
              " 'and mathematical': 10,\n",
              " 'mathematical proofs': 135,\n",
              " 'proofs require': 177,\n",
              " 'require certainty': 187,\n",
              " 'but the': 35,\n",
              " 'the team': 223,\n",
              " 'team reasoned': 203,\n",
              " 'reasoned that': 183,\n",
              " 'that machine': 210,\n",
              " 'learning could': 122,\n",
              " 'could help': 58,\n",
              " 'help to': 89,\n",
              " 'to detect': 234,\n",
              " 'detect patterns': 66,\n",
              " 'patterns such': 170,\n",
              " 'as the': 17,\n",
              " 'the relationship': 222,\n",
              " 'relationship between': 184,\n",
              " 'between two': 32,\n",
              " 'two types': 250,\n",
              " 'types of': 251,\n",
              " 'of object': 151,\n",
              " 'object mathematicians': 145,\n",
              " 'mathematicians could': 137,\n",
              " 'could then': 59,\n",
              " 'then try': 229,\n",
              " 'try to': 246,\n",
              " 'to work': 242,\n",
              " 'work out': 265,\n",
              " 'out the': 163,\n",
              " 'the precise': 220,\n",
              " 'precise relationship': 173,\n",
              " 'relationship by': 185,\n",
              " 'by formulating': 37,\n",
              " 'formulating what': 82,\n",
              " 'what they': 260,\n",
              " 'they call': 231,\n",
              " 'call conjecture': 40,\n",
              " 'conjecture and': 54,\n",
              " 'and then': 11,\n",
              " 'then attempting': 226,\n",
              " 'attempting to': 21,\n",
              " 'to write': 244,\n",
              " 'write rigorous': 268,\n",
              " 'rigorous proof': 191,\n",
              " 'proof that': 176,\n",
              " 'that turns': 214,\n",
              " 'turns that': 248,\n",
              " 'that statement': 212,\n",
              " 'statement into': 198,\n",
              " 'into certainty': 106,\n",
              " 'because machine': 30,\n",
              " 'learning requires': 124,\n",
              " 'requires lots': 189,\n",
              " 'lots of': 128,\n",
              " 'of data': 148,\n",
              " 'data to': 61,\n",
              " 'to train': 240,\n",
              " 'train on': 245,\n",
              " 'on one': 158,\n",
              " 'one requirement': 160,\n",
              " 'requirement was': 188,\n",
              " 'was to': 258,\n",
              " 'to be': 232,\n",
              " 'be able': 23,\n",
              " 'able to': 1,\n",
              " 'to calculate': 233,\n",
              " 'calculate properties': 38,\n",
              " 'properties for': 179,\n",
              " 'for large': 77,\n",
              " 'large numbers': 121,\n",
              " 'numbers of': 144,\n",
              " 'of objects': 152,\n",
              " 'objects in': 146,\n",
              " 'in the': 102,\n",
              " 'the case': 216,\n",
              " 'case of': 45,\n",
              " 'of knots': 150,\n",
              " 'knots the': 116,\n",
              " 'team calculated': 202,\n",
              " 'calculated several': 39,\n",
              " 'several properties': 195,\n",
              " 'properties called': 178,\n",
              " 'called invariants': 41,\n",
              " 'invariants for': 107,\n",
              " 'for millions': 79,\n",
              " 'millions of': 139,\n",
              " 'of different': 149,\n",
              " 'different knots': 67,\n",
              " 'he researchers': 88,\n",
              " 'researchers then': 190,\n",
              " 'then moved': 227,\n",
              " 'moved on': 142,\n",
              " 'on to': 159,\n",
              " 'to working': 243,\n",
              " 'working out': 266,\n",
              " 'out which': 165,\n",
              " 'which ai': 261,\n",
              " 'ai technique': 2,\n",
              " 'technique would': 205,\n",
              " 'would be': 267,\n",
              " 'be most': 28,\n",
              " 'most helpful': 140,\n",
              " 'helpful for': 90,\n",
              " 'for finding': 76,\n",
              " 'finding pattern': 74,\n",
              " 'pattern that': 169,\n",
              " 'that linked': 209,\n",
              " 'linked two': 127,\n",
              " 'two properties': 249,\n",
              " 'properties one': 180,\n",
              " 'one technique': 161,\n",
              " 'technique in': 204,\n",
              " 'in particular': 101,\n",
              " 'particular called': 167,\n",
              " 'called saliency': 42,\n",
              " 'saliency maps': 192,\n",
              " 'maps turned': 132,\n",
              " 'turned out': 247,\n",
              " 'out to': 164,\n",
              " 'be especially': 26,\n",
              " 'especially helpful': 70,\n",
              " 'helpful it': 91,\n",
              " 'it is': 109,\n",
              " 'is often': 108,\n",
              " 'often used': 155,\n",
              " 'used in': 253,\n",
              " 'in computer': 100,\n",
              " 'computer vision': 52,\n",
              " 'vision to': 256,\n",
              " 'to identify': 237,\n",
              " 'identify which': 93,\n",
              " 'which parts': 262,\n",
              " 'parts of': 168,\n",
              " 'of an': 147,\n",
              " 'an image': 4,\n",
              " 'image carry': 95,\n",
              " 'carry the': 44,\n",
              " 'the most': 219,\n",
              " 'most relevant': 141,\n",
              " 'relevant information': 186,\n",
              " 'information saliency': 103,\n",
              " 'maps pointed': 131,\n",
              " 'pointed to': 172,\n",
              " 'to knot': 238,\n",
              " 'knot properties': 114,\n",
              " 'properties that': 181,\n",
              " 'that were': 215,\n",
              " 'were likely': 259,\n",
              " 'likely to': 125,\n",
              " 'be linked': 27,\n",
              " 'linked to': 126,\n",
              " 'to each': 235,\n",
              " 'each other': 68,\n",
              " 'other and': 162,\n",
              " 'and generated': 7,\n",
              " 'generated formula': 84,\n",
              " 'formula that': 81,\n",
              " 'that seemed': 211,\n",
              " 'seemed to': 193,\n",
              " 'be correct': 25,\n",
              " 'correct in': 56,\n",
              " 'in all': 98,\n",
              " 'all cases': 3,\n",
              " 'cases that': 46,\n",
              " 'be tested': 29,\n",
              " 'tested lackenby': 207,\n",
              " 'and juhász': 8,\n",
              " 'juhász then': 113,\n",
              " 'then provided': 228,\n",
              " 'provided rigorous': 182,\n",
              " 'that the': 213,\n",
              " 'the formula': 217,\n",
              " 'formula applied': 80,\n",
              " 'applied to': 14,\n",
              " 'to very': 241,\n",
              " 'very large': 255,\n",
              " 'large class': 119,\n",
              " 'class of': 49}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_unigram = CountVectorizer(ngram_range=(1,1))\n",
        "X = vectorizer_unigram.fit_transform(allsentences)\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSLj66U5fU_-",
        "outputId": "8fa13495-50c0-4881-890c-25b212a458d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 0 0 2 1 0 0 0 0 2 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0\n",
            "  0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 2 0\n",
            "  0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1\n",
            "  0 0 0 0 0 3 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
            "  1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
            "  1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 2 0 0 0 0 0 0 0 0 0 0 1 0 2\n",
            "  0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0\n",
            "  0 0 1 0 1 1 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 1 0 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
            "  0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 2 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
            "  1 0 0 0 3 3 2 0 1 3 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 1 0 0 0 0 0 0 2 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 4 0 1\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  1 0 0 0 0 2 0 0 0 3 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 1 1 2 0 0 1 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1\n",
            "  0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 2 0 0 1 0 0 0 1 0 2 0 1 0 1 3 1\n",
            "  0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 2 0 0 0 2 0 0 0 0 0 0 2 1 0 0 0 0 2 1 1\n",
            "  1 1 2 0 1 1 1 0 0 1 0 0 0 0 1 0 2 1 0 0 1 0 0 0 1 1 2 1 0 0 0 0 0 0 0 0\n",
            "  0 2 0 1 5 2 2 0 0 8 0 0 1 0 1 0 0 1 0 1 1 0 1 0 2 0 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectorizer_unigram.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h38xcwwM3WX",
        "outputId": "14a4b6df-7ec4-425c-9c44-8fb32df0fb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "174"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_unigram.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POvCBEZ_faA8",
        "outputId": "5b8c0971-4ee6-4d7e-8f85-86e744a01eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 149,\n",
              " 'idea': 66,\n",
              " 'for': 55,\n",
              " 'maths': 97,\n",
              " 'collaboration': 33,\n",
              " 'was': 165,\n",
              " 'sparked': 139,\n",
              " 'by': 20,\n",
              " 'casual': 29,\n",
              " 'conversation': 38,\n",
              " 'in': 70,\n",
              " '2019': 0,\n",
              " 'between': 17,\n",
              " 'mathematician': 95,\n",
              " 'geordie': 60,\n",
              " 'williamson': 169,\n",
              " 'at': 11,\n",
              " 'university': 160,\n",
              " 'of': 105,\n",
              " 'sydney': 143,\n",
              " 'australia': 14,\n",
              " 'and': 5,\n",
              " 'deepmind': 43,\n",
              " 'chief': 31,\n",
              " 'executive': 50,\n",
              " 'neuroscientist': 101,\n",
              " 'demis': 44,\n",
              " 'hassabis': 62,\n",
              " 'lackenby': 84,\n",
              " 'colleague': 34,\n",
              " 'oxford': 111,\n",
              " 'andrás': 6,\n",
              " 'juhász': 80,\n",
              " 'both': 18,\n",
              " 'knot': 81,\n",
              " 'theorists': 151,\n",
              " 'soon': 138,\n",
              " 'joined': 79,\n",
              " 'project': 121,\n",
              " 'initially': 73,\n",
              " 'work': 170,\n",
              " 'focused': 54,\n",
              " 'on': 107,\n",
              " 'identifying': 68,\n",
              " 'mathematical': 94,\n",
              " 'problems': 120,\n",
              " 'that': 148,\n",
              " 'could': 40,\n",
              " 'be': 15,\n",
              " 'attacked': 12,\n",
              " 'using': 162,\n",
              " 'technology': 146,\n",
              " 'machine': 90,\n",
              " 'learning': 86,\n",
              " 'enables': 48,\n",
              " 'computers': 36,\n",
              " 'to': 153,\n",
              " 'feed': 52,\n",
              " 'large': 85,\n",
              " 'data': 41,\n",
              " 'sets': 136,\n",
              " 'make': 91,\n",
              " 'guesses': 61,\n",
              " 'such': 141,\n",
              " 'as': 10,\n",
              " 'matching': 93,\n",
              " 'surveillance': 142,\n",
              " 'camera': 25,\n",
              " 'image': 69,\n",
              " 'known': 83,\n",
              " 'face': 51,\n",
              " 'from': 58,\n",
              " 'database': 42,\n",
              " 'photographs': 116,\n",
              " 'but': 19,\n",
              " 'its': 78,\n",
              " 'answers': 7,\n",
              " 'are': 9,\n",
              " 'inherently': 72,\n",
              " 'probabilistic': 119,\n",
              " 'proofs': 123,\n",
              " 'require': 129,\n",
              " 'certainty': 30,\n",
              " 'team': 144,\n",
              " 'reasoned': 126,\n",
              " 'help': 64,\n",
              " 'detect': 45,\n",
              " 'patterns': 115,\n",
              " 'relationship': 127,\n",
              " 'two': 158,\n",
              " 'types': 159,\n",
              " 'object': 103,\n",
              " 'mathematicians': 96,\n",
              " 'then': 150,\n",
              " 'try': 155,\n",
              " 'out': 110,\n",
              " 'precise': 118,\n",
              " 'formulating': 57,\n",
              " 'what': 167,\n",
              " 'they': 152,\n",
              " 'call': 23,\n",
              " 'conjecture': 37,\n",
              " 'attempting': 13,\n",
              " 'write': 173,\n",
              " 'rigorous': 133,\n",
              " 'proof': 122,\n",
              " 'turns': 157,\n",
              " 'statement': 140,\n",
              " 'into': 74,\n",
              " 'because': 16,\n",
              " 'requires': 131,\n",
              " 'lots': 89,\n",
              " 'train': 154,\n",
              " 'one': 108,\n",
              " 'requirement': 130,\n",
              " 'able': 1,\n",
              " 'calculate': 21,\n",
              " 'properties': 124,\n",
              " 'numbers': 102,\n",
              " 'objects': 104,\n",
              " 'case': 27,\n",
              " 'knots': 82,\n",
              " 'calculated': 22,\n",
              " 'several': 137,\n",
              " 'called': 24,\n",
              " 'invariants': 75,\n",
              " 'millions': 98,\n",
              " 'different': 46,\n",
              " 'he': 63,\n",
              " 'researchers': 132,\n",
              " 'moved': 100,\n",
              " 'working': 171,\n",
              " 'which': 168,\n",
              " 'ai': 2,\n",
              " 'technique': 145,\n",
              " 'would': 172,\n",
              " 'most': 99,\n",
              " 'helpful': 65,\n",
              " 'finding': 53,\n",
              " 'pattern': 114,\n",
              " 'linked': 88,\n",
              " 'particular': 112,\n",
              " 'saliency': 134,\n",
              " 'maps': 92,\n",
              " 'turned': 156,\n",
              " 'especially': 49,\n",
              " 'it': 77,\n",
              " 'is': 76,\n",
              " 'often': 106,\n",
              " 'used': 161,\n",
              " 'computer': 35,\n",
              " 'vision': 164,\n",
              " 'identify': 67,\n",
              " 'parts': 113,\n",
              " 'an': 4,\n",
              " 'carry': 26,\n",
              " 'relevant': 128,\n",
              " 'information': 71,\n",
              " 'pointed': 117,\n",
              " 'were': 166,\n",
              " 'likely': 87,\n",
              " 'each': 47,\n",
              " 'other': 109,\n",
              " 'generated': 59,\n",
              " 'formula': 56,\n",
              " 'seemed': 135,\n",
              " 'correct': 39,\n",
              " 'all': 3,\n",
              " 'cases': 28,\n",
              " 'tested': 147,\n",
              " 'provided': 125,\n",
              " 'applied': 8,\n",
              " 'very': 163,\n",
              " 'class': 32}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA8OmHZklSCU",
        "outputId": "99fb95db-963b-4e7c-814b-65ef306d7352"
      },
      "source": [
        "allsentences = [\"joe waited for the train\"]\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer_trigram = CountVectorizer(ngram_range=(3,3))\n",
        "X = vectorizer_trigram.fit_transform(allsentences)\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO3al5MxlYpz",
        "outputId": "db2ba5c1-cffe-4491-b7af-f6a466144b03"
      },
      "source": [
        "vectorizer_trigram.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'joe waited for': 1, 'waited for the': 2, 'for the train': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsfSaZCTozeo",
        "outputId": "39948d11-bc1d-4177-a5bf-cbad1c069dd5"
      },
      "source": [
        "whitman = nltk.corpus.gutenberg.words('whitman-leaves.txt')\n",
        "print(whitman)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', 'Leaves', 'of', 'Grass', 'by', 'Walt', 'Whitman', ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkQcCtJYo1xN",
        "outputId": "87d73c23-401c-4cfe-84c9-e6ce7c4fe6d3"
      },
      "source": [
        "len(whitman)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "154883"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GF-GwqAo4eC",
        "outputId": "d50d5652-f1bf-45ae-8d94-8b1ef71ec165"
      },
      "source": [
        "len(set(whitman))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14329"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wssJFD8LqRkq",
        "outputId": "22186f19-a943-4259-90c5-8ed06afce159"
      },
      "source": [
        "allsentences = [\"The idea for a maths collaboration was sparked by a casual conversation in 2019 between mathematician Geordie Williamson at the University of Sydney in Australia and DeepMind’s chief executive, neuroscientist Demis Hassabis. Lackenby and a colleague at Oxford, András Juhász, both knot theorists, soon joined the project.\", \"Initially, the work focused on identifying mathematical problems that could be attacked using DeepMind’s technology. Machine learning enables computers to feed on large data sets and make guesses, such as matching a surveillance-camera image to a known face from a database of photographs. But its answers are inherently probabilistic, and mathematical proofs require certainty.\", \"But the team reasoned that machine learning could help to detect patterns, such as the relationship between two types of object. Mathematicians could then try to work out the precise relationship by formulating what they call a conjecture, and then attempting to write a rigorous proof that turns that statement into a certainty.\",\n",
        "            \"Because machine learning requires lots of data to train on, one requirement was to be able to calculate properties for large numbers of objects: in the case of knots, the team calculated several properties, called invariants, for millions of different knots.\",\n",
        "            \"he researchers then moved on to working out which AI technique would be most helpful for finding a pattern that linked two properties. One technique in particular, called saliency maps, turned out to be especially helpful. It is often used in computer vision to identify which parts of an image carry the most-relevant information. Saliency maps pointed to knot properties that were likely to be linked to each other, and generated a formula that seemed to be correct in all cases that could be tested. Lackenby and Juhász then provided a rigorous proof that the formula applied to a very large class of knots\"]\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(allsentences)\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 0 0 2 1 0 0 0 0 2 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0\n",
            "  0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 2 0\n",
            "  0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1\n",
            "  0 0 0 0 0 3 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
            "  1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
            "  1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 2 0 0 0 0 0 0 0 0 0 0 1 0 2\n",
            "  0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0\n",
            "  0 0 1 0 1 1 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 1 0 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
            "  0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 2 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
            "  1 0 0 0 3 3 2 0 1 3 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 1 0 0 0 0 0 0 2 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 4 0 1\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  1 0 0 0 0 2 0 0 0 3 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 1 1 2 0 0 1 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1\n",
            "  0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 2 0 0 1 0 0 0 1 0 2 0 1 0 1 3 1\n",
            "  0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 2 0 0 0 2 0 0 0 0 0 0 2 1 0 0 0 0 2 1 1\n",
            "  1 1 2 0 1 1 1 0 0 1 0 0 0 0 1 0 2 1 0 0 1 0 0 0 1 1 2 1 0 0 0 0 0 0 0 0\n",
            "  0 2 0 1 5 2 2 0 0 8 0 0 1 0 1 0 0 1 0 1 1 0 1 0 2 0 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiobmrxNqWEg",
        "outputId": "2c22c270-1db6-4e60-8df9-d41c0470dbe0"
      },
      "source": [
        "vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 149,\n",
              " 'idea': 66,\n",
              " 'for': 55,\n",
              " 'maths': 97,\n",
              " 'collaboration': 33,\n",
              " 'was': 165,\n",
              " 'sparked': 139,\n",
              " 'by': 20,\n",
              " 'casual': 29,\n",
              " 'conversation': 38,\n",
              " 'in': 70,\n",
              " '2019': 0,\n",
              " 'between': 17,\n",
              " 'mathematician': 95,\n",
              " 'geordie': 60,\n",
              " 'williamson': 169,\n",
              " 'at': 11,\n",
              " 'university': 160,\n",
              " 'of': 105,\n",
              " 'sydney': 143,\n",
              " 'australia': 14,\n",
              " 'and': 5,\n",
              " 'deepmind': 43,\n",
              " 'chief': 31,\n",
              " 'executive': 50,\n",
              " 'neuroscientist': 101,\n",
              " 'demis': 44,\n",
              " 'hassabis': 62,\n",
              " 'lackenby': 84,\n",
              " 'colleague': 34,\n",
              " 'oxford': 111,\n",
              " 'andrás': 6,\n",
              " 'juhász': 80,\n",
              " 'both': 18,\n",
              " 'knot': 81,\n",
              " 'theorists': 151,\n",
              " 'soon': 138,\n",
              " 'joined': 79,\n",
              " 'project': 121,\n",
              " 'initially': 73,\n",
              " 'work': 170,\n",
              " 'focused': 54,\n",
              " 'on': 107,\n",
              " 'identifying': 68,\n",
              " 'mathematical': 94,\n",
              " 'problems': 120,\n",
              " 'that': 148,\n",
              " 'could': 40,\n",
              " 'be': 15,\n",
              " 'attacked': 12,\n",
              " 'using': 162,\n",
              " 'technology': 146,\n",
              " 'machine': 90,\n",
              " 'learning': 86,\n",
              " 'enables': 48,\n",
              " 'computers': 36,\n",
              " 'to': 153,\n",
              " 'feed': 52,\n",
              " 'large': 85,\n",
              " 'data': 41,\n",
              " 'sets': 136,\n",
              " 'make': 91,\n",
              " 'guesses': 61,\n",
              " 'such': 141,\n",
              " 'as': 10,\n",
              " 'matching': 93,\n",
              " 'surveillance': 142,\n",
              " 'camera': 25,\n",
              " 'image': 69,\n",
              " 'known': 83,\n",
              " 'face': 51,\n",
              " 'from': 58,\n",
              " 'database': 42,\n",
              " 'photographs': 116,\n",
              " 'but': 19,\n",
              " 'its': 78,\n",
              " 'answers': 7,\n",
              " 'are': 9,\n",
              " 'inherently': 72,\n",
              " 'probabilistic': 119,\n",
              " 'proofs': 123,\n",
              " 'require': 129,\n",
              " 'certainty': 30,\n",
              " 'team': 144,\n",
              " 'reasoned': 126,\n",
              " 'help': 64,\n",
              " 'detect': 45,\n",
              " 'patterns': 115,\n",
              " 'relationship': 127,\n",
              " 'two': 158,\n",
              " 'types': 159,\n",
              " 'object': 103,\n",
              " 'mathematicians': 96,\n",
              " 'then': 150,\n",
              " 'try': 155,\n",
              " 'out': 110,\n",
              " 'precise': 118,\n",
              " 'formulating': 57,\n",
              " 'what': 167,\n",
              " 'they': 152,\n",
              " 'call': 23,\n",
              " 'conjecture': 37,\n",
              " 'attempting': 13,\n",
              " 'write': 173,\n",
              " 'rigorous': 133,\n",
              " 'proof': 122,\n",
              " 'turns': 157,\n",
              " 'statement': 140,\n",
              " 'into': 74,\n",
              " 'because': 16,\n",
              " 'requires': 131,\n",
              " 'lots': 89,\n",
              " 'train': 154,\n",
              " 'one': 108,\n",
              " 'requirement': 130,\n",
              " 'able': 1,\n",
              " 'calculate': 21,\n",
              " 'properties': 124,\n",
              " 'numbers': 102,\n",
              " 'objects': 104,\n",
              " 'case': 27,\n",
              " 'knots': 82,\n",
              " 'calculated': 22,\n",
              " 'several': 137,\n",
              " 'called': 24,\n",
              " 'invariants': 75,\n",
              " 'millions': 98,\n",
              " 'different': 46,\n",
              " 'he': 63,\n",
              " 'researchers': 132,\n",
              " 'moved': 100,\n",
              " 'working': 171,\n",
              " 'which': 168,\n",
              " 'ai': 2,\n",
              " 'technique': 145,\n",
              " 'would': 172,\n",
              " 'most': 99,\n",
              " 'helpful': 65,\n",
              " 'finding': 53,\n",
              " 'pattern': 114,\n",
              " 'linked': 88,\n",
              " 'particular': 112,\n",
              " 'saliency': 134,\n",
              " 'maps': 92,\n",
              " 'turned': 156,\n",
              " 'especially': 49,\n",
              " 'it': 77,\n",
              " 'is': 76,\n",
              " 'often': 106,\n",
              " 'used': 161,\n",
              " 'computer': 35,\n",
              " 'vision': 164,\n",
              " 'identify': 67,\n",
              " 'parts': 113,\n",
              " 'an': 4,\n",
              " 'carry': 26,\n",
              " 'relevant': 128,\n",
              " 'information': 71,\n",
              " 'pointed': 117,\n",
              " 'were': 166,\n",
              " 'likely': 87,\n",
              " 'each': 47,\n",
              " 'other': 109,\n",
              " 'generated': 59,\n",
              " 'formula': 56,\n",
              " 'seemed': 135,\n",
              " 'correct': 39,\n",
              " 'all': 3,\n",
              " 'cases': 28,\n",
              " 'tested': 147,\n",
              " 'provided': 125,\n",
              " 'applied': 8,\n",
              " 'very': 163,\n",
              " 'class': 32}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer_trigram = CountVectorizer(ngram_range=(3,3))\n",
        "X = vectorizer_trigram.fit_transform(allsentences)\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa9Nr7xgX_tL",
        "outputId": "cef26f45-19ab-4097-bfd0-b97f77a4a097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 1 ... 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectorizer_trigram.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poa8UgaMNog0",
        "outputId": "68eceb03-7c0c-4fb8-f462-b1846666a1c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "276"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " vectorizer_trigram.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r176l5lBYIMc",
        "outputId": "9037d58b-0502-4609-8519-dfa203ff473d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the idea for': 222,\n",
              " 'idea for maths': 92,\n",
              " 'for maths collaboration': 78,\n",
              " 'maths collaboration was': 140,\n",
              " 'collaboration was sparked': 50,\n",
              " 'was sparked by': 264,\n",
              " 'sparked by casual': 200,\n",
              " 'by casual conversation': 36,\n",
              " 'casual conversation in': 47,\n",
              " 'conversation in 2019': 55,\n",
              " 'in 2019 between': 97,\n",
              " '2019 between mathematician': 0,\n",
              " 'between mathematician geordie': 31,\n",
              " 'mathematician geordie williamson': 138,\n",
              " 'geordie williamson at': 85,\n",
              " 'williamson at the': 270,\n",
              " 'at the university': 19,\n",
              " 'the university of': 228,\n",
              " 'university of sydney': 259,\n",
              " 'of sydney in': 156,\n",
              " 'sydney in australia': 205,\n",
              " 'in australia and': 99,\n",
              " 'australia and deepmind': 22,\n",
              " 'and deepmind chief': 6,\n",
              " 'deepmind chief executive': 64,\n",
              " 'chief executive neuroscientist': 48,\n",
              " 'executive neuroscientist demis': 71,\n",
              " 'neuroscientist demis hassabis': 145,\n",
              " 'demis hassabis lackenby': 66,\n",
              " 'hassabis lackenby and': 87,\n",
              " 'lackenby and colleague': 117,\n",
              " 'and colleague at': 5,\n",
              " 'colleague at oxford': 51,\n",
              " 'at oxford andrás': 18,\n",
              " 'oxford andrás juhász': 168,\n",
              " 'andrás juhász both': 12,\n",
              " 'juhász both knot': 111,\n",
              " 'both knot theorists': 33,\n",
              " 'knot theorists soon': 114,\n",
              " 'theorists soon joined': 234,\n",
              " 'soon joined the': 199,\n",
              " 'joined the project': 110,\n",
              " 'initially the work': 105,\n",
              " 'the work focused': 229,\n",
              " 'work focused on': 271,\n",
              " 'focused on identifying': 75,\n",
              " 'on identifying mathematical': 158,\n",
              " 'identifying mathematical problems': 94,\n",
              " 'mathematical problems that': 136,\n",
              " 'problems that could': 177,\n",
              " 'that could be': 212,\n",
              " 'could be attacked': 57,\n",
              " 'be attacked using': 24,\n",
              " 'attacked using deepmind': 20,\n",
              " 'using deepmind technology': 261,\n",
              " 'deepmind technology machine': 65,\n",
              " 'technology machine learning': 210,\n",
              " 'machine learning enables': 130,\n",
              " 'learning enables computers': 123,\n",
              " 'enables computers to': 69,\n",
              " 'computers to feed': 53,\n",
              " 'to feed on': 243,\n",
              " 'feed on large': 73,\n",
              " 'on large data': 159,\n",
              " 'large data sets': 120,\n",
              " 'data sets and': 61,\n",
              " 'sets and make': 197,\n",
              " 'and make guesses': 9,\n",
              " 'make guesses such': 132,\n",
              " 'guesses such as': 86,\n",
              " 'such as matching': 202,\n",
              " 'as matching surveillance': 16,\n",
              " 'matching surveillance camera': 135,\n",
              " 'surveillance camera image': 204,\n",
              " 'camera image to': 43,\n",
              " 'image to known': 96,\n",
              " 'to known face': 246,\n",
              " 'known face from': 116,\n",
              " 'face from database': 72,\n",
              " 'from database of': 83,\n",
              " 'database of photographs': 63,\n",
              " 'of photographs but': 155,\n",
              " 'photographs but its': 173,\n",
              " 'but its answers': 34,\n",
              " 'its answers are': 109,\n",
              " 'answers are inherently': 13,\n",
              " 'are inherently probabilistic': 15,\n",
              " 'inherently probabilistic and': 104,\n",
              " 'probabilistic and mathematical': 176,\n",
              " 'and mathematical proofs': 10,\n",
              " 'mathematical proofs require': 137,\n",
              " 'proofs require certainty': 180,\n",
              " 'but the team': 35,\n",
              " 'the team reasoned': 227,\n",
              " 'team reasoned that': 207,\n",
              " 'reasoned that machine': 186,\n",
              " 'that machine learning': 214,\n",
              " 'machine learning could': 129,\n",
              " 'learning could help': 122,\n",
              " 'could help to': 59,\n",
              " 'help to detect': 89,\n",
              " 'to detect patterns': 241,\n",
              " 'detect patterns such': 67,\n",
              " 'patterns such as': 172,\n",
              " 'such as the': 203,\n",
              " 'as the relationship': 17,\n",
              " 'the relationship between': 225,\n",
              " 'relationship between two': 187,\n",
              " 'between two types': 32,\n",
              " 'two types of': 257,\n",
              " 'types of object': 258,\n",
              " 'of object mathematicians': 153,\n",
              " 'object mathematicians could': 147,\n",
              " 'mathematicians could then': 139,\n",
              " 'could then try': 60,\n",
              " 'then try to': 233,\n",
              " 'try to work': 253,\n",
              " 'to work out': 249,\n",
              " 'work out the': 272,\n",
              " 'out the precise': 165,\n",
              " 'the precise relationship': 224,\n",
              " 'precise relationship by': 175,\n",
              " 'relationship by formulating': 188,\n",
              " 'by formulating what': 37,\n",
              " 'formulating what they': 82,\n",
              " 'what they call': 267,\n",
              " 'they call conjecture': 235,\n",
              " 'call conjecture and': 40,\n",
              " 'conjecture and then': 54,\n",
              " 'and then attempting': 11,\n",
              " 'then attempting to': 230,\n",
              " 'attempting to write': 21,\n",
              " 'to write rigorous': 251,\n",
              " 'write rigorous proof': 275,\n",
              " 'rigorous proof that': 193,\n",
              " 'proof that turns': 179,\n",
              " 'that turns that': 218,\n",
              " 'turns that statement': 255,\n",
              " 'that statement into': 216,\n",
              " 'statement into certainty': 201,\n",
              " 'because machine learning': 30,\n",
              " 'machine learning requires': 131,\n",
              " 'learning requires lots': 124,\n",
              " 'requires lots of': 191,\n",
              " 'lots of data': 128,\n",
              " 'of data to': 150,\n",
              " 'data to train': 62,\n",
              " 'to train on': 247,\n",
              " 'train on one': 252,\n",
              " 'on one requirement': 160,\n",
              " 'one requirement was': 162,\n",
              " 'requirement was to': 190,\n",
              " 'was to be': 265,\n",
              " 'to be able': 236,\n",
              " 'be able to': 23,\n",
              " 'able to calculate': 1,\n",
              " 'to calculate properties': 240,\n",
              " 'calculate properties for': 38,\n",
              " 'properties for large': 182,\n",
              " 'for large numbers': 77,\n",
              " 'large numbers of': 121,\n",
              " 'numbers of objects': 146,\n",
              " 'of objects in': 154,\n",
              " 'objects in the': 148,\n",
              " 'in the case': 102,\n",
              " 'the case of': 220,\n",
              " 'case of knots': 45,\n",
              " 'of knots the': 152,\n",
              " 'knots the team': 115,\n",
              " 'the team calculated': 226,\n",
              " 'team calculated several': 206,\n",
              " 'calculated several properties': 39,\n",
              " 'several properties called': 198,\n",
              " 'properties called invariants': 181,\n",
              " 'called invariants for': 41,\n",
              " 'invariants for millions': 106,\n",
              " 'for millions of': 79,\n",
              " 'millions of different': 141,\n",
              " 'of different knots': 151,\n",
              " 'he researchers then': 88,\n",
              " 'researchers then moved': 192,\n",
              " 'then moved on': 231,\n",
              " 'moved on to': 144,\n",
              " 'on to working': 161,\n",
              " 'to working out': 250,\n",
              " 'working out which': 273,\n",
              " 'out which ai': 167,\n",
              " 'which ai technique': 268,\n",
              " 'ai technique would': 2,\n",
              " 'technique would be': 209,\n",
              " 'would be most': 274,\n",
              " 'be most helpful': 28,\n",
              " 'most helpful for': 142,\n",
              " 'helpful for finding': 90,\n",
              " 'for finding pattern': 76,\n",
              " 'finding pattern that': 74,\n",
              " 'pattern that linked': 171,\n",
              " 'that linked two': 213,\n",
              " 'linked two properties': 127,\n",
              " 'two properties one': 256,\n",
              " 'properties one technique': 183,\n",
              " 'one technique in': 163,\n",
              " 'technique in particular': 208,\n",
              " 'in particular called': 101,\n",
              " 'particular called saliency': 169,\n",
              " 'called saliency maps': 42,\n",
              " 'saliency maps turned': 195,\n",
              " 'maps turned out': 134,\n",
              " 'turned out to': 254,\n",
              " 'out to be': 166,\n",
              " 'to be especially': 238,\n",
              " 'be especially helpful': 26,\n",
              " 'especially helpful it': 70,\n",
              " 'helpful it is': 91,\n",
              " 'it is often': 108,\n",
              " 'is often used': 107,\n",
              " 'often used in': 157,\n",
              " 'used in computer': 260,\n",
              " 'in computer vision': 100,\n",
              " 'computer vision to': 52,\n",
              " 'vision to identify': 263,\n",
              " 'to identify which': 244,\n",
              " 'identify which parts': 93,\n",
              " 'which parts of': 269,\n",
              " 'parts of an': 170,\n",
              " 'of an image': 149,\n",
              " 'an image carry': 4,\n",
              " 'image carry the': 95,\n",
              " 'carry the most': 44,\n",
              " 'the most relevant': 223,\n",
              " 'most relevant information': 143,\n",
              " 'relevant information saliency': 189,\n",
              " 'information saliency maps': 103,\n",
              " 'saliency maps pointed': 194,\n",
              " 'maps pointed to': 133,\n",
              " 'pointed to knot': 174,\n",
              " 'to knot properties': 245,\n",
              " 'knot properties that': 113,\n",
              " 'properties that were': 184,\n",
              " 'that were likely': 219,\n",
              " 'were likely to': 266,\n",
              " 'likely to be': 125,\n",
              " 'to be linked': 239,\n",
              " 'be linked to': 27,\n",
              " 'linked to each': 126,\n",
              " 'to each other': 242,\n",
              " 'each other and': 68,\n",
              " 'other and generated': 164,\n",
              " 'and generated formula': 7,\n",
              " 'generated formula that': 84,\n",
              " 'formula that seemed': 81,\n",
              " 'that seemed to': 215,\n",
              " 'seemed to be': 196,\n",
              " 'to be correct': 237,\n",
              " 'be correct in': 25,\n",
              " 'correct in all': 56,\n",
              " 'in all cases': 98,\n",
              " 'all cases that': 3,\n",
              " 'cases that could': 46,\n",
              " 'could be tested': 58,\n",
              " 'be tested lackenby': 29,\n",
              " 'tested lackenby and': 211,\n",
              " 'lackenby and juhász': 118,\n",
              " 'and juhász then': 8,\n",
              " 'juhász then provided': 112,\n",
              " 'then provided rigorous': 232,\n",
              " 'provided rigorous proof': 185,\n",
              " 'proof that the': 178,\n",
              " 'that the formula': 217,\n",
              " 'the formula applied': 221,\n",
              " 'formula applied to': 80,\n",
              " 'applied to very': 14,\n",
              " 'to very large': 248,\n",
              " 'very large class': 262,\n",
              " 'large class of': 119,\n",
              " 'class of knots': 49}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjt6AuawCbHq"
      },
      "source": [
        "## TF-IDF Vectorizer\n",
        "\n",
        "\n",
        "TF-IDF stands for term frequency-inverse document frequency. TF-IDF weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
        "\n",
        "**Term Frequency (TF)**: is a scoring of the frequency of the word in the current document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. The term frequency is often divided by the document length to normalize.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/404/1*SUAeubfQGK_w0XZWQW6V1Q.png)\n",
        "\n",
        "\n",
        "**Inverse Document Frequency (IDF)**: is a scoring of how rare the word is across documents. IDF is a measure of how rare a term is. Rarer the term, more is the IDF score.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/411/1*T57j-UDzXizqG40FUfmkLw.png)\n",
        "\n",
        "\n",
        "Thus,\n",
        "\n",
        "![alt text](https://miro.medium.com/max/215/1*YrgmAeG7KNRB4dQcGcsdyg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxNmcj76Df5b"
      },
      "source": [
        "The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allsentences = [\"The idea for a maths collaboration was sparked by a casual conversation in 2019 between mathematician Geordie Williamson at the University of Sydney in Australia and DeepMind’s chief executive, neuroscientist Demis Hassabis. Lackenby and a colleague at Oxford, András Juhász, both knot theorists, soon joined the project.\", \"Initially, the work focused on identifying mathematical problems that could be attacked using DeepMind’s technology. Machine learning enables computers to feed on large data sets and make guesses, such as matching a surveillance-camera image to a known face from a database of photographs. But its answers are inherently probabilistic, and mathematical proofs require certainty.\", \"But the team reasoned that machine learning could help to detect patterns, such as the relationship between two types of object. Mathematicians could then try to work out the precise relationship by formulating what they call a conjecture, and then attempting to write a rigorous proof that turns that statement into a certainty.\",\n",
        "            \"Because machine learning requires lots of data to train on, one requirement was to be able to calculate properties for large numbers of objects: in the case of knots, the team calculated several properties, called invariants, for millions of different knots.\",\n",
        "            \"he researchers then moved on to working out which AI technique would be most helpful for finding a pattern that linked two properties. One technique in particular, called saliency maps, turned out to be especially helpful. It is often used in computer vision to identify which parts of an image carry the most-relevant information. Saliency maps pointed to knot properties that were likely to be linked to each other, and generated a formula that seemed to be correct in all cases that could be tested. Lackenby and Juhász then provided a rigorous proof that the formula applied to a very large class of knots\"]\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(allsentences)\n",
        "print(X.toarray()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8yOQyudfvP8",
        "outputId": "0910f91d-a919-4fc0-9d17-2b5a28063d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1574478  0.         0.         0.         0.         0.17740669\n",
            " 0.1574478  0.         0.         0.         0.         0.31489561\n",
            " 0.         0.         0.1574478  0.         0.         0.1270279\n",
            " 0.1574478  0.         0.1270279  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.1574478\n",
            " 0.         0.1574478  0.         0.1574478  0.1574478  0.\n",
            " 0.         0.         0.1574478  0.         0.         0.\n",
            " 0.         0.1270279  0.1574478  0.         0.         0.\n",
            " 0.         0.         0.1574478  0.         0.         0.\n",
            " 0.         0.10544463 0.         0.         0.         0.\n",
            " 0.1574478  0.         0.1574478  0.         0.         0.\n",
            " 0.1574478  0.         0.         0.         0.21088926 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.1574478  0.1270279  0.1270279  0.         0.\n",
            " 0.1270279  0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.1574478\n",
            " 0.         0.1574478  0.         0.         0.         0.1574478\n",
            " 0.         0.         0.         0.07502472 0.         0.\n",
            " 0.         0.         0.         0.1574478  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.1574478  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.1574478  0.1574478  0.         0.         0.         0.1574478\n",
            " 0.         0.         0.         0.         0.         0.22507417\n",
            " 0.         0.1574478  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.1574478  0.\n",
            " 0.         0.         0.         0.1270279  0.         0.\n",
            " 0.         0.1574478  0.         0.         0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p9wfAtxf_NT",
        "outputId": "68a7e0f8-cc0a-4fc2-8a40-fd1ea8d4ab5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 149,\n",
              " 'idea': 66,\n",
              " 'for': 55,\n",
              " 'maths': 97,\n",
              " 'collaboration': 33,\n",
              " 'was': 165,\n",
              " 'sparked': 139,\n",
              " 'by': 20,\n",
              " 'casual': 29,\n",
              " 'conversation': 38,\n",
              " 'in': 70,\n",
              " '2019': 0,\n",
              " 'between': 17,\n",
              " 'mathematician': 95,\n",
              " 'geordie': 60,\n",
              " 'williamson': 169,\n",
              " 'at': 11,\n",
              " 'university': 160,\n",
              " 'of': 105,\n",
              " 'sydney': 143,\n",
              " 'australia': 14,\n",
              " 'and': 5,\n",
              " 'deepmind': 43,\n",
              " 'chief': 31,\n",
              " 'executive': 50,\n",
              " 'neuroscientist': 101,\n",
              " 'demis': 44,\n",
              " 'hassabis': 62,\n",
              " 'lackenby': 84,\n",
              " 'colleague': 34,\n",
              " 'oxford': 111,\n",
              " 'andrás': 6,\n",
              " 'juhász': 80,\n",
              " 'both': 18,\n",
              " 'knot': 81,\n",
              " 'theorists': 151,\n",
              " 'soon': 138,\n",
              " 'joined': 79,\n",
              " 'project': 121,\n",
              " 'initially': 73,\n",
              " 'work': 170,\n",
              " 'focused': 54,\n",
              " 'on': 107,\n",
              " 'identifying': 68,\n",
              " 'mathematical': 94,\n",
              " 'problems': 120,\n",
              " 'that': 148,\n",
              " 'could': 40,\n",
              " 'be': 15,\n",
              " 'attacked': 12,\n",
              " 'using': 162,\n",
              " 'technology': 146,\n",
              " 'machine': 90,\n",
              " 'learning': 86,\n",
              " 'enables': 48,\n",
              " 'computers': 36,\n",
              " 'to': 153,\n",
              " 'feed': 52,\n",
              " 'large': 85,\n",
              " 'data': 41,\n",
              " 'sets': 136,\n",
              " 'make': 91,\n",
              " 'guesses': 61,\n",
              " 'such': 141,\n",
              " 'as': 10,\n",
              " 'matching': 93,\n",
              " 'surveillance': 142,\n",
              " 'camera': 25,\n",
              " 'image': 69,\n",
              " 'known': 83,\n",
              " 'face': 51,\n",
              " 'from': 58,\n",
              " 'database': 42,\n",
              " 'photographs': 116,\n",
              " 'but': 19,\n",
              " 'its': 78,\n",
              " 'answers': 7,\n",
              " 'are': 9,\n",
              " 'inherently': 72,\n",
              " 'probabilistic': 119,\n",
              " 'proofs': 123,\n",
              " 'require': 129,\n",
              " 'certainty': 30,\n",
              " 'team': 144,\n",
              " 'reasoned': 126,\n",
              " 'help': 64,\n",
              " 'detect': 45,\n",
              " 'patterns': 115,\n",
              " 'relationship': 127,\n",
              " 'two': 158,\n",
              " 'types': 159,\n",
              " 'object': 103,\n",
              " 'mathematicians': 96,\n",
              " 'then': 150,\n",
              " 'try': 155,\n",
              " 'out': 110,\n",
              " 'precise': 118,\n",
              " 'formulating': 57,\n",
              " 'what': 167,\n",
              " 'they': 152,\n",
              " 'call': 23,\n",
              " 'conjecture': 37,\n",
              " 'attempting': 13,\n",
              " 'write': 173,\n",
              " 'rigorous': 133,\n",
              " 'proof': 122,\n",
              " 'turns': 157,\n",
              " 'statement': 140,\n",
              " 'into': 74,\n",
              " 'because': 16,\n",
              " 'requires': 131,\n",
              " 'lots': 89,\n",
              " 'train': 154,\n",
              " 'one': 108,\n",
              " 'requirement': 130,\n",
              " 'able': 1,\n",
              " 'calculate': 21,\n",
              " 'properties': 124,\n",
              " 'numbers': 102,\n",
              " 'objects': 104,\n",
              " 'case': 27,\n",
              " 'knots': 82,\n",
              " 'calculated': 22,\n",
              " 'several': 137,\n",
              " 'called': 24,\n",
              " 'invariants': 75,\n",
              " 'millions': 98,\n",
              " 'different': 46,\n",
              " 'he': 63,\n",
              " 'researchers': 132,\n",
              " 'moved': 100,\n",
              " 'working': 171,\n",
              " 'which': 168,\n",
              " 'ai': 2,\n",
              " 'technique': 145,\n",
              " 'would': 172,\n",
              " 'most': 99,\n",
              " 'helpful': 65,\n",
              " 'finding': 53,\n",
              " 'pattern': 114,\n",
              " 'linked': 88,\n",
              " 'particular': 112,\n",
              " 'saliency': 134,\n",
              " 'maps': 92,\n",
              " 'turned': 156,\n",
              " 'especially': 49,\n",
              " 'it': 77,\n",
              " 'is': 76,\n",
              " 'often': 106,\n",
              " 'used': 161,\n",
              " 'computer': 35,\n",
              " 'vision': 164,\n",
              " 'identify': 67,\n",
              " 'parts': 113,\n",
              " 'an': 4,\n",
              " 'carry': 26,\n",
              " 'relevant': 128,\n",
              " 'information': 71,\n",
              " 'pointed': 117,\n",
              " 'were': 166,\n",
              " 'likely': 87,\n",
              " 'each': 47,\n",
              " 'other': 109,\n",
              " 'generated': 59,\n",
              " 'formula': 56,\n",
              " 'seemed': 135,\n",
              " 'correct': 39,\n",
              " 'all': 3,\n",
              " 'cases': 28,\n",
              " 'tested': 147,\n",
              " 'provided': 125,\n",
              " 'applied': 8,\n",
              " 'very': 163,\n",
              " 'class': 32}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "data = fetch_20newsgroups()\n",
        "data.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teFFNI8GYWVZ",
        "outputId": "c4443942-4faf-46f9-c455-767799ebe39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = ['soc.religion.christian', 'sci.space','rec.motorcycles','comp.windows.x', 'comp.graphics'  ]\n",
        "train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "test = fetch_20newsgroups(subset='test', categories=categories)"
      ],
      "metadata": {
        "id": "Pr6hSK41YlKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (train.data[11])\n",
        "print (train.target[11])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua_hM8Y4ZQtg",
        "outputId": "f614871a-39fe-4473-aa73-802ca87018a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: schnitzi@osceola.cs.ucf.edu (Mark Schnitzius)\n",
            "Subject: Re: Atheists and Hell\n",
            "Organization: University of Central Florida\n",
            "Lines: 70\n",
            "\n",
            "db7n+@andrew.cmu.edu (D. Andrew Byler) writes:\n",
            "\n",
            ">Mark Schnitzius writes:\n",
            "\n",
            ">>>  Literal interpreters of the Bible will have a problem with this view, since\n",
            ">>>the Bible talks about the fires of Hell and such.  \n",
            ">> \n",
            ">>This is something I've always found confusing.  If all your nerve endings\n",
            ">>die with your physical body, why would flame hurt you?  How can one \"wail\n",
            ">>and gnash teeth\" with no lungs and no teeth?\n",
            "\n",
            ">One can feel physical pain by having a body, which, if you know the\n",
            ">doctrine of the resurrection of the body, is what people will have after\n",
            ">the great judgement.  \"We look for the resurrection of the dead, and the\n",
            ">life of the world to come.\"  - Nicene-Constantinopolitan Creed.  You\n",
            ">will have both body and soul in hell - eventually.\n",
            "\n",
            "Now this is getting interesting!\n",
            "\n",
            "I was raised Roman Catholic before becoming an atheist, so I have stated\n",
            "this Creed you quote nearly every Sunday until I was about 18.  For some\n",
            "reason, I always took the 'resurrection' in this statement to mean the\n",
            "resurrection of the soul, but I guess resurrection does strictly mean\n",
            "the raising of the physical body.  I have some questions on this point:\n",
            "\n",
            "1.  I always thought that Christians believe the descent into hell was \n",
            "pretty much immediate, and that there are people burning in hell right\n",
            "now.  You seem to be implying that it will not occur until after the\n",
            "\"great judgement\" (which I read as meaning the proverbial Judgment Day).\n",
            "I was always a little confused on this point, even when I was with the\n",
            "church -- maybe someone can clear it up for me.  Where will my \"soul\"\n",
            "(which, by the way, I don't believe in) exist until that time?\n",
            "\n",
            "2.  Will the new body I will have be created out of the same atoms \n",
            "that my body now is made of, or will it be built from scratch?  My\n",
            "physical body now is susceptible to aging, etc. -- so I guess my\n",
            "new body will have to be radically different in order to be immortal\n",
            "so it can be tortured for all eternity?\n",
            "\n",
            "3.  Since I will have a physical body, I assume it will need a physical\n",
            "place to exist in -- where is this hell?  In the center of the earth?\n",
            "Do you think we could find it if we dig?\n",
            "\n",
            "Mark Schnitzius\n",
            "schnitzi@eola.cs.ucf.edu\n",
            "Univ. of Central Florida\n",
            "\n",
            "[There is not complete agreement on the details of the afterlife.  I\n",
            "think the most common view is that final disposition does not occur\n",
            "until a final judgement, which is still in the future.  In the\n",
            "meantime, some believe that people \"sleep\" until the final\n",
            "resurrection (or because God is above time, pass directly from death\n",
            "to the future time when the resurrection occurs), while others believe\n",
            "that souls have a disembodied, pre-resurrection existence until then.\n",
            "There are probably other alternatives that I'm omitting.\n",
            "\n",
            "The new body is generally conceived of being implemented in a\n",
            "different \"technology\" than the current one, one which is not mortal.\n",
            "(Paul talks about the mortal being raised to immortality, and Jesus'\n",
            "resurrected body -- which is the first example -- clearly was not\n",
            "subject to the same kind of limitations as ours.)  It is assumed that\n",
            "there are enough similarities that people will recognize each other,\n",
            "but I don't think most people claim to know the details.  I don't\n",
            "think I'd say it's the same atoms.  I'd assume there would be some\n",
            "analog of a physical place, but I wouldn't expect to find it under the\n",
            "earth or up in the sky.  I'd suspect that it's in another dimension,\n",
            "outside this physical world, or whatever.  But again, we have little\n",
            "in the way of details.\n",
            "\n",
            "--clh]\n",
            "\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSjACk-XaCur",
        "outputId": "f985321e-c8eb-4b13-ce58-d4136c5b46dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 1, ..., 2, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.data[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD_8VGP3aIvI",
        "outputId": "57f3c9df-7cd6-4c3d-ffec-bb40e83964c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: dealy@narya.gsfc.nasa.gov (Brian Dealy - CSC)\n",
            "Subject: Re: Fresco status?\n",
            "Organization: NASA/Goddard Space Flight Center\n",
            "Lines: 34\n",
            "Distribution: world\n",
            "NNTP-Posting-Host: narya.gsfc.nasa.gov\n",
            "Originator: dealy@narya.gsfc.nasa.gov\n",
            "\n",
            "\n",
            "Issue 5 of the X Resource (the published proceedings of the 7th Annual X\n",
            "Technical Conference) has an paper by Mark Linton and Chuck Price\n",
            "titled \"Building Distributed interfaces with Fresco\".\n",
            "\n",
            "The summary describes Fresco (formerly known as XC++) as an X consortium effort.\n",
            "Without doing a complete review of the paper, I'll just mention the goals\n",
            "as stated in one section of the article.  the effort has the goal of providing\n",
            "the next generation toolkit with functionality beyond the Xt toolkit or Xlib.\n",
            "Features they want in FRESCO include:\n",
            "\n",
            "lightweight Objects, such as Interviews Glyphs\n",
            "Structured Graphics\n",
            "Resolution independence\n",
            "Natural C++ programming interface\n",
            "edit-in-place embedding\n",
            "distributed user interface components\n",
            "Multithreading\n",
            "\n",
            "This by no means captures the complete content of the paper. The Conclusions\n",
            "sections mentions that a rough draft specification should be available in\n",
            "early 93, with no schedule (paper presented in Jan 93) for a complete sample\n",
            "implementation.\n",
            "\n",
            "I am not affiliated with any of the people or places mentioned above.\n",
            "\n",
            "-- \n",
            "Brian Dealy                |301-572-8267| It not knowing where it's at  \n",
            "dealy@kong.gsfc.nasa.gov   |            | that's important,it's knowing\n",
            "!uunet!dftsrv!kong!dealy   |            | where it's not at...  B.Dylan\n",
            "-- \n",
            "Brian Dealy                |301-572-8267| It not knowing where it's at  \n",
            "dealy@kong.gsfc.nasa.gov   |            | that's important,it's knowing\n",
            "!uunet!dftsrv!kong!dealy   |            | where it's not at...  B.Dylan\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.target[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiviNVUSaif9",
        "outputId": "1521abcc-df35-414b-f529-6d2ea702bedb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now each newsletter would have to be converted into numbers\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "# model_01 = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "# model_02 = make_pipeline(TfidfVectorizer, SCV(C=10))"
      ],
      "metadata": {
        "id": "K5dOJIgcawiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train.data, train.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "YCeCuWHpbTr1",
        "outputId": "8b360037-be16-43da-8ff4-7a5fe160dd7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
              "                ('multinomialnb', MultinomialNB())])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidfvectorizer&#x27;, TfidfVectorizer()),\n",
              "                (&#x27;multinomialnb&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidfvectorizer&#x27;, TfidfVectorizer()),\n",
              "                (&#x27;multinomialnb&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = model.predict(test.data)"
      ],
      "metadata": {
        "id": "SrDdPUdMcEm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBq1JLDncLYu",
        "outputId": "957b64f0-cadb-403f-fb3d-57a08af39669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 3 1 ... 2 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print (classification_report(test.target, labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w_iCjdvcOge",
        "outputId": "d75b1eba-a67a-4006-e0ad-4f6c1eed8db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.79      0.82       389\n",
            "           1       0.92      0.83      0.87       395\n",
            "           2       0.96      0.97      0.96       398\n",
            "           3       0.94      0.91      0.92       394\n",
            "           4       0.81      0.98      0.89       398\n",
            "\n",
            "    accuracy                           0.90      1974\n",
            "   macro avg       0.90      0.90      0.89      1974\n",
            "weighted avg       0.90      0.90      0.89      1974\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print (confusion_matrix(test.target, labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrhFyavycaBg",
        "outputId": "ab4842b4-626d-4a6c-ea73-a3922c43ccdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[306  23  10  12  38]\n",
            " [ 43 326   3   7  16]\n",
            " [  0   1 385   1  11]\n",
            " [  4   4   1 359  26]\n",
            " [  1   0   1   4 392]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.steps[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujx5T92jhDCU",
        "outputId": "88a10c1d-c2c0-44e6-bb85-ddbe66892999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tfidfvectorizer', TfidfVectorizer())"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_category(s, train=train, model = model):\n",
        "    pred = model.predict([s])\n",
        "    print (pred)\n",
        "    return train.target_names[pred[0]]"
      ],
      "metadata": {
        "id": "DPHbOzNXdSqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_category(\"Whats the mileage of the motorcycle in the lot?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "IonMDq08dlgN",
        "outputId": "e9e4f3b4-9bcf-4a83-ad23-57e9da6b4a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rec.motorcycles'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_category(\"Let's talk about our investments and its returns over this fiscal year.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "GufyeX9Qeds9",
        "outputId": "158a7993-cf12-4204-bff1-aed54cbe328e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'soc.religion.christian'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_category(\"Sachin scored a double hundred\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "O9OWVI3Dedgr",
        "outputId": "c174f955-9628-45b5-d3e2-8b6285c52ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comp.graphics'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_category(\"Windows xp was the best OS ever.!!!!!\")"
      ],
      "metadata": {
        "id": "4IWKv4DNe2fm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3f4d9b26-3dca-4bae-91b7-177da8158a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comp.windows.x'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJjoboYZwL6w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "ec134276-9765-43ed-f4c1-71aad858b3e3"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f8c0cb75-752f-4ec7-bc45-03bbd9163e9a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f8c0cb75-752f-4ec7-bc45-03bbd9163e9a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2eNYFFCwCSU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "ccc6d174-323c-44fd-8656-30a8d2ba9208"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read json into a dataframe\n",
        "df_idf=pd.read_json(\"stackoverflow-data-idf.json\",lines=True)\n",
        "\n",
        "# print schema\n",
        "print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
        "print(\"Number of questions,columns=\",df_idf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-4c8010442caf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# read json into a dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stackoverflow-data-idf.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0mdata_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m             )\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected object or value"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDT8RjbwwZxA"
      },
      "source": [
        "import re\n",
        "def pre_process(text):\n",
        "\n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "\n",
        "    #remove tags\n",
        "    text=re.sub(\"</?.*?>\",\" <> \",text)\n",
        "\n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "\n",
        "    return text\n",
        "\n",
        "df_idf['text'] = df_idf['title'] + df_idf['body']\n",
        "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))\n",
        "\n",
        "#show the first 'text'\n",
        "df_idf['text'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-hv_RowkZs6"
      },
      "source": [
        "df_idf['text'][67]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZY_dD836oIo"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyyPBtvrwgbM"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_stop_words(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "\n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return frozenset(stop_set)\n",
        "\n",
        "#load a set of stop words\n",
        "stopwords=get_stop_words(\"stopwords.txt\")\n",
        "\n",
        "#get the text column\n",
        "docs=df_idf['text'].tolist()\n",
        "\n",
        "#create a vocabulary of words,\n",
        "#ignore words that appear in more than 90% of documents,\n",
        "#eliminate stop words\n",
        "cv=CountVectorizer(max_df=0.9,stop_words=stopwords)\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBMYQrrlxKH2"
      },
      "source": [
        "word_count_vector.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrXtBzFnlmRS"
      },
      "source": [
        "word_count_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCfMx2VkxOWF"
      },
      "source": [
        "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=50000)\n",
        "word_count_vector=cv.fit_transform(docs)\n",
        "word_count_vector.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8femwmsxRSp"
      },
      "source": [
        "list(cv.vocabulary_.keys())[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef-9jLnjxTt4"
      },
      "source": [
        "\n",
        "list(cv.get_feature_names())[2000:2015]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m5wAh2BxqD7"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAxAwjrbxsPV"
      },
      "source": [
        "\n",
        "tfidf_transformer.idf_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSIgkppuyZL-"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZO76ZJAyeyM"
      },
      "source": [
        "# read test docs into a dataframe and concatenate title and body\n",
        "df_test=pd.read_json(\"stackoverflow-test.json\",lines=True)\n",
        "df_test['text'] = df_test['title'] + df_test['body']\n",
        "df_test['text'] =df_test['text'].apply(lambda x:pre_process(x))\n",
        "\n",
        "# get test docs into a list\n",
        "docs_test=df_test['text'].tolist()\n",
        "docs_title=df_test['title'].tolist()\n",
        "docs_body=df_test['body'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JdTFJgvltor"
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=5):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "\n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "\n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgqavv0slzE_"
      },
      "source": [
        "# you only needs to do this once\n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "# get the document that we want to extract keywords from\n",
        "doc=docs_test[28]\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "#extract only the top n; n here is 10\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "# now print the results\n",
        "print(\"\\n=====Title=====\")\n",
        "print(docs_title[28])\n",
        "print(\"\\n=====Body=====\")\n",
        "print(docs_body[28])\n",
        "print(\"\\n===Keywords===\")\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xMva08Rl5EY"
      },
      "source": [
        "# put the common code into several methods\n",
        "def get_keywords(idx):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs_test[idx]]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "    return keywords\n",
        "\n",
        "def print_results(idx,keywords):\n",
        "    # now print the results\n",
        "    print(\"\\n=====Title=====\")\n",
        "    print(docs_title[idx])\n",
        "    print(\"\\n=====Body=====\")\n",
        "    print(docs_body[idx])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        print(k,keywords[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwaH93fRnUmW"
      },
      "source": [
        "idx=450\n",
        "keywords=get_keywords(idx)\n",
        "print_results(idx,keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx=89\n",
        "keywords=get_keywords(idx)\n",
        "print_results(idx,keywords)"
      ],
      "metadata": {
        "id": "Fns5rKG-bmC2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}